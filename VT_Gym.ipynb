<<<<<<< HEAD
{"cells":[{"cell_type":"markdown","metadata":{"id":"eD7vpMS924a1"},"source":["This Notebook contains implementations of neural network models that process video frames and pose landmarks to classify actions and predict ratings.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"fwXbWGQOyJDD","executionInfo":{"status":"ok","timestamp":1733652269783,"user_tz":-120,"elapsed":26348,"user":{"displayName":"ai research","userId":"07349987832510586956"}}},"outputs":[],"source":["import pandas as pd\n","import cv2\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from transformers import AutoImageProcessor, AutoModelForPreTraining\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","import json"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FmdwF_C45U--","outputId":"fd67d5f1-39b0-4563-8329-7cf68ccdf174","executionInfo":{"status":"ok","timestamp":1733652283286,"user_tz":-120,"elapsed":986,"user":{"displayName":"ai research","userId":"07349987832510586956"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA is available. PyTorch can use the GPU.\n","Device name: Tesla T4\n"]}],"source":["# Check if CUDA (GPU support) is available\n","if torch.cuda.is_available():\n","    print(\"CUDA is available. PyTorch can use the GPU.\")\n","    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n","else:\n","    print(\"CUDA is not available. PyTorch is using the CPU.\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"KdOB4Yjz50De","executionInfo":{"status":"ok","timestamp":1733652285282,"user_tz":-120,"elapsed":8,"user":{"displayName":"ai research","userId":"07349987832510586956"}}},"outputs":[],"source":["base_path = ''\n"]},{"cell_type":"markdown","metadata":{"id":"GsKfIuDB2zsP"},"source":["## Functions\n","\n","### `load_and_resize_frames`\n","- **Description**: Loads and resizes video frames from specified file paths.\n","- **Parameters**:\n","  - `num_video_frontal`: Identifier for the video file.\n","  - `num_idx`: Index for the specific video segment.\n","  - `num_frames`: Number of frames to load.\n","  - `size`: Target size for resizing frames (default: (224, 224)).\n","- **Returns**: List of resized frames.\n","\n","### `extract_pose_landmarks`\n","- **Description**: Extracts pose landmarks from a list of video frames using MediaPipe.\n","- **Parameters**:\n","  - `video_frames`: List of frames (each of size [224, 224, 3]).\n","- **Returns**: Array of pose landmarks with shape [num_frames, 33, 3].\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"64pDhqby2ohg","executionInfo":{"status":"ok","timestamp":1733652297948,"user_tz":-120,"elapsed":467,"user":{"displayName":"ai research","userId":"07349987832510586956"}}},"outputs":[],"source":["def load_and_resize_frames(num_video, action, frontORlat, num_idx, num_frames, size=(224, 224)):\n","    \"\"\"\n","    Loads and resizes video frames from specified file paths based on the action type.\n","\n","    Args:\n","        num_video (int): Identifier for the video file.\n","        action (str): The action being performed ('deadlift', 'squat', 'lunges').\n","        frontORlat (int): 1 for frontal frames, 0 for lateral frames.\n","        num_idx (int): Index for the specific video segment.\n","        num_frames (int): Number of frames to load.\n","        size (tuple): Target size for resizing frames (default is (224, 224)).\n","\n","    Returns:\n","        list: A list of resized frames.\n","    \"\"\"\n","    frames = []\n","\n","    # Determine the folder paths based on the action\n","    if action == 'Deadlift':\n","        frontal_folder = base_path+'Deadlift_Frames'\n","        lateral_folder =  base_path+'Deadlift_Frames'\n","    elif action == 'Squat':\n","        frontal_folder =  base_path+'Squat_Frames'\n","        lateral_folder =  base_path+'Squat_Frames'\n","    elif action == 'lunges':\n","        frontal_folder =  base_path+'Lunges_Frames'\n","        lateral_folder =  base_path+'Lunges_Frames'\n","    else:\n","        raise ValueError(f\"Unknown action: {action}\")\n","\n","    # Load the frames based on frontORlat flag\n","    if frontORlat == 1:\n","        folder = frontal_folder\n","    else:\n","        folder = lateral_folder\n","\n","    # Load and resize frames\n","    for i in range(1, num_frames + 1):  # Loop from 1 to num_frames (e.g., 16)\n","        path = f\"{folder}/{num_video}_idx_{num_idx}_{i}.jpg\"\n","        img = cv2.imread(path)\n","        if img is not None:\n","            img_resized = cv2.resize(img, size)  # Resize to the specified size (default is 224x224)\n","            frames.append(img_resized)\n","        else:\n","            print(f\"Warning: Could not load image at {path}\")\n","\n","    return frames\n","\n","def extract_pose_landmarks(video_frames):\n","    \"\"\"\n","    Extracts pose landmarks from a list of video frames using MediaPipe.\n","\n","    Args:\n","        video_frames (list): List of frames (each of size [224, 224, 3]).\n","\n","    Returns:\n","        np.array: Array of pose landmarks with shape [num_frames, 33, 3].\n","    \"\"\"\n","    pose_landmarks = []\n","\n","    for frame in video_frames:\n","        # Ensure the frame is in the right format\n","        if frame.shape != (224, 224, 3):\n","            print(f\"Unexpected frame shape: {frame.shape}\")\n","            continue\n","        # Convert the frame from float64 [0, 1] to uint8 [0, 255]\n","        frame_uint8 = (np.clip(frame * 255, 0, 255)).astype(np.uint8)  # Shape: [224, 224, 3]\n","\n","        # Extract pose landmarks\n","        results = pose.process(frame_uint8)\n","\n","        if results.pose_landmarks:\n","            # Get the landmarks (33 landmarks, each with x, y, z)\n","            landmarks = [[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark]\n","        else:\n","            # If no landmarks are detected, we use a zero vector\n","            landmarks = np.zeros((33, 3))\n","\n","        pose_landmarks.append(landmarks)\n","\n","    return np.array(pose_landmarks)  # Shape: [num_frames, 33, 3]"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"DNbu6JEg5U-_","outputId":"3057b4de-dd20-4b23-b1e2-9a598b4b21de","executionInfo":{"status":"error","timestamp":1733652301952,"user_tz":-120,"elapsed":576,"user":{"displayName":"ai research","userId":"07349987832510586956"}}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'squat_edited.xlsx'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-a69e4f9dceca>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_squat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'squat_edited.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load JSON files and convert to NumPy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'squat_edited.xlsx'"]}],"source":["import json\n","import numpy as np\n","import pandas as pd\n","\n","df_squat = pd.read_excel(base_path + 'squat_edited.xlsx')\n","\n","# Load JSON files and convert to NumPy arrays\n","def load_json_as_numpy(json_file):\n","    with open(json_file, 'r') as file:\n","        data = json.load(file)  # Load the JSON data\n","    return np.array(data)  # Convert the list to a NumPy array\n","\n","# Load the front and lateral poses\n","front_pose_array = load_json_as_numpy(base_path + 'front_pose_squat.json')\n","lat_pose_array = load_json_as_numpy(base_path + 'lat_pose_squat.json')\n","\n","# Make sure `front_pose` and `lat_pose` columns exist\n","if 'front_pose' not in df_squat.columns:\n","    df_squat['front_pose'] = None\n","if 'lat_pose' not in df_squat.columns:\n","    df_squat['lat_pose'] = None\n","\n","# Assign the loaded arrays back to the DataFrame\n","# Make sure the lengths match\n","if len(front_pose_array) == len(df_squat) and len(lat_pose_array) == len(df_squat):\n","    df_squat['front_pose'] = list(front_pose_array)\n","    df_squat['lat_pose'] = list(lat_pose_array)\n","else:\n","    print(\"Error: The length of the loaded arrays does not match the DataFrame.\")\n","\n","# Display the updated DataFrame\n","print(df_squat[['front_pose', 'lat_pose']])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"phgbHMZC5U_A"},"outputs":[],"source":["# Define your split ratios\n","train_ratio = 0.7\n","val_ratio = 0.15\n","test_ratio = 0.15\n","\n","# Calculate the number of samples for each set\n","total_samples = len(df_squat)\n","train_size = int(total_samples * train_ratio)\n","val_size = int(total_samples * val_ratio)\n","\n","# Split the DataFrame\n","train_df_squat = df_squat.iloc[:train_size]                    # First 70% for training\n","val_df_squat = df_squat.iloc[train_size:train_size + val_size]  # Next 15% for validation\n","test_df_squat = df_squat.iloc[train_size + val_size:]           # Last 15% for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":805},"id":"p6g0uol45U_B","outputId":"5aa658a6-6992-405e-c119-af754fa4d441"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Num Video Frontal</th>\n","      <th>Num Video Lateral</th>\n","      <th>NumIdx</th>\n","      <th>Action</th>\n","      <th>Feet Out 30 F</th>\n","      <th>Score: Frontal( - / 1)</th>\n","      <th>Whole Feet Flat On the Floor (1) L</th>\n","      <th>Bend Hips and Knees Simultaniously (1) L</th>\n","      <th>Hips backwards (1) L</th>\n","      <th>Lower back neural (1) L</th>\n","      <th>Hips are lower than knees level (1 point) L</th>\n","      <th>Score: Lateral ( - / 5)</th>\n","      <th>Total Score - /6</th>\n","      <th>class</th>\n","      <th>rating</th>\n","      <th>front_pose</th>\n","      <th>lat_pose</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>Squat</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>[[[0.49896547198295593, 0.24871453642845154, -...</td>\n","      <td>[[[0.48301321268081665, 0.2528548836708069, 0....</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>Squat</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>[[[0.5140300393104553, 0.22558534145355225, -0...</td>\n","      <td>[[[0.5150753259658813, 0.25385501980781555, 0....</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>Squat</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>[[[0.5227701663970947, 0.22155773639678955, -0...</td>\n","      <td>[[[0.523187518119812, 0.23705779016017914, 0.1...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>Squat</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>[[[0.5229289531707764, 0.2611807584762573, -0....</td>\n","      <td>[[[0.5000579953193665, 0.24061907827854156, 0....</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>Squat</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>[[[0.5273605585098267, 0.22662483155727386, -0...</td>\n","      <td>[[[0.5319615006446838, 0.2524169683456421, 0.0...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>201</th>\n","      <td>85</td>\n","      <td>86</td>\n","      <td>1</td>\n","      <td>Squat</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...</td>\n","      <td>[[[0.3942027986049652, 0.23040449619293213, -0...</td>\n","    </tr>\n","    <tr>\n","      <th>202</th>\n","      <td>85</td>\n","      <td>86</td>\n","      <td>2</td>\n","      <td>Squat</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>[[[0.4873969852924347, 0.1548309326171875, -0....</td>\n","      <td>[[[0.4314277470111847, 0.22929038107395172, -0...</td>\n","    </tr>\n","    <tr>\n","      <th>203</th>\n","      <td>85</td>\n","      <td>86</td>\n","      <td>3</td>\n","      <td>Squat</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>[[[0.4695884585380554, 0.15741464495658875, -0...</td>\n","      <td>[[[0.39387422800064087, 0.2241220325231552, -0...</td>\n","    </tr>\n","    <tr>\n","      <th>204</th>\n","      <td>85</td>\n","      <td>86</td>\n","      <td>4</td>\n","      <td>Squat</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>[[[0.47849878668785095, 0.20681476593017578, -...</td>\n","      <td>[[[0.41384050250053406, 0.22573530673980713, -...</td>\n","    </tr>\n","    <tr>\n","      <th>205</th>\n","      <td>87</td>\n","      <td>88</td>\n","      <td>0</td>\n","      <td>Squat</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>[[[0.43349573016166687, 0.1754622608423233, -0...</td>\n","      <td>[[[0.3537933826446533, 0.18060621619224548, -0...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>206 rows Ã— 17 columns</p>\n","</div>"],"text/plain":["     Num Video Frontal  Num Video Lateral  NumIdx Action  Feet Out 30 F  \\\n","0                    1                  2       0  Squat              0   \n","1                    1                  2       1  Squat              0   \n","2                    1                  2       2  Squat              0   \n","3                    1                  2       3  Squat              0   \n","4                    1                  2       4  Squat              0   \n","..                 ...                ...     ...    ...            ...   \n","201                 85                 86       1  Squat              1   \n","202                 85                 86       2  Squat              1   \n","203                 85                 86       3  Squat              1   \n","204                 85                 86       4  Squat              1   \n","205                 87                 88       0  Squat              1   \n","\n","     Score: Frontal( - / 1)  Whole Feet Flat On the Floor (1) L  \\\n","0                         0                                   1   \n","1                         0                                   0   \n","2                         0                                   0   \n","3                         0                                   0   \n","4                         0                                   0   \n","..                      ...                                 ...   \n","201                       1                                   0   \n","202                       1                                   0   \n","203                       1                                   0   \n","204                       1                                   0   \n","205                       1                                   0   \n","\n","     Bend Hips and Knees Simultaniously (1) L  Hips backwards (1) L  \\\n","0                                           1                     1   \n","1                                           1                     1   \n","2                                           1                     1   \n","3                                           1                     1   \n","4                                           1                     1   \n","..                                        ...                   ...   \n","201                                         1                     0   \n","202                                         1                     0   \n","203                                         1                     0   \n","204                                         1                     0   \n","205                                         1                     0   \n","\n","     Lower back neural (1) L  Hips are lower than knees level (1 point) L  \\\n","0                          1                                            0   \n","1                          1                                            0   \n","2                          1                                            0   \n","3                          1                                            0   \n","4                          1                                            0   \n","..                       ...                                          ...   \n","201                        1                                            1   \n","202                        1                                            1   \n","203                        1                                            1   \n","204                        1                                            1   \n","205                        1                                            0   \n","\n","     Score: Lateral ( - / 5)  Total Score - /6  class  rating  \\\n","0                          4                 4      1       4   \n","1                          3                 3      1       3   \n","2                          3                 3      1       3   \n","3                          3                 3      1       3   \n","4                          3                 3      1       3   \n","..                       ...               ...    ...     ...   \n","201                        3                 4      1       4   \n","202                        3                 4      1       4   \n","203                        3                 4      1       4   \n","204                        3                 4      1       4   \n","205                        2                 3      1       3   \n","\n","                                            front_pose  \\\n","0    [[[0.49896547198295593, 0.24871453642845154, -...   \n","1    [[[0.5140300393104553, 0.22558534145355225, -0...   \n","2    [[[0.5227701663970947, 0.22155773639678955, -0...   \n","3    [[[0.5229289531707764, 0.2611807584762573, -0....   \n","4    [[[0.5273605585098267, 0.22662483155727386, -0...   \n","..                                                 ...   \n","201  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n","202  [[[0.4873969852924347, 0.1548309326171875, -0....   \n","203  [[[0.4695884585380554, 0.15741464495658875, -0...   \n","204  [[[0.47849878668785095, 0.20681476593017578, -...   \n","205  [[[0.43349573016166687, 0.1754622608423233, -0...   \n","\n","                                              lat_pose  \n","0    [[[0.48301321268081665, 0.2528548836708069, 0....  \n","1    [[[0.5150753259658813, 0.25385501980781555, 0....  \n","2    [[[0.523187518119812, 0.23705779016017914, 0.1...  \n","3    [[[0.5000579953193665, 0.24061907827854156, 0....  \n","4    [[[0.5319615006446838, 0.2524169683456421, 0.0...  \n","..                                                 ...  \n","201  [[[0.3942027986049652, 0.23040449619293213, -0...  \n","202  [[[0.4314277470111847, 0.22929038107395172, -0...  \n","203  [[[0.39387422800064087, 0.2241220325231552, -0...  \n","204  [[[0.41384050250053406, 0.22573530673980713, -...  \n","205  [[[0.3537933826446533, 0.18060621619224548, -0...  \n","\n","[206 rows x 17 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["train_df_squat"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4cXvpCRM5U_B","outputId":"644634f3-f7a9-4ecd-ada0-47ec9f8d097f"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                            front_pose  \\\n","0    [[[0.6141521334648132, 0.2384583204984665, -0....   \n","1    [[[0.7448023557662964, 0.29284247756004333, -0...   \n","2    [[[0.6720446944236755, 0.26214927434921265, -1...   \n","3    [[[0.7419819831848145, 0.2703408896923065, -0....   \n","4    [[[0.7648568749427795, 0.3041684925556183, -1....   \n","..                                                 ...   \n","264  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n","265  [[[0.6985482573509216, 0.3878808617591858, -0....   \n","266  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n","267  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n","268  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n","\n","                                              lat_pose  \n","0    [[[0.5667536854743958, 0.2648758590221405, -0....  \n","1    [[[0.5568003058433533, 0.25841817259788513, 0....  \n","2    [[[0.6044440865516663, 0.25434428453445435, -0...  \n","3    [[[0.614557683467865, 0.25464609265327454, -0....  \n","4    [[[0.591785728931427, 0.26665791869163513, 0.0...  \n","..                                                 ...  \n","264  [[[0.6106436848640442, 0.25392264127731323, -0...  \n","265  [[[0.6022240519523621, 0.23920293152332306, -0...  \n","266  [[[0.6188439130783081, 0.1534697264432907, -0....  \n","267  [[[0.5743303894996643, 0.17498759925365448, -0...  \n","268  [[[0.5832579731941223, 0.18293842673301697, -0...  \n","\n","[269 rows x 2 columns]\n"]}],"source":["df_dead = pd.read_excel(base_path + 'deadlift_edited.xlsx')\n","\n","# Load JSON files and convert to NumPy arrays\n","def load_json_as_numpy(json_file):\n","    with open(json_file, 'r') as file:\n","        data = json.load(file)  # Load the JSON data\n","    return np.array(data)  # Convert the list to a NumPy array\n","\n","# Load the front and lateral poses\n","front_pose_array = load_json_as_numpy(base_path + 'front_pose_dead.json')\n","lat_pose_array = load_json_as_numpy(base_path + 'lat_pose_dead.json')\n","\n","# Make sure `front_pose` and `lat_pose` columns exist\n","if 'front_pose' not in df_dead.columns:\n","    df_dead['front_pose'] = None\n","if 'lat_pose' not in df_dead.columns:\n","    df_dead['lat_pose'] = None\n","\n","# Assign the loaded arrays back to the DataFrame\n","# Make sure the lengths match\n","if len(front_pose_array) == len(df_dead) and len(lat_pose_array) == len(df_dead):\n","    df_dead['front_pose'] = list(front_pose_array)\n","    df_dead['lat_pose'] = list(lat_pose_array)\n","else:\n","    print(\"Error: The length of the loaded arrays does not match the DataFrame.\")\n","\n","# Display the updated DataFrame\n","print(df_dead[['front_pose', 'lat_pose']])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p7af0bK55U_B"},"outputs":[],"source":["# Define your split ratios\n","train_ratio = 0.7\n","val_ratio = 0.15\n","test_ratio = 0.15\n","\n","# Calculate the number of samples for each set\n","total_samples = len(df_dead)\n","train_size = int(total_samples * train_ratio)\n","val_size = int(total_samples * val_ratio)\n","\n","# Split the DataFrame\n","train_df_dead = df_dead.iloc[:train_size]                    # First 70% for training\n","val_df_dead = df_dead.iloc[train_size:train_size + val_size]  # Next 15% for validation\n","test_df_dead = df_dead.iloc[train_size + val_size:]           # Last 15% for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T-pXveAf5U_B","outputId":"0ab18bab-bc66-453a-9998-77a8b4a42b21"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                            front_pose  \\\n","0    [[[0.5307167768478394, 0.26830747723579407, -0...   \n","1    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n","2    [[[0.5470829606056213, 0.22924844920635223, -0...   \n","3    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n","4    [[[0.5109826922416687, 0.2688097357749939, -0....   \n","..                                                 ...   \n","101  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n","102  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n","103  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n","104  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n","105  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n","\n","                                              lat_pose  \n","0    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n","1    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n","2    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n","3    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n","4    [[[0.3619050979614258, 0.2965300977230072, -0....  \n","..                                                 ...  \n","101  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n","102  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n","103  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n","104  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n","105  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n","\n","[106 rows x 2 columns]\n"]}],"source":["df_lunge = pd.read_excel(base_path + 'lunges_edited.xlsx')\n","\n","# Load JSON files and convert to NumPy arrays\n","def load_json_as_numpy(json_file):\n","    with open(json_file, 'r') as file:\n","        data = json.load(file)  # Load the JSON data\n","    return np.array(data)  # Convert the list to a NumPy array\n","\n","# Load the front and lateral poses\n","front_pose_array = load_json_as_numpy(base_path + 'front_pose_lunges.json')\n","lat_pose_array = load_json_as_numpy(base_path + 'lat_pose_lunges.json')\n","\n","# Make sure `front_pose` and `lat_pose` columns exist\n","if 'front_pose' not in df_lunge.columns:\n","    df_lunge['front_pose'] = None\n","if 'lat_pose' not in df_lunge.columns:\n","    df_lunge['lat_pose'] = None\n","\n","# Assign the loaded arrays back to the DataFrame\n","# Make sure the lengths match\n","if len(front_pose_array) == len(df_lunge) and len(lat_pose_array) == len(df_lunge):\n","    df_lunge['front_pose'] = list(front_pose_array)\n","    df_lunge['lat_pose'] = list(lat_pose_array)\n","else:\n","    print(\"Error: The length of the loaded arrays does not match the DataFrame.\")\n","\n","# Display the updated DataFrame\n","print(df_lunge[['front_pose', 'lat_pose']])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJSBgqjs5U_C"},"outputs":[],"source":["# Define your split ratios\n","train_ratio = 0.7\n","val_ratio = 0.15\n","test_ratio = 0.15\n","\n","# Calculate the number of samples for each set\n","total_samples = len(df_lunge)\n","train_size = int(total_samples * train_ratio)\n","val_size = int(total_samples * val_ratio)\n","\n","# Split the DataFrame\n","train_df_lunge = df_lunge.iloc[:train_size]                    # First 70% for training\n","val_df_lunge = df_lunge.iloc[train_size:train_size + val_size]  # Next 15% for validation\n","test_df_lunge = df_lunge.iloc[train_size + val_size:]           # Last 15% for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kHrmZE3a5U_C"},"outputs":[],"source":["\n","# Keep only the specified columns and concatenate the DataFrames\n","train_df = pd.concat([\n","    train_df_squat,\n","    train_df_lunge,\n","    train_df_dead\n","])\n","\n","train_df = train_df.sample(frac=1, random_state=1).reset_index(drop=True)\n","\n","val_df = pd.concat([\n","    val_df_squat,\n","    val_df_lunge,\n","    val_df_dead\n","])\n","\n","val_df = val_df.sample(frac=1, random_state=1).reset_index(drop=True)\n","\n","test_df = pd.concat([\n","    test_df_squat,\n","    test_df_lunge,\n","    test_df_dead\n","])\n","\n","test_df = test_df.sample(frac=1, random_state=1).reset_index(drop=True)\n"]},{"cell_type":"markdown","metadata":{"id":"ArEquwo729de"},"source":["### `Video_Model`\n","- **Description**: A model that utilizes a pretrained VideoMAE model for video representation learning.\n","- **Methods**:\n","  - `forward(pixel_values, bool_masked_pos)`: Forward pass through the VideoMAE model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VDb-Go7SBJD_"},"outputs":[],"source":["class Video_Model(nn.Module):\n","    \"\"\"\n","    Video_Model is a PyTorch neural network module that leverages a pretrained\n","    VideoMAE model for video representation learning.\n","\n","    Attributes:\n","        videomae (AutoModelForPreTraining): Pretrained VideoMAE model.\n","        pooling (nn.AdaptiveAvgPool1d): Layer for mean pooling of patch embeddings.\n","        fc1, fc2, fc3 (nn.Linear): Fully connected layers for additional feature extraction.\n","        relu1, relu2, relu3 (nn.ReLU): ReLU activation functions.\n","\n","    Methods:\n","        forward(pixel_values, bool_masked_pos): Performs the forward pass using pixel values.\n","    \"\"\"\n","    def __init__(self, pretrained_model_name=\"MCG-NJU/videomae-base\", hidden_size=2048):\n","        super(Video_Model, self).__init__()\n","\n","        # Load the pretrained VideoMAE model\n","        self.videomae = AutoModelForPreTraining.from_pretrained(pretrained_model_name)\n","\n","        # Pooling layer to aggregate the patch embeddings (mean pooling)\n","        self.pooling = nn.AdaptiveAvgPool1d(1)  # Converts [batch_size, 782, 1536] -> [batch_size, 1536]\n","\n","        # MLP layers with more depth (no final layer)\n","        self.fc1 = nn.Linear(1536, hidden_size)\n","        self.relu1 = nn.ReLU()\n","\n","        self.fc2 = nn.Linear(hidden_size, hidden_size)\n","        self.relu2 = nn.ReLU()\n","\n","        self.fc3 = nn.Linear(hidden_size, hidden_size)\n","        self.relu3 = nn.ReLU()\n","\n","    def forward(self, pixel_values, bool_masked_pos=None):\n","        # Forward pass through the pretrained VideoMAE model\n","        outputs = self.videomae(pixel_values, bool_masked_pos=bool_masked_pos)\n","\n","        # Get the output embeddings [batch_size, 782, 1536]\n","        embeddings = outputs[1]\n","\n","        # Apply pooling to get [batch_size, 1536]\n","        pooled_embeddings = self.pooling(embeddings.permute(0, 2, 1)).squeeze(-1)\n","\n","        # Pass through the deeper MLP\n","        x = self.fc1(pooled_embeddings)\n","        x = self.relu1(x)\n","\n","        x = self.fc2(x)\n","        x = self.relu2(x)\n","\n","        x = self.fc3(x)\n","        x = self.relu3(x)\n","\n","        # Return the final feature vector instead of predictions\n","        return x  # Shape: [batch_size, hidden_size]"]},{"cell_type":"markdown","metadata":{"id":"TWS-aEv93AO-"},"source":["### `Pose_Model`\n","- **Description**: A model that processes pose landmarks to output a feature vector.\n","- **Methods**:\n","  - `forward(pose_landmarks)`: Forward pass through the Pose model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fjTdgCRKCjfX"},"outputs":[],"source":["class Pose_Model(nn.Module):\n","    \"\"\"\n","    Pose_Model is a PyTorch neural network module designed to process pose landmarks\n","    and output a feature vector for further analysis.\n","    \"\"\"\n","\n","    def __init__(self, input_size=33 * 3, hidden_size1=512, hidden_size2=1024, hidden_size3=256, final_size=2048, dropout_rate=0.5):\n","        super(Pose_Model, self).__init__()\n","\n","        # Define the neural network layers\n","        self.fc1 = nn.Linear(input_size, hidden_size1)  # First hidden layer with reduced size\n","        self.bn1 = nn.BatchNorm1d(hidden_size1)  # Batch Normalization\n","        self.relu1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(dropout_rate)  # Dropout for regularization\n","\n","        self.fc2 = nn.Linear(hidden_size1, hidden_size2)  # Second hidden layer with increased size\n","        self.bn2 = nn.BatchNorm1d(hidden_size2)  # Batch Normalization\n","        self.relu2 = nn.LeakyReLU(0.2)  # Leaky ReLU activation\n","        self.dropout2 = nn.Dropout(dropout_rate)  # Dropout for regularization\n","\n","        self.fc3 = nn.Linear(hidden_size2, hidden_size3)  # Third hidden layer with reduced size\n","        self.bn3 = nn.BatchNorm1d(hidden_size3)  # Batch Normalization\n","        self.relu3 = nn.ELU()  # ELU activation\n","        self.dropout3 = nn.Dropout(dropout_rate)  # Dropout for regularization\n","\n","        self.fc4 = nn.Linear(hidden_size3, final_size)  # Output layer\n","        self.relu4 = nn.ReLU()\n","\n","    def forward(self, pose_landmarks):\n","        \"\"\"\n","        Forward pass through the Pose Model.\n","\n","        pose_landmarks: Tensor of shape [batch_size, num_frames, 33, 3]\n","        \"\"\"\n","        try:\n","            batch_size, num_frames, _, _ = pose_landmarks.shape\n","\n","            # Flatten the pose landmarks for each frame: [batch_size, num_frames, 33*3]\n","            pose_landmarks = pose_landmarks.view(batch_size, num_frames, -1)\n","\n","            # Process all frames at once\n","            x = self.fc1(pose_landmarks.view(-1, pose_landmarks.size(-1)))  # Shape: [batch_size*num_frames, hidden_size1]\n","            x = self.bn1(x)  # Batch Normalization\n","            x = self.relu1(x)\n","            x = self.dropout1(x)  # Apply Dropout\n","\n","            x = self.fc2(x)\n","            x = self.bn2(x)  # Batch Normalization\n","            x = self.relu2(x)\n","            x = self.dropout2(x)  # Apply Dropout\n","\n","            x = self.fc3(x)\n","            x = self.bn3(x)  # Batch Normalization\n","            x = self.relu3(x)\n","            x = self.dropout3(x)  # Apply Dropout\n","\n","            x = self.fc4(x)  # Final layer without additional activation (could add if needed)\n","            x = self.relu4(x)\n","\n","            # Reshape back to [batch_size, num_frames, final_size]\n","            frame_features = x.view(batch_size, num_frames, -1)  # Shape: [batch_size, num_frames, final_size]\n","\n","            # Aggregate the frame features (mean pooling)\n","            pooled_features = frame_features.mean(dim=1)  # Shape: [batch_size, final_size]\n","        except Exception as e:\n","            print(\"An error occurred during bala:\")\n","            print(f\"Error: {e}\")\n","        return pooled_features  # Shape: [batch_size, final_size]"]},{"cell_type":"markdown","metadata":{"id":"6UXT9pOQ3CLt"},"source":["### `Combined_Video_Pose_Model`\n","- **Description**: Combines the outputs of `Video_Model` and `Pose_Model` for comprehensive predictions.\n","- **Methods**:\n","  - `forward(pixel_values, bool_masked_pos, pose_landmarks)`: Combines features from video and pose models.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BrwwwfUoDvLl"},"outputs":[],"source":["class Combined_Video_Pose_Model(nn.Module):\n","    \"\"\"\n","    Combined_Video_Pose_Model integrates both video and pose models to provide\n","    a comprehensive hidden representation.\n","    \"\"\"\n","\n","    def __init__(self, video_hidden_size=2048, pose_hidden_size=2048, combined_hidden_size=1024, hidden_layer_size=2048, dropout_rate=0.5):\n","        super(Combined_Video_Pose_Model, self).__init__()\n","\n","        # Video model (returns a 2048-dimensional feature vector)\n","        self.video_model = Video_Model(hidden_size=video_hidden_size)\n","\n","        # Pose model (returns a 2048-dimensional feature vector)\n","        self.pose_model = Pose_Model(final_size=pose_hidden_size)\n","\n","        # Separate processing for video features\n","        self.video_fc1 = nn.Linear(video_hidden_size, combined_hidden_size)\n","        self.video_relu1 = nn.ReLU()\n","        self.video_dropout1 = nn.Dropout(dropout_rate)\n","\n","        # Separate processing for pose features\n","        self.pose_fc1 = nn.Linear(pose_hidden_size, combined_hidden_size)\n","        self.pose_relu1 = nn.ReLU()\n","        self.pose_dropout1 = nn.Dropout(dropout_rate)\n","\n","        # Combine the features from the two models\n","        combined_input_size = 2 * combined_hidden_size  # Combined input from both models\n","\n","        # Fully connected layers for the combined model\n","        self.fc1 = nn.Linear(combined_input_size, hidden_layer_size)  # Input should be 4096\n","        self.relu1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(dropout_rate)\n","\n","        self.fc2 = nn.Linear(hidden_layer_size, combined_hidden_size)  # Output layer to maintain original architecture\n","        self.relu2 = nn.ReLU()\n","\n","    def forward(self, pixel_values, bool_masked_pos, pose_landmarks):\n","        \"\"\"\n","        Forward pass through the combined model.\n","        Args:\n","            pixel_values: Input to the Video Model.\n","            bool_masked_pos: Masking input to the Video Model.\n","            pose_landmarks: Input to the Pose Model.\n","\n","        Returns:\n","            x: The hidden representation of size 1024.\n","        \"\"\"\n","        # Ensure to log the input shapes for debuggi\n","\n","        # Forward pass through the video model\n","        video_features = self.video_model(pixel_values, bool_masked_pos=bool_masked_pos)\n","        if video_features is None:\n","            raise ValueError(\"video_model returned None.\")\n","\n","        # Process video features separately\n","        video_processed = self.video_fc1(video_features)\n","        video_processed = self.video_relu1(video_processed)\n","        video_processed = self.video_dropout1(video_processed)\n","\n","        # Forward pass through the pose model\n","        pose_features = self.pose_model(pose_landmarks)\n","        if pose_features is None:\n","            raise ValueError(\"pose_model returned None.\")\n","\n","        # Process pose features separately\n","        pose_processed = self.pose_fc1(pose_features)\n","        pose_processed = self.pose_relu1(pose_processed)\n","        pose_processed = self.pose_dropout1(pose_processed)\n","\n","        # Concatenate the processed feature vectors from both models\n","        combined_features = torch.cat((video_processed, pose_processed), dim=1)  # Shape should be [batch_size, 4096]\n","\n","        # Pass through the combined MLP\n","        x = self.fc1(combined_features)\n","        x = self.relu1(x)\n","        x = self.dropout1(x)\n","\n","        # Pass through the second fully connected layer\n","        x = self.fc2(x)\n","        x = self.relu2(x)\n","        return x  # Return hidden layer output only (Shape: [batch_size, combined_hidden_size])\n"]},{"cell_type":"markdown","metadata":{"id":"1Y8h4fUACDZm"},"source":["## `Dual_Combined_Model`\n","\n","- **Description**: Combines two instances of `Combined_Video_Pose_Model` to produce classification and rating outputs.\n","\n","- **Methods**:\n","  - `forward(pixel_values_1, bool_masked_pos_1, pose_landmarks_tensor_1, pixel_values_2, bool_masked_pos_2, pose_landmarks_tensor_2)`: Forward pass through the dual model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HufVACb4BVN"},"outputs":[],"source":["class Dual_Combined_Model(nn.Module):\n","    \"\"\"\n","    Dual_Combined_Model combines two instances of Combined_Video_Pose_Model\n","    to produce classification and rating outputs.\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(Dual_Combined_Model, self).__init__()\n","\n","        # Initialize two instances of Combined_Video_Pose_Model\n","        self.model_1 = Combined_Video_Pose_Model()\n","        self.model_2 = Combined_Video_Pose_Model()\n","\n","        # Fully connected layers for classification\n","        self.classification_layer = nn.Linear(2048, 512)  # Initial layer after concatenation\n","        self.classification_relu = nn.ReLU()\n","        self.classification_dropout = nn.Dropout(0.5)\n","\n","        self.classification_output = nn.Linear(512, 3)  # Output layer for 3 classes\n","\n","    def forward(self, pixel_values_1, bool_masked_pos_1, pose_landmarks_tensor_1,\n","                pixel_values_2, bool_masked_pos_2, pose_landmarks_tensor_2):\n","        try:\n","            # Forward pass through the first combined model\n","            hidden_output_1 = self.model_1(pixel_values_1, bool_masked_pos_1, pose_landmarks_tensor_1)\n","\n","            # Forward pass through the second combined model\n","            hidden_output_2 = self.model_2(pixel_values_2, bool_masked_pos_2, pose_landmarks_tensor_2)\n","\n","            # Check for None outputs\n","            if hidden_output_1 is None or hidden_output_2 is None:\n","                raise ValueError(\"One of the models returned None.\")\n","\n","            # Concatenate the hidden outputs from both models\n","            combined_hidden = torch.cat((hidden_output_1, hidden_output_2), dim=1)  # Shape: [batch_size, 4096]\n","\n","            # Classification path\n","            classification_hidden = self.classification_layer(combined_hidden)\n","            classification_hidden = self.classification_relu(classification_hidden)\n","            classification_hidden = self.classification_dropout(classification_hidden)\n","\n","            classification_output = self.classification_output(classification_hidden)  # Shape: [batch_size, 3]\n","\n","            return classification_output, combined_hidden    # Return outputs\n","\n","        except Exception as e:\n","            print(\"An error occurred during forward pass:\")\n","            print(f\"Error: {e}\")\n","            raise  # Reraise the exception to maintain the stack trace\n"]},{"cell_type":"markdown","metadata":{"id":"VHyY8adi9rN6"},"source":["## `CriteriaPredictionModel`\n","\n","- **Description**: A deep neural network for predicting multiple binary ratings (yes/no) using a series of fully connected layers, ReLU activations, and dropout for regularization. The model outputs a probability for each rating.\n","\n","- **Methods**:\n","  - `forward(x)`: Forward pass through the network. The input is passed through five fully connected layers, each with ReLU and dropout, and the final output is processed by a sigmoid activation to produce binary classification probabilities.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQAWzMLI5U_E"},"outputs":[],"source":["class CriteriaPredictionModel(nn.Module):\n","    \"\"\"\n","    RatingPredictionModel predicts multiple yes/no ratings with a deeper network architecture.\n","    The model uses binary classification for each output rating (5 in total).\n","    \"\"\"\n","    def __init__(self, input_size=4096, hidden_size1=2048, hidden_size2=1024, output_size=5, dropout_rate=0.5):\n","        super(CriteriaPredictionModel, self).__init__()\n","\n","        # First fully connected layer\n","        self.fc1 = nn.Linear(input_size, hidden_size1)\n","        self.relu1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(dropout_rate)\n","\n","        # Second fully connected layer\n","        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n","        self.relu2 = nn.ReLU()\n","        self.dropout2 = nn.Dropout(dropout_rate)\n","\n","        # Adding more depth\n","        self.fc3 = nn.Linear(hidden_size2, hidden_size2 // 2)\n","        self.relu3 = nn.ReLU()\n","        self.dropout3 = nn.Dropout(dropout_rate)\n","\n","        self.fc4 = nn.Linear(hidden_size2 // 2, hidden_size2 // 4)\n","        self.relu4 = nn.ReLU()\n","        self.dropout4 = nn.Dropout(dropout_rate)\n","\n","        self.fc5 = nn.Linear(hidden_size2 // 4, hidden_size2 // 8)\n","        self.relu5 = nn.ReLU()\n","        self.dropout5 = nn.Dropout(dropout_rate)\n","\n","        # Output layer for binary classification (yes/no for each of the 5 ratings)\n","        self.output_layer = nn.Linear(hidden_size2 // 8, output_size)\n","        self.sigmoid = nn.Sigmoid()  # Sigmoid for binary yes/no predictions\n","\n","    def forward(self, x):\n","        # Forward pass through the network with multiple layers\n","        x = self.fc1(x)\n","        x = self.relu1(x)\n","        x = self.dropout1(x)\n","\n","        x = self.fc2(x)\n","        x = self.relu2(x)\n","        x = self.dropout2(x)\n","\n","        x = self.fc3(x)\n","        x = self.relu3(x)\n","        x = self.dropout3(x)\n","\n","        x = self.fc4(x)\n","        x = self.relu4(x)\n","        x = self.dropout4(x)\n","\n","        x = self.fc5(x)\n","        x = self.relu5(x)\n","        x = self.dropout5(x)\n","\n","        # Final binary classification\n","        ratings = self.output_layer(x)\n","        return self.sigmoid(ratings)  # Returns 5 values, each in range [0, 1] for yes/no classification\n"]},{"cell_type":"markdown","metadata":{"id":"PytDrrlz9xq7"},"source":["## `DeepClassificationWithRatingModel`\n","\n","- **Description**: The `DeepClassificationWithRatingModel` integrates the `DeepDualCombinedModel` and adds a separate rating prediction model. If the classification output predicts class `0`, the rating model is triggered. It uses different criteria models for deadlift, squat, and lunges based on the predicted class.\n","\n","- **Methods**:\n","  - `forward(combined_hidden, predicted_class)`:\n","    - Takes the `combined_hidden` state and the `predicted_class`.\n","    - If class `0` is predicted, the `dead_criteria_model` is used to predict ratings.\n","    - If class `1` is predicted, the `squat_criteria_model` is used.\n","    - If class `2` is predicted, the `lunges_criteria_model` is used.\n","    - Returns the predicted ratings based on the class.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xisbxmLc5U_E"},"outputs":[],"source":["class DeepClassificationWithRatingModel(nn.Module):\n","    \"\"\"\n","    DeepClassificationWithRatingModel integrates the DeepDualCombinedModel and\n","    adds a separate rating prediction model. If the classification output predicts class `0`,\n","    the rating model is triggered.\n","    \"\"\"\n","    def __init__(self):\n","        super(DeepClassificationWithRatingModel, self).__init__()\n","\n","        # Rating model with more layers\n","        self.dead_criteria_model = CriteriaPredictionModel(input_size=2048, hidden_size1=2048, hidden_size2=1024, output_size=5, dropout_rate=0.5)\n","        self.lunges_criteria_model = CriteriaPredictionModel(input_size=2048, hidden_size1=2048, hidden_size2=1024, output_size=7, dropout_rate=0.5)\n","        self.squat_criteria_model = CriteriaPredictionModel(input_size=2048, hidden_size1=2048, hidden_size2=1024, output_size=6, dropout_rate=0.5)\n","\n","    def forward(self,combined_hidden,predicted_class):\n","        try:\n","            # Initialize ratings as None\n","            ratings = None\n","\n","            # If class `0` is predicted, trigger rating prediction\n","            if predicted_class == 0:\n","                ratings = self.dead_criteria_model(combined_hidden)\n","            elif predicted_class == 1:\n","                ratings = self.squat_criteria_model(combined_hidden)\n","            elif predicted_class == 2:\n","                ratings = self.lunges_criteria_model(combined_hidden)\n","            return ratings\n","\n","        except Exception as e:\n","            print(f\"Error during forward pass in DeepClassificationWithRatingModel: {e}\")\n","            raise"]},{"cell_type":"markdown","metadata":{"id":"KmiORcTVCe8E"},"source":["## `PoseVideoDataset`\n","\n","- **Description**: A custom PyTorch Dataset designed to load video frames and pose landmarks, along with action class labels and ratings. It supports deadlift, squat, and lunge actions with different rating models for each. The dataset processes video frames, extracts pose landmarks, and normalizes ratings based on action class.\n","\n","- **Methods**:\n","  - `__len__()`: Returns the number of samples in the dataset.\n","  - `__getitem__(idx)`: Loads and processes the data at the specified index:\n","    - Loads video frames for both frontal and lateral views.\n","    - Processes and normalizes video frames using the processor.\n","    - Extracts pose landmarks and action class labels.\n","    - Retrieves and normalizes ratings based on action class (Deadlift, Squat, or Lunge).\n","  - `_process_ratings(df)`: Processes and normalizes the ratings data from the corresponding DataFrame:\n","    - Extracts relevant columns (ending in 'F' or 'L').\n","    - Normalizes the scores and applies a threshold (0 or 1 based on the condition).\n","    - Returns the mean of the scores for each rating.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzjcU07Q9FLq"},"outputs":[],"source":["\n","class PoseVideoDataset(Dataset):\n","    \"\"\"\n","    Custom PyTorch Dataset to load video frames and pose landmarks, along with class and ratings labels.\n","\n","    Args:\n","        df (pd.DataFrame): A DataFrame containing the dataset information, including video paths and labels.\n","        num_frames (int): Number of frames to load per video.\n","        processor: A pre-processing function to resize and normalize video frames.\n","        train_df_dead (pd.DataFrame): DataFrame containing ratings for deadlifts.\n","        train_df_squat (pd.DataFrame): DataFrame containing ratings for squats.\n","        train_df_lunge (pd.DataFrame): DataFrame containing ratings for lunges.\n","\n","    Returns:\n","        A tuple containing the processed video frames, pose landmarks, labels, and ratings.\n","    \"\"\"\n","\n","    def __init__(self, df, num_frames, processor):\n","        self.df = df\n","        self.num_frames = num_frames\n","        self.processor = processor\n","        self.train_df_dead = train_df_dead\n","        self.train_df_squat = train_df_squat\n","        self.train_df_lunge = train_df_lunge\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","\n","        # Get frontal and lateral video paths and index\n","        num_video_frontal = row['Num Video Frontal']\n","        num_video_lateral = row['Num Video Lateral']\n","        num_idx = row['NumIdx']\n","        action = row['Action']\n","\n","        # Load video frames and extract pose landmarks\n","        video_frames_frontal = load_and_resize_frames(num_video_frontal, action, 1, num_idx, self.num_frames)\n","        video_frames_lateral = load_and_resize_frames(num_video_lateral, action, 0, num_idx, self.num_frames)\n","\n","        # Process the video frames to get pixel values\n","        pixel_values_frontal = self.processor(list(np.clip(np.array(video_frames_frontal), 0, 255)), return_tensors=\"pt\").pixel_values.squeeze(0)\n","        pixel_values_lateral = self.processor(list(np.clip(np.array(video_frames_lateral), 0, 255)), return_tensors=\"pt\").pixel_values.squeeze(0)\n","\n","        model = AutoModelForPreTraining.from_pretrained(\"MCG-NJU/videomae-base\")  # Load model for masking\n","        num_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\n","        seq_length = (16 // model.config.tubelet_size) * num_patches_per_frame\n","        bool_masked_pos_frontal = torch.randint(0, 2, (seq_length,)).bool()\n","        bool_masked_pos_lateral = torch.randint(0, 2, (seq_length,)).bool()\n","\n","        # Extract pose landmarks from video frames\n","        pose_landmarks_frontal = row['front_pose']\n","        pose_landmarks_lateral = row['lat_pose']\n","        pose_landmarks_tensor_frontal = torch.tensor(pose_landmarks_frontal).float()\n","        pose_landmarks_tensor_lateral = torch.tensor(pose_landmarks_lateral).float()\n","\n","        # Get labels (action class)\n","        label_class = torch.tensor(row['class'], dtype=torch.long)\n","\n","        # Initialize ratings\n","        ratings = None\n","\n","        # Check label_class and load appropriate DataFrame\n","        if label_class.item() == 0:  # Deadlift\n","            ratings = self._process_ratings(self.train_df_dead,row)\n","        elif label_class.item() == 1:  # Squat\n","            ratings = self._process_ratings(self.train_df_squat,row)\n","        elif label_class.item() == 2:  # Lunge\n","            ratings = self._process_ratings(self.train_df_lunge,row)\n","\n","        # Convert ratings to tensor\n","        ratings = torch.tensor(ratings, dtype=torch.float32) if ratings is not None else None\n","\n","        return (pixel_values_frontal, bool_masked_pos_frontal, pose_landmarks_tensor_frontal,\n","                pixel_values_lateral, bool_masked_pos_lateral, pose_landmarks_tensor_lateral,\n","                label_class, ratings)\n","\n","    def _process_ratings(self, df,row):\n","        \"\"\"\n","        Process the ratings DataFrame to extract and normalize scores.\n","\n","        Args:\n","            df (pd.DataFrame): DataFrame containing ratings for the specific action.\n","\n","        Returns:\n","            list: Normalized and thresholded scores.\n","        \"\"\"\n","        # Select columns ending with 'F' or 'L'\n","        relevant_columns = [col for col in df.columns if col.endswith('F') or col.endswith('L')]\n","\n","        # Extract ratings and normalize\n","        scores = row[relevant_columns].values\n","\n","\n","        # Apply threshold: Convert to 0 or 1 based on specific thresholding conditions\n","        thresholded_scores = np.where(scores >= 0.5, 1, 0)\n","\n","        return thresholded_scores.tolist()  # Return mean of the scores for each sample\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CBl_FBpQBa1e"},"outputs":[],"source":["def create_dataloader(df, num_frames, processor, batch_size=8, shuffle=True):\n","    \"\"\"\n","    Creates a PyTorch DataLoader from the PoseVideoDataset.\n","\n","    Args:\n","        df (pd.DataFrame): DataFrame containing the dataset information.\n","        num_frames (int): The number of frames to extract from each video.\n","        processor: A video frame pre-processing function.\n","        batch_size (int, optional): Batch size for the DataLoader. Defaults to 8.\n","        shuffle (bool, optional): Whether to shuffle the data. Defaults to True.\n","\n","    Returns:\n","        DataLoader: A PyTorch DataLoader for the dataset.\n","    \"\"\"\n","    dataset = PoseVideoDataset(df, num_frames, processor)\n","    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n"]},{"cell_type":"markdown","metadata":{"id":"NvW6aZGJ-QGS"},"source":["## `train_combined_model`\n","\n","- **Description**: This function trains both the classification model (`Dual_Combined_Model`) and the rating prediction model (`CriteriaPredictionModel`). It evaluates them after each epoch and saves the best-performing models based on validation loss. It also includes early stopping if no improvement is observed for a set number of epochs.\n","\n","- **Args**:\n","  - `model (torch.nn.Module)`: The classification model (e.g., `Dual_Combined_Model`).\n","  - `criteria_model (torch.nn.Module)`: The rating prediction model (e.g., `CriteriaPredictionModel`).\n","  - `train_dataloader (DataLoader)`: A DataLoader providing the training data.\n","  - `eval_dataloader (DataLoader)`: A DataLoader providing the evaluation data.\n","  - `epochs (int, optional)`: Number of epochs to train. Defaults to 1000.\n","  - `lr (float, optional)`: Learning rate for the optimizer. Defaults to 1e-4.\n","  - `device (str, optional)`: The device to train the model on ('cpu' or 'cuda'). Defaults to 'cpu'.\n","  - `clip_grad_norm (float, optional)`: Maximum norm for gradient clipping. Defaults to 1.0.\n","  - `patience (int, optional)`: Number of epochs with no improvement after which training will be stopped. Defaults to 5.\n","\n","- **Returns**:\n","  - None: The function prints the training progress, validation loss, classification accuracy, and rating accuracy during training.\n","\n","- **Steps**:\n","  1. **Model Initialization**: Moves both the classification and rating models to the specified device.\n","  2. **Optimizer Setup**: Uses the Adam optimizer for both models with the specified learning rate.\n","  3. **Loss Functions**: Defines loss functions for both classification (`CrossEntropyLoss`) and ratings (`BCEWithLogitsLoss`).\n","  4. **Training Loop**:\n","     - Performs a forward pass through the classification model and computes the classification loss.\n","     - Computes the rating prediction loss only if the predicted class matches the actual class.\n","     - Combines both the classification and rating losses and performs backpropagation.\n","  5. **Gradient Clipping**: Clips gradients to avoid exploding gradients during backpropagation.\n","  6. **Model Evaluation**: Evaluates the models after each epoch and prints the results.\n","  7. **Early Stopping**: Monitors validation loss and triggers early stopping if no improvement is observed for a specified number of epochs.\n","  8. **Model Checkpointing**: Saves the best-performing models based on validation loss.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PSZbVvGz-Itz"},"outputs":[],"source":["def train_combined_model(model, criteria_model, train_dataloader, eval_dataloader, epochs=1000, lr=1e-4,\n","                         device='cpu', clip_grad_norm=1.0, patience=5):\n","    \"\"\"\n","    Trains both the classification and rating prediction models, evaluates them after each epoch.\n","\n","    Args:\n","        model (torch.nn.Module): The classification model (Dual_Combined_Model).\n","        criteria_model (torch.nn.Module): The rating prediction model (RatingPredictionModel).\n","        train_dataloader (DataLoader): A DataLoader providing the training data.\n","        eval_dataloader (DataLoader): A DataLoader providing the evaluation data.\n","        epochs (int, optional): Number of epochs to train. Defaults to 1000.\n","        lr (float, optional): Learning rate for the optimizer. Defaults to 1e-4.\n","        device (str, optional): The device to train the model on ('cpu' or 'cuda'). Defaults to 'cpu'.\n","        clip_grad_norm (float, optional): Maximum norm for gradient clipping. Defaults to 1.0.\n","        patience (int, optional): Number of epochs with no improvement after which training will be stopped. Defaults to 5.\n","\n","    Returns:\n","        None: Prints the training progress, validation loss, classification accuracy, and rating accuracy.\n","    \"\"\"\n","    model.to(device)\n","    criteria_model.to(device)\n","\n","    # Set up optimizer and loss functions\n","    optimizer = optim.Adam(list(model.parameters()) + list(criteria_model.parameters()), lr=lr)\n","    criterion_class = nn.CrossEntropyLoss()  # Loss for classification task\n","    criterion_ratings = nn.BCEWithLogitsLoss()  # Loss for rating task (binary)\n","\n","    # Learning rate scheduler\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n","\n","    best_eval_loss = float('inf')\n","    patience_counter = 0\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        criteria_model.train()\n","        running_loss = 0.0\n","        a = 0\n","\n","        for batch in train_dataloader:\n","            # Unpack batch data and move to the specified device\n","            (pixel_values_frontal, bool_masked_pos_frontal, pose_landmarks_tensor_frontal,\n","             pixel_values_lateral, bool_masked_pos_lateral, pose_landmarks_tensor_lateral,\n","             label_class, ratings) = [tensor.to(device) for tensor in batch]\n","\n","            optimizer.zero_grad()\n","\n","            # Forward pass through the classification model\n","            classification_output, combined_hidden = model(\n","                pixel_values_frontal, bool_masked_pos_frontal, pose_landmarks_tensor_frontal,\n","                pixel_values_lateral, bool_masked_pos_lateral, pose_landmarks_tensor_lateral\n","            )\n","            _, predicted_class = torch.max(classification_output, 1)\n","\n","            # Loss for classification\n","            loss_class = criterion_class(classification_output, label_class)\n","            print(f\"real class : {label_class} and predicted class : {predicted_class}\")\n","            print(f\"loss class : {loss_class}\")\n","            # Forward pass through the rating prediction model\n","            # We need to trigger the correct rating model based on the predicted class\n","            for i in range(label_class.size(0)):  # Iterate over each sample in the batch\n","                actual_class = label_class[i].item()\n","                predicted_class_item = predicted_class[i].item()\n","                # Loss for ratings (if the predicted class matches the actual class)\n","                if predicted_class_item == actual_class:\n","                    if actual_class == 0:  # Deadlift\n","                        ratings_output = criteria_model.dead_criteria_model(combined_hidden[i])\n","                    elif actual_class == 1:  # Squat\n","                        ratings_output = criteria_model.squat_criteria_model(combined_hidden[i])\n","                    elif actual_class == 2:  # Lunges\n","                        ratings_output = criteria_model.lunges_criteria_model(combined_hidden[i])\n","\n","                    print(f\"ratings : {ratings[i]} and ratings_output : {ratings_output}\")\n","                    loss_ratings = criterion_ratings(ratings_output, ratings[i].float())\n","\n","                else:\n","                    # High loss if the predicted class doesn't match\n","                    loss_ratings = torch.tensor(100.0, device=device, requires_grad=True)\n","\n","                print(f\"loss ratings : {loss_ratings}\")\n","                # Combine classification and rating losses\n","                loss = loss_class + loss_ratings\n","\n","                # Backward pass and optimization\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(list(model.parameters()) + list(criteria_model.parameters()), clip_grad_norm)\n","                optimizer.step()\n","\n","                running_loss += loss.item()\n","                print(f\"{a} and data_size: {len(train_dataloader)} and epoch: {epoch + 1}\")\n","                a += 1\n","\n","        avg_loss = running_loss / len(train_dataloader)\n","        print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {avg_loss:.4f}\")\n","\n","        # Evaluate both models after each epoch\n","        eval_loss, eval_accuracy, eval_rating_accuracy = evaluate_combined_model(\n","            model, criteria_model, eval_dataloader, criterion_class, criterion_ratings, device\n","        )\n","        print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {eval_loss:.4f}, \"\n","              f\"Classification Accuracy: {eval_accuracy:.4f}, \"\n","              f\"Rating Accuracy (Deadlift): {eval_rating_accuracy['deadlift']:.4f}, \"\n","              f\"Rating Accuracy (Squat): {eval_rating_accuracy['squat']:.4f}, \"\n","              f\"Rating Accuracy (Lunges): {eval_rating_accuracy['lunges']:.4f}\")\n","\n","        # Scheduler step\n","        scheduler.step(eval_loss)\n","\n","        # Check for early stopping\n","        if eval_loss < best_eval_loss:\n","            best_eval_loss = eval_loss\n","            patience_counter = 0\n","            torch.save(model.state_dict(), f\"best_combined_model_epoch_{epoch + 1}.pt\")\n","            torch.save(criteria_model.state_dict(), f\"best_rating_model_epoch_{epoch + 1}.pt\")\n","            print(\"Model checkpoint saved.\")\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(\"Early stopping triggered.\")\n","                break\n","\n","    print(\"Training complete.\")"]},{"cell_type":"markdown","metadata":{"id":"-dusNp1b-Z1k"},"source":["## `evaluate_combined_model`\n","\n","- **Description**: This function evaluates both the classification and rating prediction models on the provided dataset. It computes the average evaluation loss, classification accuracy, and rating accuracies for different actions (deadlift, squat, lunges) using a given `DataLoader` and loss functions.\n","\n","- **Args**:\n","  - `classification_model (torch.nn.Module)`: The classification model (e.g., `Dual_Combined_Model`).\n","  - `criteria_model (torch.nn.Module)`: The rating prediction model (e.g., `CriteriaPredictionModel`).\n","  - `dataloader (DataLoader)`: A DataLoader providing the evaluation data.\n","  - `criterion_class (nn.Module)`: The loss function for the classification task.\n","  - `criterion_ratings (nn.Module)`: The loss function for the rating task.\n","  - `device (str)`: The device to perform evaluation on ('cpu' or 'cuda').\n","\n","- **Returns**:\n","  - `float`: The average evaluation loss across the dataset.\n","  - `dict`: A dictionary containing the classification accuracy and rating accuracies for each action (deadlift, squat, lunges).\n","\n","- **Steps**:\n","  1. **Set Evaluation Mode**: Sets the models (`classification_model` and `criteria_model`) to evaluation mode.\n","  2. **Initialize Metrics**: Initializes accumulators for loss, classification accuracy, and rating accuracy for each action.\n","  3. **Evaluation Loop**:\n","     - Performs a forward pass through the classification model to get predictions and computes classification loss.\n","     - For each sample in the batch, if the predicted class matches the actual class, it computes the rating accuracy for that action.\n","     - The rating predictions are compared with the ground truth ratings, and accuracy is calculated for each action (deadlift, squat, lunges).\n","  4. **Compute Average Metrics**: Computes the average classification loss, overall classification accuracy, and rating accuracy for each action.\n","  5. **Return Results**: Returns the average loss, classification accuracy, and rating accuracies.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hqEEcG285U_E"},"outputs":[],"source":["def evaluate_combined_model(classification_model, criteria_model, dataloader, criterion_class, criterion_ratings, device):\n","    \"\"\"\n","    Evaluates both the classification and rating models on the provided dataset.\n","\n","    Args:\n","        classification_model (torch.nn.Module): The classification model.\n","        criteria_model (torch.nn.Module): The rating prediction model.\n","        dataloader (DataLoader): A DataLoader providing the evaluation data.\n","        criterion_class (nn.Module): The loss function for classification.\n","        criterion_ratings (nn.Module): The loss function for ratings.\n","        device (str): The device to perform evaluation on ('cpu' or 'cuda').\n","\n","    Returns:\n","        float: Average evaluation loss.\n","        dict: Classification accuracy and rating accuracies for each action (deadlift, squat, lunges).\n","    \"\"\"\n","    classification_model.eval()\n","    criteria_model.eval()\n","    total_loss = 0.0\n","    correct_predictions_class = 0\n","    total_samples_class = 0\n","\n","    # Initialize accumulators for rating accuracy per feature\n","    rating_accumulators = {'deadlift': 0, 'squat': 0, 'lunges': 0}\n","    total_rating_samples = {'deadlift': 0, 'squat': 0, 'lunges': 0}\n","\n","    a =0\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            (pixel_values_frontal, bool_masked_pos_frontal, pose_landmarks_tensor_frontal,\n","             pixel_values_lateral, bool_masked_pos_lateral, pose_landmarks_tensor_lateral,\n","             label_class, ratings) = [tensor.to(device) for tensor in batch]\n","\n","            # Forward pass through the classification model\n","            classification_output, combined_hidden = classification_model(\n","                pixel_values_frontal, bool_masked_pos_frontal, pose_landmarks_tensor_frontal,\n","                pixel_values_lateral, bool_masked_pos_lateral, pose_landmarks_tensor_lateral\n","            )\n","            _, predicted_class = torch.max(classification_output, 1)\n","\n","            # Classification loss and accuracy\n","            loss_class = criterion_class(classification_output, label_class)\n","            total_loss += loss_class.item()\n","            correct_predictions_class += (predicted_class == label_class).sum().item()\n","            total_samples_class += label_class.size(0)\n","\n","            # Rating accuracy calculation per feature (only if predicted class matches actual class)\n","            for i in range(label_class.size(0)):  # Loop over each sample in the batch\n","                actual_class = label_class[i].item()\n","                predicted_class_item = predicted_class[i].item()\n","\n","                if actual_class == predicted_class_item:\n","                    if actual_class == 0:  # Deadlift\n","                        ratings_output = criteria_model.dead_criteria_model(combined_hidden[i])\n","                        predicted_ratings = torch.sigmoid(ratings_output) > 0.5\n","                        correct_ratings = (predicted_ratings == ratings[i].byte()).sum().item()\n","                        rating_accumulators['deadlift'] += correct_ratings\n","                        total_rating_samples['deadlift'] += ratings[i].numel()\n","                        print(f\"Predicted : {predicted_ratings}\")\n","                        print(f\"actual : {ratings[i].byte()}\")\n","                        print(correct_ratings)\n","                        print(f\"rating_accumulators : {rating_accumulators['deadlift']}\")\n","                        print(f\"total_rating_samples : {total_rating_samples['deadlift']}\")\n","                    elif actual_class == 1:  # Squat\n","                        ratings_output = criteria_model.squat_criteria_model(combined_hidden[i])\n","                        predicted_ratings = torch.sigmoid(ratings_output) > 0.5\n","                        correct_ratings = (predicted_ratings == ratings[i].byte()).sum().item()\n","                        rating_accumulators['squat'] += correct_ratings\n","                        total_rating_samples['squat'] += ratings[i].numel()\n","                        print(f\"Predicted : {predicted_ratings}\")\n","                        print(f\"actual : {ratings[i].byte()}\")\n","                        print(f\"correct_ratings : {correct_ratings}\")\n","                        print(f\"rating_accumulators : {rating_accumulators['squat']}\")\n","                        print(f\"total_rating_samples : {total_rating_samples['squat']}\")\n","                    elif actual_class == 2:  # Lunges\n","                        ratings_output = criteria_model.lunges_criteria_model(combined_hidden[i])\n","                        predicted_ratings = torch.sigmoid(ratings_output) > 0.5\n","                        correct_ratings = (predicted_ratings == ratings[i].byte()).sum().item()\n","                        rating_accumulators['lunges'] += correct_ratings\n","                        total_rating_samples['lunges'] += ratings[i].numel()\n","                        print(f\"Predicted : {predicted_ratings}\")\n","                        print(f\"actual : {ratings[i].byte()}\")\n","                        print(correct_ratings)\n","                        print(f\"rating_accumulators : {rating_accumulators['lunges']}\")\n","                        print(f\"total_rating_samples : {total_rating_samples['lunges']}\")\n","\n","\n","            print(f\"{a} , {len(dataloader)}\")\n","            a=a=1\n","\n","    # Calculate the average loss and classification accuracy\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy_class = correct_predictions_class / total_samples_class if total_samples_class > 0 else 0.0\n","    # Calculate rating accuracy for each action\n","    rating_accuracy = {\n","        'deadlift': rating_accumulators['deadlift'] / total_rating_samples['deadlift'] if total_rating_samples['deadlift'] > 0 else 0.0,\n","        'squat': rating_accumulators['squat'] / total_rating_samples['squat'] if total_rating_samples['squat'] > 0 else 0.0,\n","        'lunges': rating_accumulators['lunges'] / total_rating_samples['lunges'] if total_rating_samples['lunges'] > 0 else 0.0\n","    }\n","\n","    return avg_loss, accuracy_class, rating_accuracy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p7ro9pEnuXbR"},"outputs":[],"source":["processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n","\n","dataloader = create_dataloader(train_df, 16, processor, batch_size=1)\n","eval_dataloader = create_dataloader(val_df, 16, processor, batch_size=1)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8eOrHYUp5U_F","outputId":"124f83ed-b707-4752-a5d6-52b8ade6db48"},"outputs":[{"name":"stderr","output_type":"stream","text":["Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mAutoImageProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMCG-NJU/videomae-base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m create_dataloader(train_df, \u001b[38;5;241m16\u001b[39m, processor, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m create_dataloader(val_df, \u001b[38;5;241m16\u001b[39m, processor, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\transformers\\models\\auto\\image_processing_auto.py:370\u001b[0m, in \u001b[0;36mAutoImageProcessor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         image_processor_class\u001b[38;5;241m.\u001b[39mregister_for_auto_class()\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 370\u001b[0m         image_processor_class \u001b[38;5;241m=\u001b[39m \u001b[43mimage_processor_class_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_processor_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_processor_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Last try: we use the IMAGE_PROCESSOR_MAPPING.\u001b[39;00m\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\transformers\\models\\auto\\image_processing_auto.py:117\u001b[0m, in \u001b[0;36mimage_processor_class_from_name\u001b[1;34m(class_name)\u001b[0m\n\u001b[0;32m    115\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\transformers\\utils\\import_utils.py:1136\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1134\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1136\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1137\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\transformers\\utils\\import_utils.py:1146\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1149\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1150\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1151\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\transformers\\models\\videomae\\image_processing_videomae.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor, BatchFeature, get_size_dict\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     center_crop,\n\u001b[0;32m     24\u001b[0m     get_resize_output_image_size,\n\u001b[0;32m     25\u001b[0m     normalize,\n\u001b[0;32m     26\u001b[0m     rescale,\n\u001b[0;32m     27\u001b[0m     resize,\n\u001b[0;32m     28\u001b[0m     to_channel_dimension_format,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     IMAGENET_STANDARD_MEAN,\n\u001b[0;32m     32\u001b[0m     IMAGENET_STANDARD_STD,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     valid_images,\n\u001b[0;32m     39\u001b[0m )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorType, is_vision_available, logging\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\transformers\\image_transforms.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\tensorflow\\__init__.py:41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_six\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\tensorflow\\python\\__init__.py:49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# from tensorflow.python import keras\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_column_lib \u001b[38;5;28;01mas\u001b[39;00m feature_column\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# from tensorflow.python.layers import layers\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_lib.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import,line-too-long,wildcard-import,g-bad-import-order\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence_feature_column\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py:147\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse_tensor \u001b[38;5;28;01mas\u001b[39;00m sparse_tensor_lib\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n\u001b[1;32m--> 147\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_ops\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\tensorflow\\python\\layers\\base.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m division\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy_tf_layers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[0;32m     22\u001b[0m InputSpec \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mInputSpec\n\u001b[0;32m     24\u001b[0m keras_style_scope \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mkeras_style_scope\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# See b/110718070#comment18 for more details about this import.\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\tensorflow\\python\\keras\\models.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics \u001b[38;5;28;01mas\u001b[39;00m metrics_module\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizer_v1\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Built-in activation functions.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m advanced_activations\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize_keras_object\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize_keras_object\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\__init__.py:22\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Generic layers.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputLayer\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_spec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputSpec\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributed_training_utils\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_tensor\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node \u001b[38;5;28;01mas\u001b[39;00m node_module\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:67\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_snake_case  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_tensor_or_tensor_list  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math_ops\n","File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap_external>:941\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap_external>:1040\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Initialize the model\n","dual_combined_model = Dual_Combined_Model()\n","rating_model = DeepClassificationWithRatingModel()\n","\n","print(device)\n","\n","# Train the model\n","train_combined_model(dual_combined_model, rating_model, dataloader, eval_dataloader,\n","                     epochs=1000, lr=1e-4, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-28a40QuXbR","outputId":"075bd4b4-2922-4bd8-cc33-93d76e415e5d"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model_epoch_1.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m criteria_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_rating_model_epoch_1.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m dual_combined_model \u001b[38;5;241m=\u001b[39m \u001b[43mDual_Combined_Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m rating_model \u001b[38;5;241m=\u001b[39m DeepClassificationWithRatingModel()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the saved model weights\u001b[39;00m\n","Cell \u001b[1;32mIn[16], line 11\u001b[0m, in \u001b[0;36mDual_Combined_Model.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28msuper\u001b[39m(Dual_Combined_Model, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize two instances of Combined_Video_Pose_Model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_1 \u001b[38;5;241m=\u001b[39m \u001b[43mCombined_Video_Pose_Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_2 \u001b[38;5;241m=\u001b[39m Combined_Video_Pose_Model()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Fully connected layers for classification\u001b[39;00m\n","Cell \u001b[1;32mIn[15], line 11\u001b[0m, in \u001b[0;36mCombined_Video_Pose_Model.__init__\u001b[1;34m(self, video_hidden_size, pose_hidden_size, combined_hidden_size, hidden_layer_size, dropout_rate)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28msuper\u001b[39m(Combined_Video_Pose_Model, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Video model (returns a 2048-dimensional feature vector)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_model \u001b[38;5;241m=\u001b[39m \u001b[43mVideo_Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_hidden_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Pose model (returns a 2048-dimensional feature vector)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpose_model \u001b[38;5;241m=\u001b[39m Pose_Model(final_size\u001b[38;5;241m=\u001b[39mpose_hidden_size)\n","Cell \u001b[1;32mIn[13], line 19\u001b[0m, in \u001b[0;36mVideo_Model.__init__\u001b[1;34m(self, pretrained_model_name, hidden_size)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28msuper\u001b[39m(Video_Model, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Load the pretrained VideoMAE model\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideomae \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForPreTraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Pooling layer to aggregate the patch embeddings (mean pooling)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mAdaptiveAvgPool1d(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Converts [batch_size, 782, 1536] -> [batch_size, 1536]\u001b[39;00m\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    470\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    472\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    473\u001b[0m     )\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m )\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\transformers\\modeling_utils.py:2629\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2626\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(init_empty_weights())\n\u001b[0;32m   2628\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m-> 2629\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2631\u001b[0m \u001b[38;5;66;03m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[0;32m   2632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_keep_in_fp32_modules:\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:768\u001b[0m, in \u001b[0;36mVideoMAEForPreTraining.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_to_decoder \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mdecoder_hidden_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_token \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, config\u001b[38;5;241m.\u001b[39mdecoder_hidden_size))\n\u001b[1;32m--> 768\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_sinusoid_encoding_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideomae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_hidden_size\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m VideoMAEDecoder(config, num_patches\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideomae\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mnum_patches)\n\u001b[0;32m    774\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:114\u001b[0m, in \u001b[0;36mget_sinusoid_encoding_table\u001b[1;34m(n_position, d_hid)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_position_angle_vec\u001b[39m(position):\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [position \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (hid_j \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m d_hid) \u001b[38;5;28;01mfor\u001b[39;00m hid_j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d_hid)]\n\u001b[1;32m--> 114\u001b[0m sinusoid_table \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([get_position_angle_vec(pos_i) \u001b[38;5;28;01mfor\u001b[39;00m pos_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_position)])\n\u001b[0;32m    115\u001b[0m sinusoid_table[:, \u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msin(sinusoid_table[:, \u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# dim 2i\u001b[39;00m\n\u001b[0;32m    116\u001b[0m sinusoid_table[:, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcos(sinusoid_table[:, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# dim 2i+1\u001b[39;00m\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:114\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_position_angle_vec\u001b[39m(position):\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [position \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (hid_j \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m d_hid) \u001b[38;5;28;01mfor\u001b[39;00m hid_j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d_hid)]\n\u001b[1;32m--> 114\u001b[0m sinusoid_table \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mget_position_angle_vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_i\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m pos_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_position)])\n\u001b[0;32m    115\u001b[0m sinusoid_table[:, \u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msin(sinusoid_table[:, \u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# dim 2i\u001b[39;00m\n\u001b[0;32m    116\u001b[0m sinusoid_table[:, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcos(sinusoid_table[:, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# dim 2i+1\u001b[39;00m\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:112\u001b[0m, in \u001b[0;36mget_sinusoid_encoding_table.<locals>.get_position_angle_vec\u001b[1;34m(position)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_position_angle_vec\u001b[39m(position):\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [position \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (hid_j \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m d_hid) \u001b[38;5;28;01mfor\u001b[39;00m hid_j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d_hid)]\n","File \u001b[1;32md:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:112\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_position_angle_vec\u001b[39m(position):\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [position \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (hid_j \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m d_hid) \u001b[38;5;28;01mfor\u001b[39;00m hid_j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d_hid)]\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Define the path to the saved model weights\n","model_path = \"best_model_epoch_1.pt\"\n","criteria_model_path = \"best_rating_model_epoch_1.pt\"\n","\n","dual_combined_model = Dual_Combined_Model()\n","rating_model = DeepClassificationWithRatingModel()\n","\n","# Load the saved model weights\n","dual_combined_model.load_state_dict(torch.load(model_path))\n","rating_model.load_state_dict(torch.load(criteria_model_path))\n","\n","print(device)\n","\n","# Continue training the model\n","train_combined_model(dual_combined_model, rating_model, dataloader, eval_dataloader,\n","                     epochs=1000, lr=1e-4, device=device)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD7vpMS924a1"
   },
   "source": [
    "This Notebook contains implementations of neural network models that process video frames and pose landmarks to classify actions and predict ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fwXbWGQOyJDD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoImageProcessor, AutoModelForPreTraining\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FmdwF_C45U--",
    "outputId": "01dcd918-f449-45df-dc5e-e29054b2faee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch can use the GPU.\n",
      "Device name: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch can use the GPU.\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is using the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KdOB4Yjz50De",
    "outputId": "3889fa4e-aba3-4f6b-f899-b1e1766f94eb"
   },
   "outputs": [],
   "source": [
    "base_path = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsKfIuDB2zsP"
   },
   "source": [
    "## Functions\n",
    "\n",
    "### `load_and_resize_frames`\n",
    "- **Description**: Loads and resizes video frames from specified file paths.\n",
    "- **Parameters**:\n",
    "  - `num_video_frontal`: Identifier for the video file.\n",
    "  - `num_idx`: Index for the specific video segment.\n",
    "  - `num_frames`: Number of frames to load.\n",
    "  - `size`: Target size for resizing frames (default: (224, 224)).\n",
    "- **Returns**: List of resized frames.\n",
    "\n",
    "### `extract_pose_landmarks`\n",
    "- **Description**: Extracts pose landmarks from a list of video frames using MediaPipe.\n",
    "- **Parameters**:\n",
    "  - `video_frames`: List of frames (each of size [224, 224, 3]).\n",
    "- **Returns**: Array of pose landmarks with shape [num_frames, 33, 3].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "64pDhqby2ohg"
   },
   "outputs": [],
   "source": [
    "def load_and_resize_frames(num_video, action, frontORlat, num_idx, num_frames, size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Loads and resizes video frames from specified file paths based on the action type.\n",
    "\n",
    "    Args:\n",
    "        num_video (int): Identifier for the video file.\n",
    "        action (str): The action being performed ('deadlift', 'squat', 'lunges').\n",
    "        frontORlat (int): 1 for frontal frames, 0 for lateral frames.\n",
    "        num_idx (int): Index for the specific video segment.\n",
    "        num_frames (int): Number of frames to load.\n",
    "        size (tuple): Target size for resizing frames (default is (224, 224)).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of resized frames.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "\n",
    "    # Determine the folder paths based on the action\n",
    "    if action == 'Deadlift':\n",
    "        frontal_folder = base_path+'Deadlift_Frames'\n",
    "        lateral_folder =  base_path+'Deadlift_Frames'\n",
    "    elif action == 'Squat':\n",
    "        frontal_folder =  base_path+'Squat_Frames'\n",
    "        lateral_folder =  base_path+'Squat_Frames'\n",
    "    elif action == 'lunges':\n",
    "        frontal_folder =  base_path+'Lunges_Frames'\n",
    "        lateral_folder =  base_path+'Lunges_Frames'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown action: {action}\")\n",
    "\n",
    "    # Load the frames based on frontORlat flag\n",
    "    if frontORlat == 1:\n",
    "        folder = frontal_folder\n",
    "    else:\n",
    "        folder = lateral_folder\n",
    "\n",
    "    # Load and resize frames\n",
    "    for i in range(1, num_frames + 1):  # Loop from 1 to num_frames (e.g., 16)\n",
    "        path = f\"{folder}/{num_video}_idx_{num_idx}_{i}.jpg\"\n",
    "        img = cv2.imread(path)\n",
    "        if img is not None:\n",
    "            img_resized = cv2.resize(img, size)  # Resize to the specified size (default is 224x224)\n",
    "            frames.append(img_resized)\n",
    "        else:\n",
    "            print(f\"Warning: Could not load image at {path}\")\n",
    "\n",
    "    return frames\n",
    "\n",
    "def extract_pose_landmarks(video_frames):\n",
    "    \"\"\"\n",
    "    Extracts pose landmarks from a list of video frames using MediaPipe.\n",
    "\n",
    "    Args:\n",
    "        video_frames (list): List of frames (each of size [224, 224, 3]).\n",
    "\n",
    "    Returns:\n",
    "        np.array: Array of pose landmarks with shape [num_frames, 33, 3].\n",
    "    \"\"\"\n",
    "    pose_landmarks = []\n",
    "\n",
    "    for frame in video_frames:\n",
    "        # Ensure the frame is in the right format\n",
    "        if frame.shape != (224, 224, 3):\n",
    "            print(f\"Unexpected frame shape: {frame.shape}\")\n",
    "            continue\n",
    "        # Convert the frame from float64 [0, 1] to uint8 [0, 255]\n",
    "        frame_uint8 = (np.clip(frame * 255, 0, 255)).astype(np.uint8)  # Shape: [224, 224, 3]\n",
    "\n",
    "        # Extract pose landmarks\n",
    "        results = pose.process(frame_uint8)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            # Get the landmarks (33 landmarks, each with x, y, z)\n",
    "            landmarks = [[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark]\n",
    "        else:\n",
    "            # If no landmarks are detected, we use a zero vector\n",
    "            landmarks = np.zeros((33, 3))\n",
    "\n",
    "        pose_landmarks.append(landmarks)\n",
    "\n",
    "    return np.array(pose_landmarks)  # Shape: [num_frames, 33, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DNbu6JEg5U-_",
    "outputId": "2119f0de-5282-4087-e249-1a3845887f59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            front_pose  \\\n",
      "0    [[[0.49896547198295593, 0.24871453642845154, -...   \n",
      "1    [[[0.5140300393104553, 0.22558534145355225, -0...   \n",
      "2    [[[0.5227701663970947, 0.22155773639678955, -0...   \n",
      "3    [[[0.5229289531707764, 0.2611807584762573, -0....   \n",
      "4    [[[0.5273605585098267, 0.22662483155727386, -0...   \n",
      "..                                                 ...   \n",
      "290  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "291  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "292  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "293  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "294  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "\n",
      "                                              lat_pose  \n",
      "0    [[[0.48301321268081665, 0.2528548836708069, 0....  \n",
      "1    [[[0.5150753259658813, 0.25385501980781555, 0....  \n",
      "2    [[[0.523187518119812, 0.23705779016017914, 0.1...  \n",
      "3    [[[0.5000579953193665, 0.24061907827854156, 0....  \n",
      "4    [[[0.5319615006446838, 0.2524169683456421, 0.0...  \n",
      "..                                                 ...  \n",
      "290  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "291  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "292  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "293  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "294  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "\n",
      "[295 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_squat = pd.read_excel(base_path + 'squat_edited.xlsx')\n",
    "\n",
    "# Load JSON files and convert to NumPy arrays\n",
    "def load_json_as_numpy(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)  # Load the JSON data\n",
    "    return np.array(data)  # Convert the list to a NumPy array\n",
    "\n",
    "# Load the front and lateral poses\n",
    "front_pose_array = load_json_as_numpy(base_path + 'front_pose_squat.json')\n",
    "lat_pose_array = load_json_as_numpy(base_path + 'lat_pose_squat.json')\n",
    "\n",
    "# Make sure `front_pose` and `lat_pose` columns exist\n",
    "if 'front_pose' not in df_squat.columns:\n",
    "    df_squat['front_pose'] = None\n",
    "if 'lat_pose' not in df_squat.columns:\n",
    "    df_squat['lat_pose'] = None\n",
    "\n",
    "# Assign the loaded arrays back to the DataFrame\n",
    "# Make sure the lengths match\n",
    "if len(front_pose_array) == len(df_squat) and len(lat_pose_array) == len(df_squat):\n",
    "    df_squat['front_pose'] = list(front_pose_array)\n",
    "    df_squat['lat_pose'] = list(lat_pose_array)\n",
    "else:\n",
    "    print(\"Error: The length of the loaded arrays does not match the DataFrame.\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_squat[['front_pose', 'lat_pose']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "phgbHMZC5U_A"
   },
   "outputs": [],
   "source": [
    "# Define your split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "total_samples = len(df_squat)\n",
    "train_size = int(total_samples * train_ratio)\n",
    "val_size = int(total_samples * val_ratio)\n",
    "\n",
    "# Split the DataFrame\n",
    "train_df_squat = df_squat.iloc[:train_size]                    # First 70% for training\n",
    "val_df_squat = df_squat.iloc[train_size:train_size + val_size]  # Next 15% for validation\n",
    "test_df_squat = df_squat.iloc[train_size + val_size:]           # Last 15% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805
    },
    "id": "p6g0uol45U_B",
    "outputId": "5aa658a6-6992-405e-c119-af754fa4d441"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num Video Frontal</th>\n",
       "      <th>Num Video Lateral</th>\n",
       "      <th>NumIdx</th>\n",
       "      <th>Action</th>\n",
       "      <th>Feet Out 30 F</th>\n",
       "      <th>Score: Frontal( - / 1)</th>\n",
       "      <th>Whole Feet Flat On the Floor (1) L</th>\n",
       "      <th>Bend Hips and Knees Simultaniously (1) L</th>\n",
       "      <th>Hips backwards (1) L</th>\n",
       "      <th>Lower back neural (1) L</th>\n",
       "      <th>Hips are lower than knees level (1 point) L</th>\n",
       "      <th>Score: Lateral ( - / 5)</th>\n",
       "      <th>Total Score - /6</th>\n",
       "      <th>class</th>\n",
       "      <th>rating</th>\n",
       "      <th>front_pose</th>\n",
       "      <th>lat_pose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Squat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[[0.49896547198295593, 0.24871453642845154, -...</td>\n",
       "      <td>[[[0.48301321268081665, 0.2528548836708069, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Squat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[[[0.5140300393104553, 0.22558534145355225, -0...</td>\n",
       "      <td>[[[0.5150753259658813, 0.25385501980781555, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Squat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[[[0.5227701663970947, 0.22155773639678955, -0...</td>\n",
       "      <td>[[[0.523187518119812, 0.23705779016017914, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Squat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[[[0.5229289531707764, 0.2611807584762573, -0....</td>\n",
       "      <td>[[[0.5000579953193665, 0.24061907827854156, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Squat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[[[0.5273605585098267, 0.22662483155727386, -0...</td>\n",
       "      <td>[[[0.5319615006446838, 0.2524169683456421, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>Squat</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...</td>\n",
       "      <td>[[[0.3942027986049652, 0.23040449619293213, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>2</td>\n",
       "      <td>Squat</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[[0.4873969852924347, 0.1548309326171875, -0....</td>\n",
       "      <td>[[[0.4314277470111847, 0.22929038107395172, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>3</td>\n",
       "      <td>Squat</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[[0.4695884585380554, 0.15741464495658875, -0...</td>\n",
       "      <td>[[[0.39387422800064087, 0.2241220325231552, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>Squat</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[[0.47849878668785095, 0.20681476593017578, -...</td>\n",
       "      <td>[[[0.41384050250053406, 0.22573530673980713, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>87</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>Squat</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[[[0.43349573016166687, 0.1754622608423233, -0...</td>\n",
       "      <td>[[[0.3537933826446533, 0.18060621619224548, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Num Video Frontal  Num Video Lateral  NumIdx Action  Feet Out 30 F  \\\n",
       "0                    1                  2       0  Squat              0   \n",
       "1                    1                  2       1  Squat              0   \n",
       "2                    1                  2       2  Squat              0   \n",
       "3                    1                  2       3  Squat              0   \n",
       "4                    1                  2       4  Squat              0   \n",
       "..                 ...                ...     ...    ...            ...   \n",
       "201                 85                 86       1  Squat              1   \n",
       "202                 85                 86       2  Squat              1   \n",
       "203                 85                 86       3  Squat              1   \n",
       "204                 85                 86       4  Squat              1   \n",
       "205                 87                 88       0  Squat              1   \n",
       "\n",
       "     Score: Frontal( - / 1)  Whole Feet Flat On the Floor (1) L  \\\n",
       "0                         0                                   1   \n",
       "1                         0                                   0   \n",
       "2                         0                                   0   \n",
       "3                         0                                   0   \n",
       "4                         0                                   0   \n",
       "..                      ...                                 ...   \n",
       "201                       1                                   0   \n",
       "202                       1                                   0   \n",
       "203                       1                                   0   \n",
       "204                       1                                   0   \n",
       "205                       1                                   0   \n",
       "\n",
       "     Bend Hips and Knees Simultaniously (1) L  Hips backwards (1) L  \\\n",
       "0                                           1                     1   \n",
       "1                                           1                     1   \n",
       "2                                           1                     1   \n",
       "3                                           1                     1   \n",
       "4                                           1                     1   \n",
       "..                                        ...                   ...   \n",
       "201                                         1                     0   \n",
       "202                                         1                     0   \n",
       "203                                         1                     0   \n",
       "204                                         1                     0   \n",
       "205                                         1                     0   \n",
       "\n",
       "     Lower back neural (1) L  Hips are lower than knees level (1 point) L  \\\n",
       "0                          1                                            0   \n",
       "1                          1                                            0   \n",
       "2                          1                                            0   \n",
       "3                          1                                            0   \n",
       "4                          1                                            0   \n",
       "..                       ...                                          ...   \n",
       "201                        1                                            1   \n",
       "202                        1                                            1   \n",
       "203                        1                                            1   \n",
       "204                        1                                            1   \n",
       "205                        1                                            0   \n",
       "\n",
       "     Score: Lateral ( - / 5)  Total Score - /6  class  rating  \\\n",
       "0                          4                 4      1       4   \n",
       "1                          3                 3      1       3   \n",
       "2                          3                 3      1       3   \n",
       "3                          3                 3      1       3   \n",
       "4                          3                 3      1       3   \n",
       "..                       ...               ...    ...     ...   \n",
       "201                        3                 4      1       4   \n",
       "202                        3                 4      1       4   \n",
       "203                        3                 4      1       4   \n",
       "204                        3                 4      1       4   \n",
       "205                        2                 3      1       3   \n",
       "\n",
       "                                            front_pose  \\\n",
       "0    [[[0.49896547198295593, 0.24871453642845154, -...   \n",
       "1    [[[0.5140300393104553, 0.22558534145355225, -0...   \n",
       "2    [[[0.5227701663970947, 0.22155773639678955, -0...   \n",
       "3    [[[0.5229289531707764, 0.2611807584762573, -0....   \n",
       "4    [[[0.5273605585098267, 0.22662483155727386, -0...   \n",
       "..                                                 ...   \n",
       "201  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
       "202  [[[0.4873969852924347, 0.1548309326171875, -0....   \n",
       "203  [[[0.4695884585380554, 0.15741464495658875, -0...   \n",
       "204  [[[0.47849878668785095, 0.20681476593017578, -...   \n",
       "205  [[[0.43349573016166687, 0.1754622608423233, -0...   \n",
       "\n",
       "                                              lat_pose  \n",
       "0    [[[0.48301321268081665, 0.2528548836708069, 0....  \n",
       "1    [[[0.5150753259658813, 0.25385501980781555, 0....  \n",
       "2    [[[0.523187518119812, 0.23705779016017914, 0.1...  \n",
       "3    [[[0.5000579953193665, 0.24061907827854156, 0....  \n",
       "4    [[[0.5319615006446838, 0.2524169683456421, 0.0...  \n",
       "..                                                 ...  \n",
       "201  [[[0.3942027986049652, 0.23040449619293213, -0...  \n",
       "202  [[[0.4314277470111847, 0.22929038107395172, -0...  \n",
       "203  [[[0.39387422800064087, 0.2241220325231552, -0...  \n",
       "204  [[[0.41384050250053406, 0.22573530673980713, -...  \n",
       "205  [[[0.3537933826446533, 0.18060621619224548, -0...  \n",
       "\n",
       "[206 rows x 17 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_squat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cXvpCRM5U_B",
    "outputId": "644634f3-f7a9-4ecd-ada0-47ec9f8d097f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            front_pose  \\\n",
      "0    [[[0.6141521334648132, 0.2384583204984665, -0....   \n",
      "1    [[[0.7448023557662964, 0.29284247756004333, -0...   \n",
      "2    [[[0.6720446944236755, 0.26214927434921265, -1...   \n",
      "3    [[[0.7419819831848145, 0.2703408896923065, -0....   \n",
      "4    [[[0.7648568749427795, 0.3041684925556183, -1....   \n",
      "..                                                 ...   \n",
      "264  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "265  [[[0.6985482573509216, 0.3878808617591858, -0....   \n",
      "266  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "267  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "268  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "\n",
      "                                              lat_pose  \n",
      "0    [[[0.5667536854743958, 0.2648758590221405, -0....  \n",
      "1    [[[0.5568003058433533, 0.25841817259788513, 0....  \n",
      "2    [[[0.6044440865516663, 0.25434428453445435, -0...  \n",
      "3    [[[0.614557683467865, 0.25464609265327454, -0....  \n",
      "4    [[[0.591785728931427, 0.26665791869163513, 0.0...  \n",
      "..                                                 ...  \n",
      "264  [[[0.6106436848640442, 0.25392264127731323, -0...  \n",
      "265  [[[0.6022240519523621, 0.23920293152332306, -0...  \n",
      "266  [[[0.6188439130783081, 0.1534697264432907, -0....  \n",
      "267  [[[0.5743303894996643, 0.17498759925365448, -0...  \n",
      "268  [[[0.5832579731941223, 0.18293842673301697, -0...  \n",
      "\n",
      "[269 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_dead = pd.read_excel(base_path + 'deadlift_edited.xlsx')\n",
    "\n",
    "# Load JSON files and convert to NumPy arrays\n",
    "def load_json_as_numpy(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)  # Load the JSON data\n",
    "    return np.array(data)  # Convert the list to a NumPy array\n",
    "\n",
    "# Load the front and lateral poses\n",
    "front_pose_array = load_json_as_numpy(base_path + 'front_pose_dead.json')\n",
    "lat_pose_array = load_json_as_numpy(base_path + 'lat_pose_dead.json')\n",
    "\n",
    "# Make sure `front_pose` and `lat_pose` columns exist\n",
    "if 'front_pose' not in df_dead.columns:\n",
    "    df_dead['front_pose'] = None\n",
    "if 'lat_pose' not in df_dead.columns:\n",
    "    df_dead['lat_pose'] = None\n",
    "\n",
    "# Assign the loaded arrays back to the DataFrame\n",
    "# Make sure the lengths match\n",
    "if len(front_pose_array) == len(df_dead) and len(lat_pose_array) == len(df_dead):\n",
    "    df_dead['front_pose'] = list(front_pose_array)\n",
    "    df_dead['lat_pose'] = list(lat_pose_array)\n",
    "else:\n",
    "    print(\"Error: The length of the loaded arrays does not match the DataFrame.\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_dead[['front_pose', 'lat_pose']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "p7af0bK55U_B"
   },
   "outputs": [],
   "source": [
    "# Define your split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "total_samples = len(df_dead)\n",
    "train_size = int(total_samples * train_ratio)\n",
    "val_size = int(total_samples * val_ratio)\n",
    "\n",
    "# Split the DataFrame\n",
    "train_df_dead = df_dead.iloc[:train_size]                    # First 70% for training\n",
    "val_df_dead = df_dead.iloc[train_size:train_size + val_size]  # Next 15% for validation\n",
    "test_df_dead = df_dead.iloc[train_size + val_size:]           # Last 15% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-pXveAf5U_B",
    "outputId": "0ab18bab-bc66-453a-9998-77a8b4a42b21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            front_pose  \\\n",
      "0    [[[0.5307167768478394, 0.26830747723579407, -0...   \n",
      "1    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "2    [[[0.5470829606056213, 0.22924844920635223, -0...   \n",
      "3    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "4    [[[0.5109826922416687, 0.2688097357749939, -0....   \n",
      "..                                                 ...   \n",
      "101  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "102  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "103  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "104  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "105  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "\n",
      "                                              lat_pose  \n",
      "0    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "1    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "2    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "3    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "4    [[[0.3619050979614258, 0.2965300977230072, -0....  \n",
      "..                                                 ...  \n",
      "101  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "102  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "103  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "104  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "105  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "\n",
      "[106 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_lunge = pd.read_excel(base_path + 'lunges_edited.xlsx')\n",
    "\n",
    "# Load JSON files and convert to NumPy arrays\n",
    "def load_json_as_numpy(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)  # Load the JSON data\n",
    "    return np.array(data)  # Convert the list to a NumPy array\n",
    "\n",
    "# Load the front and lateral poses\n",
    "front_pose_array = load_json_as_numpy(base_path + 'front_pose_lunges.json')\n",
    "lat_pose_array = load_json_as_numpy(base_path + 'lat_pose_lunges.json')\n",
    "\n",
    "# Make sure `front_pose` and `lat_pose` columns exist\n",
    "if 'front_pose' not in df_lunge.columns:\n",
    "    df_lunge['front_pose'] = None\n",
    "if 'lat_pose' not in df_lunge.columns:\n",
    "    df_lunge['lat_pose'] = None\n",
    "\n",
    "# Assign the loaded arrays back to the DataFrame\n",
    "# Make sure the lengths match\n",
    "if len(front_pose_array) == len(df_lunge) and len(lat_pose_array) == len(df_lunge):\n",
    "    df_lunge['front_pose'] = list(front_pose_array)\n",
    "    df_lunge['lat_pose'] = list(lat_pose_array)\n",
    "else:\n",
    "    print(\"Error: The length of the loaded arrays does not match the DataFrame.\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_lunge[['front_pose', 'lat_pose']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wJSBgqjs5U_C"
   },
   "outputs": [],
   "source": [
    "# Define your split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "total_samples = len(df_lunge)\n",
    "train_size = int(total_samples * train_ratio)\n",
    "val_size = int(total_samples * val_ratio)\n",
    "\n",
    "# Split the DataFrame\n",
    "train_df_lunge = df_lunge.iloc[:train_size]                    # First 70% for training\n",
    "val_df_lunge = df_lunge.iloc[train_size:train_size + val_size]  # Next 15% for validation\n",
    "test_df_lunge = df_lunge.iloc[train_size + val_size:]           # Last 15% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kHrmZE3a5U_C"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Keep only the specified columns and concatenate the DataFrames\n",
    "train_df = pd.concat([\n",
    "    train_df_squat,\n",
    "    train_df_lunge,\n",
    "    train_df_dead\n",
    "])\n",
    "\n",
    "train_df = train_df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "val_df = pd.concat([\n",
    "    val_df_squat,\n",
    "    val_df_lunge,\n",
    "    val_df_dead\n",
    "])\n",
    "\n",
    "val_df = val_df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "test_df = pd.concat([\n",
    "    test_df_squat,\n",
    "    test_df_lunge,\n",
    "    test_df_dead\n",
    "])\n",
    "\n",
    "test_df = test_df.sample(frac=1, random_state=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArEquwo729de"
   },
   "source": [
    "### `Video_Model`\n",
    "- **Description**: A model that utilizes a pretrained VideoMAE model for video representation learning.\n",
    "- **Methods**:\n",
    "  - `forward(pixel_values, bool_masked_pos)`: Forward pass through the VideoMAE model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VDb-Go7SBJD_"
   },
   "outputs": [],
   "source": [
    "class Video_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Video_Model is a PyTorch neural network module that leverages a pretrained\n",
    "    VideoMAE model for video representation learning.\n",
    "\n",
    "    Attributes:\n",
    "        videomae (AutoModelForPreTraining): Pretrained VideoMAE model.\n",
    "        pooling (nn.AdaptiveAvgPool1d): Layer for mean pooling of patch embeddings.\n",
    "        fc1, fc2, fc3 (nn.Linear): Fully connected layers for additional feature extraction.\n",
    "        relu1, relu2, relu3 (nn.ReLU): ReLU activation functions.\n",
    "\n",
    "    Methods:\n",
    "        forward(pixel_values, bool_masked_pos): Performs the forward pass using pixel values.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model_name=\"MCG-NJU/videomae-base\", hidden_size=2048):\n",
    "        super(Video_Model, self).__init__()\n",
    "\n",
    "        # Load the pretrained VideoMAE model\n",
    "        self.videomae = AutoModelForPreTraining.from_pretrained(pretrained_model_name)\n",
    "\n",
    "        # Pooling layer to aggregate the patch embeddings (mean pooling)\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)  # Converts [batch_size, 782, 1536] -> [batch_size, 1536]\n",
    "\n",
    "        # MLP layers with more depth (no final layer)\n",
    "        self.fc1 = nn.Linear(1536, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, pixel_values, bool_masked_pos=None):\n",
    "        # Forward pass through the pretrained VideoMAE model\n",
    "        outputs = self.videomae(pixel_values, bool_masked_pos=bool_masked_pos)\n",
    "\n",
    "        # Get the output embeddings [batch_size, 782, 1536]\n",
    "        embeddings = outputs[1]\n",
    "\n",
    "        # Apply pooling to get [batch_size, 1536]\n",
    "        pooled_embeddings = self.pooling(embeddings.permute(0, 2, 1)).squeeze(-1)\n",
    "\n",
    "        # Pass through the deeper MLP\n",
    "        x = self.fc1(pooled_embeddings)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        # Return the final feature vector instead of predictions\n",
    "        return x  # Shape: [batch_size, hidden_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWS-aEv93AO-"
   },
   "source": [
    "### `Pose_Model`\n",
    "- **Description**: A model that processes pose landmarks to output a feature vector.\n",
    "- **Methods**:\n",
    "  - `forward(pose_landmarks)`: Forward pass through the Pose model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fjTdgCRKCjfX"
   },
   "outputs": [],
   "source": [
    "class Pose_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Pose_Model is a PyTorch neural network module designed to process pose landmarks\n",
    "    and output a feature vector for further analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size=33 * 3, hidden_size1=512, hidden_size2=1024, hidden_size3=256, final_size=2048, dropout_rate=0.5):\n",
    "        super(Pose_Model, self).__init__()\n",
    "\n",
    "        # Define the neural network layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)  # First hidden layer with reduced size\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size1)  # Batch Normalization\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)  # Dropout for regularization\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)  # Second hidden layer with increased size\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size2)  # Batch Normalization\n",
    "        self.relu2 = nn.LeakyReLU(0.2)  # Leaky ReLU activation\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)  # Dropout for regularization\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)  # Third hidden layer with reduced size\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size3)  # Batch Normalization\n",
    "        self.relu3 = nn.ELU()  # ELU activation\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)  # Dropout for regularization\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_size3, final_size)  # Output layer\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "    def forward(self, pose_landmarks):\n",
    "        \"\"\"\n",
    "        Forward pass through the Pose Model.\n",
    "\n",
    "        pose_landmarks: Tensor of shape [batch_size, num_frames, 33, 3]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            batch_size, num_frames, _, _ = pose_landmarks.shape\n",
    "\n",
    "            # Flatten the pose landmarks for each frame: [batch_size, num_frames, 33*3]\n",
    "            pose_landmarks = pose_landmarks.view(batch_size, num_frames, -1)\n",
    "\n",
    "            # Process all frames at once\n",
    "            x = self.fc1(pose_landmarks.view(-1, pose_landmarks.size(-1)))  # Shape: [batch_size*num_frames, hidden_size1]\n",
    "            x = self.bn1(x)  # Batch Normalization\n",
    "            x = self.relu1(x)\n",
    "            x = self.dropout1(x)  # Apply Dropout\n",
    "\n",
    "            x = self.fc2(x)\n",
    "            x = self.bn2(x)  # Batch Normalization\n",
    "            x = self.relu2(x)\n",
    "            x = self.dropout2(x)  # Apply Dropout\n",
    "\n",
    "            x = self.fc3(x)\n",
    "            x = self.bn3(x)  # Batch Normalization\n",
    "            x = self.relu3(x)\n",
    "            x = self.dropout3(x)  # Apply Dropout\n",
    "\n",
    "            x = self.fc4(x)  # Final layer without additional activation (could add if needed)\n",
    "            x = self.relu4(x)\n",
    "\n",
    "            # Reshape back to [batch_size, num_frames, final_size]\n",
    "            frame_features = x.view(batch_size, num_frames, -1)  # Shape: [batch_size, num_frames, final_size]\n",
    "\n",
    "            # Aggregate the frame features (mean pooling)\n",
    "            pooled_features = frame_features.mean(dim=1)  # Shape: [batch_size, final_size]\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred during bala:\")\n",
    "            print(f\"Error: {e}\")\n",
    "        return pooled_features  # Shape: [batch_size, final_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UXT9pOQ3CLt"
   },
   "source": [
    "### `Combined_Video_Pose_Model`\n",
    "- **Description**: Combines the outputs of `Video_Model` and `Pose_Model` for comprehensive predictions.\n",
    "- **Methods**:\n",
    "  - `forward(pixel_values, bool_masked_pos, pose_landmarks)`: Combines features from video and pose models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BrwwwfUoDvLl"
   },
   "outputs": [],
   "source": [
    "class Combined_Video_Pose_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined_Video_Pose_Model integrates both video and pose models to provide\n",
    "    a comprehensive hidden representation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, video_hidden_size=2048, pose_hidden_size=2048, combined_hidden_size=1024, hidden_layer_size=2048, dropout_rate=0.5):\n",
    "        super(Combined_Video_Pose_Model, self).__init__()\n",
    "\n",
    "        # Video model (returns a 2048-dimensional feature vector)\n",
    "        self.video_model = Video_Model(hidden_size=video_hidden_size)\n",
    "\n",
    "        # Pose model (returns a 2048-dimensional feature vector)\n",
    "        self.pose_model = Pose_Model(final_size=pose_hidden_size)\n",
    "\n",
    "        # Separate processing for video features\n",
    "        self.video_fc1 = nn.Linear(video_hidden_size, combined_hidden_size)\n",
    "        self.video_relu1 = nn.ReLU()\n",
    "        self.video_dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Separate processing for pose features\n",
    "        self.pose_fc1 = nn.Linear(pose_hidden_size, combined_hidden_size)\n",
    "        self.pose_relu1 = nn.ReLU()\n",
    "        self.pose_dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Combine the features from the two models\n",
    "        combined_input_size = 2 * combined_hidden_size  # Combined input from both models\n",
    "\n",
    "        # Fully connected layers for the combined model\n",
    "        self.fc1 = nn.Linear(combined_input_size, hidden_layer_size)  # Input should be 4096\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_layer_size, combined_hidden_size)  # Output layer to maintain original architecture\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, pixel_values, bool_masked_pos, pose_landmarks):\n",
    "        \"\"\"\n",
    "        Forward pass through the combined model.\n",
    "        Args:\n",
    "            pixel_values: Input to the Video Model.\n",
    "            bool_masked_pos: Masking input to the Video Model.\n",
    "            pose_landmarks: Input to the Pose Model.\n",
    "\n",
    "        Returns:\n",
    "            x: The hidden representation of size 1024.\n",
    "        \"\"\"\n",
    "        # Ensure to log the input shapes for debuggi\n",
    "\n",
    "        # Forward pass through the video model\n",
    "        video_features = self.video_model(pixel_values, bool_masked_pos=bool_masked_pos)\n",
    "        if video_features is None:\n",
    "            raise ValueError(\"video_model returned None.\")\n",
    "\n",
    "        # Process video features separately\n",
    "        video_processed = self.video_fc1(video_features)\n",
    "        video_processed = self.video_relu1(video_processed)\n",
    "        video_processed = self.video_dropout1(video_processed)\n",
    "\n",
    "        # Forward pass through the pose model\n",
    "        pose_features = self.pose_model(pose_landmarks)\n",
    "        if pose_features is None:\n",
    "            raise ValueError(\"pose_model returned None.\")\n",
    "\n",
    "        # Process pose features separately\n",
    "        pose_processed = self.pose_fc1(pose_features)\n",
    "        pose_processed = self.pose_relu1(pose_processed)\n",
    "        pose_processed = self.pose_dropout1(pose_processed)\n",
    "\n",
    "        # Concatenate the processed feature vectors from both models\n",
    "        combined_features = torch.cat((video_processed, pose_processed), dim=1)  # Shape should be [batch_size, 4096]\n",
    "\n",
    "        # Pass through the combined MLP\n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Pass through the second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x  # Return hidden layer output only (Shape: [batch_size, combined_hidden_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Y8h4fUACDZm"
   },
   "source": [
    "## `Dual_Combined_Model`\n",
    "\n",
    "- **Description**: Combines two instances of `Combined_Video_Pose_Model` to produce classification and rating outputs.\n",
    "\n",
    "- **Methods**:\n",
    "  - `forward(pixel_values_1, bool_masked_pos_1, pose_landmarks_tensor_1, pixel_values_2, bool_masked_pos_2, pose_landmarks_tensor_2)`: Forward pass through the dual model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2HufVACb4BVN"
   },
   "outputs": [],
   "source": [
    "class Dual_Combined_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual_Combined_Model combines two instances of Combined_Video_Pose_Model\n",
    "    to produce classification and rating outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Dual_Combined_Model, self).__init__()\n",
    "\n",
    "        # Initialize two instances of Combined_Video_Pose_Model\n",
    "        self.model_1 = Combined_Video_Pose_Model()\n",
    "        self.model_2 = Combined_Video_Pose_Model()\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        self.classification_layer = nn.Linear(2048, 512)  # Initial layer after concatenation\n",
    "        self.classification_relu = nn.ReLU()\n",
    "        self.classification_dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.classification_output = nn.Linear(512, 3)  # Output layer for 3 classes\n",
    "\n",
    "    def forward(self, pixel_values_1, bool_masked_pos_1, pose_landmarks_tensor_1,\n",
    "                pixel_values_2, bool_masked_pos_2, pose_landmarks_tensor_2):\n",
    "        try:\n",
    "            # Forward pass through the first combined model\n",
    "            hidden_output_1 = self.model_1(pixel_values_1, bool_masked_pos_1, pose_landmarks_tensor_1)\n",
    "\n",
    "            # Forward pass through the second combined model\n",
    "            hidden_output_2 = self.model_2(pixel_values_2, bool_masked_pos_2, pose_landmarks_tensor_2)\n",
    "\n",
    "            # Check for None outputs\n",
    "            if hidden_output_1 is None or hidden_output_2 is None:\n",
    "                raise ValueError(\"One of the models returned None.\")\n",
    "\n",
    "            # Concatenate the hidden outputs from both models\n",
    "            combined_hidden = torch.cat((hidden_output_1, hidden_output_2), dim=1)  # Shape: [batch_size, 4096]\n",
    "\n",
    "            # Classification path\n",
    "            classification_hidden = self.classification_layer(combined_hidden)\n",
    "            classification_hidden = self.classification_relu(classification_hidden)\n",
    "            classification_hidden = self.classification_dropout(classification_hidden)\n",
    "\n",
    "            classification_output = self.classification_output(classification_hidden)  # Shape: [batch_size, 3]\n",
    "\n",
    "            return classification_output, combined_hidden    # Return outputs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred during forward pass:\")\n",
    "            print(f\"Error: {e}\")\n",
    "            raise  # Reraise the exception to maintain the stack trace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHyY8adi9rN6"
   },
   "source": [
    "## `CriteriaPredictionModel`\n",
    "\n",
    "- **Description**: A deep neural network for predicting multiple binary ratings (yes/no) using a series of fully connected layers, ReLU activations, and dropout for regularization. The model outputs a probability for each rating.\n",
    "\n",
    "- **Methods**:\n",
    "  - `forward(x)`: Forward pass through the network. The input is passed through five fully connected layers, each with ReLU and dropout, and the final output is processed by a sigmoid activation to produce binary classification probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rQAWzMLI5U_E"
   },
   "outputs": [],
   "source": [
    "class CriteriaPredictionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    RatingPredictionModel predicts multiple yes/no ratings with a deeper network architecture.\n",
    "    The model uses binary classification for each output rating (5 in total).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=4096, hidden_size1=2048, hidden_size2=1024, output_size=5, dropout_rate=0.5):\n",
    "        super(CriteriaPredictionModel, self).__init__()\n",
    "\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Second fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Adding more depth\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size2 // 2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_size2 // 2, hidden_size2 // 4)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc5 = nn.Linear(hidden_size2 // 4, hidden_size2 // 8)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout5 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Output layer for binary classification (yes/no for each of the 5 ratings)\n",
    "        self.output_layer = nn.Linear(hidden_size2 // 8, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid for binary yes/no predictions\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network with multiple layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.dropout5(x)\n",
    "\n",
    "        # Final binary classification\n",
    "        ratings = self.output_layer(x)\n",
    "        return self.sigmoid(ratings)  # Returns 5 values, each in range [0, 1] for yes/no classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PytDrrlz9xq7"
   },
   "source": [
    "## `DeepClassificationWithRatingModel`\n",
    "\n",
    "- **Description**: The `DeepClassificationWithRatingModel` integrates the `DeepDualCombinedModel` and adds a separate rating prediction model. If the classification output predicts class `0`, the rating model is triggered. It uses different criteria models for deadlift, squat, and lunges based on the predicted class.\n",
    "\n",
    "- **Methods**:\n",
    "  - `forward(combined_hidden, predicted_class)`:\n",
    "    - Takes the `combined_hidden` state and the `predicted_class`.\n",
    "    - If class `0` is predicted, the `dead_criteria_model` is used to predict ratings.\n",
    "    - If class `1` is predicted, the `squat_criteria_model` is used.\n",
    "    - If class `2` is predicted, the `lunges_criteria_model` is used.\n",
    "    - Returns the predicted ratings based on the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xisbxmLc5U_E"
   },
   "outputs": [],
   "source": [
    "class DeepClassificationWithRatingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepClassificationWithRatingModel integrates the DeepDualCombinedModel and\n",
    "    adds a separate rating prediction model. If the classification output predicts class `0`,\n",
    "    the rating model is triggered.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DeepClassificationWithRatingModel, self).__init__()\n",
    "\n",
    "        # Rating model with more layers\n",
    "        self.dead_criteria_model = CriteriaPredictionModel(input_size=2048, hidden_size1=2048, hidden_size2=1024, output_size=5, dropout_rate=0.5)\n",
    "        self.lunges_criteria_model = CriteriaPredictionModel(input_size=2048, hidden_size1=2048, hidden_size2=1024, output_size=7, dropout_rate=0.5)\n",
    "        self.squat_criteria_model = CriteriaPredictionModel(input_size=2048, hidden_size1=2048, hidden_size2=1024, output_size=6, dropout_rate=0.5)\n",
    "\n",
    "    def forward(self,combined_hidden,predicted_class):\n",
    "        try:\n",
    "            # Initialize ratings as None\n",
    "            ratings = None\n",
    "\n",
    "            # If class `0` is predicted, trigger rating prediction\n",
    "            if predicted_class == 0:\n",
    "                ratings = self.dead_criteria_model(combined_hidden)\n",
    "            elif predicted_class == 1:\n",
    "                ratings = self.squat_criteria_model(combined_hidden)\n",
    "            elif predicted_class == 2:\n",
    "                ratings = self.lunges_criteria_model(combined_hidden)\n",
    "            return ratings\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during forward pass in DeepClassificationWithRatingModel: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmiORcTVCe8E"
   },
   "source": [
    "## `PoseVideoDataset`\n",
    "\n",
    "- **Description**: A custom PyTorch Dataset designed to load video frames and pose landmarks, along with action class labels and ratings. It supports deadlift, squat, and lunge actions with different rating models for each. The dataset processes video frames, extracts pose landmarks, and normalizes ratings based on action class.\n",
    "\n",
    "- **Methods**:\n",
    "  - `__len__()`: Returns the number of samples in the dataset.\n",
    "  - `__getitem__(idx)`: Loads and processes the data at the specified index:\n",
    "    - Loads video frames for both frontal and lateral views.\n",
    "    - Processes and normalizes video frames using the processor.\n",
    "    - Extracts pose landmarks and action class labels.\n",
    "    - Retrieves and normalizes ratings based on action class (Deadlift, Squat, or Lunge).\n",
    "  - `_process_ratings(df)`: Processes and normalizes the ratings data from the corresponding DataFrame:\n",
    "    - Extracts relevant columns (ending in 'F' or 'L').\n",
    "    - Normalizes the scores and applies a threshold (0 or 1 based on the condition).\n",
    "    - Returns the mean of the scores for each rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gzjcU07Q9FLq"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PoseVideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset to load video frames and pose landmarks, along with class and ratings labels.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A DataFrame containing the dataset information, including video paths and labels.\n",
    "        num_frames (int): Number of frames to load per video.\n",
    "        processor: A pre-processing function to resize and normalize video frames.\n",
    "        train_df_dead (pd.DataFrame): DataFrame containing ratings for deadlifts.\n",
    "        train_df_squat (pd.DataFrame): DataFrame containing ratings for squats.\n",
    "        train_df_lunge (pd.DataFrame): DataFrame containing ratings for lunges.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the processed video frames, pose landmarks, labels, and ratings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, num_frames, processor):\n",
    "        self.df = df\n",
    "        self.num_frames = num_frames\n",
    "        self.processor = processor\n",
    "        self.train_df_dead = train_df_dead\n",
    "        self.train_df_squat = train_df_squat\n",
    "        self.train_df_lunge = train_df_lunge\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Get frontal and lateral video paths and index\n",
    "        num_video_frontal = row['Num Video Frontal']\n",
    "        num_video_lateral = row['Num Video Lateral']\n",
    "        num_idx = row['NumIdx']\n",
    "        action = row['Action']\n",
    "\n",
    "        # Load video frames and extract pose landmarks\n",
    "        video_frames_frontal = load_and_resize_frames(num_video_frontal, action, 1, num_idx, self.num_frames)\n",
    "        video_frames_lateral = load_and_resize_frames(num_video_lateral, action, 0, num_idx, self.num_frames)\n",
    "\n",
    "        # Process the video frames to get pixel values\n",
    "        pixel_values_frontal = self.processor(list(np.clip(np.array(video_frames_frontal), 0, 255)), return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        pixel_values_lateral = self.processor(list(np.clip(np.array(video_frames_lateral), 0, 255)), return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "\n",
    "        model = AutoModelForPreTraining.from_pretrained(\"MCG-NJU/videomae-base\")  # Load model for masking\n",
    "        num_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\n",
    "        seq_length = (16 // model.config.tubelet_size) * num_patches_per_frame\n",
    "        bool_masked_pos_frontal = torch.randint(0, 2, (seq_length,)).bool()\n",
    "        bool_masked_pos_lateral = torch.randint(0, 2, (seq_length,)).bool()\n",
    "\n",
    "        # Extract pose landmarks from video frames\n",
    "        pose_landmarks_frontal = row['front_pose']\n",
    "        pose_landmarks_lateral = row['lat_pose']\n",
    "        pose_landmarks_tensor_frontal = torch.tensor(pose_landmarks_frontal).float()\n",
    "        pose_landmarks_tensor_lateral = torch.tensor(pose_landmarks_lateral).float()\n",
    "\n",
    "        # Get labels (action class)\n",
    "        label_class = torch.tensor(row['class'], dtype=torch.long)\n",
    "\n",
    "        # Initialize ratings\n",
    "        ratings = None\n",
    "\n",
    "        # Check label_class and load appropriate DataFrame\n",
    "        if label_class.item() == 0:  # Deadlift\n",
    "            ratings = self._process_ratings(self.train_df_dead,row)\n",
    "        elif label_class.item() == 1:  # Squat\n",
    "            ratings = self._process_ratings(self.train_df_squat,row)\n",
    "        elif label_class.item() == 2:  # Lunge\n",
    "            ratings = self._process_ratings(self.train_df_lunge,row)\n",
    "\n",
    "        # Convert ratings to tensor\n",
    "        ratings = torch.tensor(ratings, dtype=torch.float32) if ratings is not None else None\n",
    "\n",
    "        return (pixel_values_frontal, bool_masked_pos_frontal, pose_landmarks_tensor_frontal,\n",
    "                pixel_values_lateral, bool_masked_pos_lateral, pose_landmarks_tensor_lateral,\n",
    "                label_class, ratings)\n",
    "\n",
    "    def _process_ratings(self, df,row):\n",
    "        \"\"\"\n",
    "        Process the ratings DataFrame to extract and normalize scores.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing ratings for the specific action.\n",
    "\n",
    "        Returns:\n",
    "            list: Normalized and thresholded scores.\n",
    "        \"\"\"\n",
    "        # Select columns ending with 'F' or 'L'\n",
    "        relevant_columns = [col for col in df.columns if col.endswith('F') or col.endswith('L')]\n",
    "\n",
    "        # Extract ratings and normalize\n",
    "        scores = row[relevant_columns].values\n",
    "\n",
    "\n",
    "        # Apply threshold: Convert to 0 or 1 based on specific thresholding conditions\n",
    "        thresholded_scores = np.where(scores >= 0.5, 1, 0)\n",
    "\n",
    "        return thresholded_scores.tolist()  # Return mean of the scores for each sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CBl_FBpQBa1e"
   },
   "outputs": [],
   "source": [
    "def create_dataloader(df, num_frames, processor, batch_size=8, shuffle=True):\n",
    "    \"\"\"\n",
    "    Creates a PyTorch DataLoader from the PoseVideoDataset.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the dataset information.\n",
    "        num_frames (int): The number of frames to extract from each video.\n",
    "        processor: A video frame pre-processing function.\n",
    "        batch_size (int, optional): Batch size for the DataLoader. Defaults to 8.\n",
    "        shuffle (bool, optional): Whether to shuffle the data. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: A PyTorch DataLoader for the dataset.\n",
    "    \"\"\"\n",
    "    dataset = PoseVideoDataset(df, num_frames, processor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvW6aZGJ-QGS"
   },
   "source": [
    "## `train_combined_model`\n",
    "\n",
    "- **Description**: This function trains both the classification model (`Dual_Combined_Model`) and the rating prediction model (`CriteriaPredictionModel`). It evaluates them after each epoch and saves the best-performing models based on validation loss. It also includes early stopping if no improvement is observed for a set number of epochs.\n",
    "\n",
    "- **Args**:\n",
    "  - `model (torch.nn.Module)`: The classification model (e.g., `Dual_Combined_Model`).\n",
    "  - `criteria_model (torch.nn.Module)`: The rating prediction model (e.g., `CriteriaPredictionModel`).\n",
    "  - `train_dataloader (DataLoader)`: A DataLoader providing the training data.\n",
    "  - `eval_dataloader (DataLoader)`: A DataLoader providing the evaluation data.\n",
    "  - `epochs (int, optional)`: Number of epochs to train. Defaults to 1000.\n",
    "  - `lr (float, optional)`: Learning rate for the optimizer. Defaults to 1e-4.\n",
    "  - `device (str, optional)`: The device to train the model on ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "  - `clip_grad_norm (float, optional)`: Maximum norm for gradient clipping. Defaults to 1.0.\n",
    "  - `patience (int, optional)`: Number of epochs with no improvement after which training will be stopped. Defaults to 5.\n",
    "\n",
    "- **Returns**:\n",
    "  - None: The function prints the training progress, validation loss, classification accuracy, and rating accuracy during training.\n",
    "\n",
    "- **Steps**:\n",
    "  1. **Model Initialization**: Moves both the classification and rating models to the specified device.\n",
    "  2. **Optimizer Setup**: Uses the Adam optimizer for both models with the specified learning rate.\n",
    "  3. **Loss Functions**: Defines loss functions for both classification (`CrossEntropyLoss`) and ratings (`BCEWithLogitsLoss`).\n",
    "  4. **Training Loop**:\n",
    "     - Performs a forward pass through the classification model and computes the classification loss.\n",
    "     - Computes the rating prediction loss only if the predicted class matches the actual class.\n",
    "     - Combines both the classification and rating losses and performs backpropagation.\n",
    "  5. **Gradient Clipping**: Clips gradients to avoid exploding gradients during backpropagation.\n",
    "  6. **Model Evaluation**: Evaluates the models after each epoch and prints the results.\n",
    "  7. **Early Stopping**: Monitors validation loss and triggers early stopping if no improvement is observed for a specified number of epochs.\n",
    "  8. **Model Checkpointing**: Saves the best-performing models based on validation loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PSZbVvGz-Itz"
   },
   "outputs": [],
   "source": [
    "def train_combined_model(model, criteria_model, train_dataloader, eval_dataloader, epochs=1000, lr=1e-4,\n",
    "                         device='cpu', clip_grad_norm=1.0, patience=5):\n",
    "    \"\"\"\n",
    "    Trains both the classification and rating prediction models, evaluates them after each epoch.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The classification model (Dual_Combined_Model).\n",
    "        criteria_model (torch.nn.Module): The rating prediction model (RatingPredictionModel).\n",
    "        train_dataloader (DataLoader): A DataLoader providing the training data.\n",
    "        eval_dataloader (DataLoader): A DataLoader providing the evaluation data.\n",
    "        epochs (int, optional): Number of epochs to train. Defaults to 1000.\n",
    "        lr (float, optional): Learning rate for the optimizer. Defaults to 1e-4.\n",
    "        device (str, optional): The device to train the model on ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "        clip_grad_norm (float, optional): Maximum norm for gradient clipping. Defaults to 1.0.\n",
    "        patience (int, optional): Number of epochs with no improvement after which training will be stopped. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the training progress, validation loss, classification accuracy, and rating accuracy.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    criteria_model.to(device)\n",
    "\n",
    "    # Set up optimizer and loss functions\n",
    "    optimizer = optim.Adam(list(model.parameters()) + list(criteria_model.parameters()), lr=lr)\n",
    "    criterion_class = nn.CrossEntropyLoss()  # Loss for classification task\n",
    "    criterion_ratings = nn.BCEWithLogitsLoss()  # Loss for rating task (binary)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "\n",
    "    best_eval_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        criteria_model.train()\n",
    "        running_loss = 0.0\n",
    "        a = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            # Unpack batch data and move to the specified device\n",
    "            (pixel_values_frontal, bool_masked_pos_frontal, pose_landmarks_tensor_frontal,\n",
    "             pixel_values_lateral, bool_masked_pos_lateral, pose_landmarks_tensor_lateral,\n",
    "             label_class, ratings) = [tensor.to(device) for tensor in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through the classification model\n",
    "            classification_output, combined_hidden = model(\n",
    "                pixel_values_frontal, bool_masked_pos_frontal, pose_landmarks_tensor_frontal,\n",
    "                pixel_values_lateral, bool_masked_pos_lateral, pose_landmarks_tensor_lateral\n",
    "            )\n",
    "            _, predicted_class = torch.max(classification_output, 1)\n",
    "\n",
    "            # Loss for classification\n",
    "            loss_class = criterion_class(classification_output, label_class)\n",
    "            print(f\"real class : {label_class} and predicted class : {predicted_class}\")\n",
    "            print(f\"loss class : {loss_class}\")\n",
    "            # Forward pass through the rating prediction model\n",
    "            # We need to trigger the correct rating model based on the predicted class\n",
    "            for i in range(label_class.size(0)):  # Iterate over each sample in the batch\n",
    "                actual_class = label_class[i].item()\n",
    "                predicted_class_item = predicted_class[i].item()\n",
    "                # Loss for ratings (if the predicted class matches the actual class)\n",
    "                if predicted_class_item == actual_class:\n",
    "                    if actual_class == 0:  # Deadlift\n",
    "                        ratings_output = criteria_model.dead_criteria_model(combined_hidden[i])\n",
    "                    elif actual_class == 1:  # Squat\n",
    "                        ratings_output = criteria_model.squat_criteria_model(combined_hidden[i])\n",
    "                    elif actual_class == 2:  # Lunges\n",
    "                        ratings_output = criteria_model.lunges_criteria_model(combined_hidden[i])\n",
    "\n",
    "                    print(f\"ratings : {ratings[i]} and ratings_output : {ratings_output}\")\n",
    "                    loss_ratings = criterion_ratings(ratings_output, ratings[i].float())\n",
    "                    \n",
    "                else:\n",
    "                    # High loss if the predicted class doesn't match\n",
    "                    loss_ratings = torch.tensor(100.0, device=device, requires_grad=True)\n",
    "                \n",
    "                print(f\"loss ratings : {loss_ratings}\")\n",
    "                # Combine classification and rating losses\n",
    "                loss = loss_class + loss_ratings\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(list(model.parameters()) + list(criteria_model.parameters()), clip_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                print(f\"{a} and data_size: {len(train_dataloader)} and epoch: {epoch + 1}\")\n",
    "                a += 1\n",
    "\n",
    "        avg_loss = running_loss / len(train_dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Evaluate both models after each epoch\n",
    "        eval_loss, eval_accuracy, eval_rating_accuracy = evaluate_combined_model(\n",
    "            model, criteria_model, eval_dataloader, device\n",
    "        )\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {eval_loss:.4f}, \"\n",
    "              f\"Classification Accuracy: {eval_accuracy:.4f}, \"\n",
    "              f\"Rating Accuracy (Deadlift): {eval_rating_accuracy['deadlift']:.4f}, \"\n",
    "              f\"Rating Accuracy (Squat): {eval_rating_accuracy['squat']:.4f}, \"\n",
    "              f\"Rating Accuracy (Lunges): {eval_rating_accuracy['lunges']:.4f}\")\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step(eval_loss)\n",
    "\n",
    "        # Check for early stopping\n",
    "        if eval_loss < best_eval_loss:\n",
    "            best_eval_loss = eval_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f\"best_combined_model_epoch_{epoch + 1}.pt\")\n",
    "            torch.save(criteria_model.state_dict(), f\"best_rating_model_epoch_{epoch + 1}.pt\")\n",
    "            print(\"Model checkpoint saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dusNp1b-Z1k"
   },
   "source": [
    "## `evaluate_combined_model`\n",
    "\n",
    "- **Description**: This function evaluates both the classification and rating prediction models on the provided dataset. It computes the average evaluation loss, classification accuracy, and rating accuracies for different actions (deadlift, squat, lunges) using a given `DataLoader` and loss functions.\n",
    "\n",
    "- **Args**:\n",
    "  - `classification_model (torch.nn.Module)`: The classification model (e.g., `Dual_Combined_Model`).\n",
    "  - `criteria_model (torch.nn.Module)`: The rating prediction model (e.g., `CriteriaPredictionModel`).\n",
    "  - `dataloader (DataLoader)`: A DataLoader providing the evaluation data.\n",
    "  - `criterion_class (nn.Module)`: The loss function for the classification task.\n",
    "  - `criterion_ratings (nn.Module)`: The loss function for the rating task.\n",
    "  - `device (str)`: The device to perform evaluation on ('cpu' or 'cuda').\n",
    "\n",
    "- **Returns**:\n",
    "  - `float`: The average evaluation loss across the dataset.\n",
    "  - `dict`: A dictionary containing the classification accuracy and rating accuracies for each action (deadlift, squat, lunges).\n",
    "\n",
    "- **Steps**:\n",
    "  1. **Set Evaluation Mode**: Sets the models (`classification_model` and `criteria_model`) to evaluation mode.\n",
    "  2. **Initialize Metrics**: Initializes accumulators for loss, classification accuracy, and rating accuracy for each action.\n",
    "  3. **Evaluation Loop**:\n",
    "     - Performs a forward pass through the classification model to get predictions and computes classification loss.\n",
    "     - For each sample in the batch, if the predicted class matches the actual class, it computes the rating accuracy for that action.\n",
    "     - The rating predictions are compared with the ground truth ratings, and accuracy is calculated for each action (deadlift, squat, lunges).\n",
    "  4. **Compute Average Metrics**: Computes the average classification loss, overall classification accuracy, and rating accuracy for each action.\n",
    "  5. **Return Results**: Returns the average loss, classification accuracy, and rating accuracies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "hqEEcG285U_E"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_combined_model(classification_model, criteria_model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluates both the classification and rating models on the provided dataset.\n",
    "\n",
    "    Args:\n",
    "        classification_model (torch.nn.Module): The classification model.\n",
    "        criteria_model (torch.nn.Module): The rating prediction model.\n",
    "        dataloader (DataLoader): A DataLoader providing the evaluation data.\n",
    "        criterion_class (nn.Module): The loss function for classification.\n",
    "        criterion_ratings (nn.Module): The loss function for ratings.\n",
    "        device (str): The device to perform evaluation on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        float: Average evaluation loss.\n",
    "        dict: Classification accuracy and rating accuracies for each action (deadlift, squat, lunges).\n",
    "    \"\"\"\n",
    "    classification_model.to(device)\n",
    "    criteria_model.to(device)\n",
    "    classification_model.eval()\n",
    "    criteria_model.eval()\n",
    "    criterion_class = nn.CrossEntropyLoss()  # Loss for classification task\n",
    "    criterion_ratings = nn.BCEWithLogitsLoss()  # Loss for rating task (binary)\n",
    "    total_loss = 0.0\n",
    "    correct_predictions_class = 0\n",
    "    total_samples_class = 0\n",
    "\n",
    "    # Initialize accumulators for rating accuracy per feature\n",
    "    rating_accumulators = {'deadlift': 0, 'squat': 0, 'lunges': 0}\n",
    "    total_rating_samples = {'deadlift': 0, 'squat': 0, 'lunges': 0}\n",
    "\n",
    "    # Initialize feature-level accuracy accumulators for different numbers of features per action\n",
    "    feature_accuracies = {\n",
    "        'deadlift': {'TP': [0] * 5, 'FP': [0] * 5, 'FN': [0] * 5, 'TN': [0] * 5},\n",
    "        'squat': {'TP': [0] * 6, 'FP': [0] * 6, 'FN': [0] * 6, 'TN': [0] * 6},\n",
    "        'lunges': {'TP': [0] * 7, 'FP': [0] * 7, 'FN': [0] * 7, 'TN': [0] * 7}\n",
    "    }\n",
    "    a = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            (pixel_values_frontal, bool_masked_pos_frontal, pose_landmarks_tensor_frontal,\n",
    "             pixel_values_lateral, bool_masked_pos_lateral, pose_landmarks_tensor_lateral,\n",
    "             label_class, ratings) = [tensor.to(device) for tensor in batch]\n",
    "\n",
    "            # Forward pass through the classification model\n",
    "            classification_output, combined_hidden = classification_model(\n",
    "                pixel_values_frontal, bool_masked_pos_frontal, pose_landmarks_tensor_frontal,\n",
    "                pixel_values_lateral, bool_masked_pos_lateral, pose_landmarks_tensor_lateral\n",
    "            )\n",
    "            _, predicted_class = torch.max(classification_output, 1)\n",
    "\n",
    "            # Classification loss and accuracy\n",
    "            loss_class = criterion_class(classification_output, label_class)\n",
    "            total_loss += loss_class.item()\n",
    "            correct_predictions_class += (predicted_class == label_class).sum().item()\n",
    "            total_samples_class += label_class.size(0)\n",
    "\n",
    "            a=a+1\n",
    "            print(a)\n",
    "            # Rating accuracy calculation per feature (only if predicted class matches actual class)\n",
    "            for i in range(label_class.size(0)):  # Loop over each sample in the batch\n",
    "                actual_class = label_class[i].item()\n",
    "                predicted_class_item = predicted_class[i].item()\n",
    "\n",
    "                if actual_class == predicted_class_item:\n",
    "                    if actual_class == 0:  # Deadlift\n",
    "                        ratings_output = criteria_model.dead_criteria_model(combined_hidden[i])\n",
    "                        predicted_ratings = torch.sigmoid(ratings_output) > 0.5\n",
    "                        correct_ratings = (predicted_ratings == ratings[i].byte()).sum().item()\n",
    "                        rating_accumulators['deadlift'] += correct_ratings\n",
    "                        total_rating_samples['deadlift'] += ratings[i].numel()\n",
    "\n",
    "                        # Feature-level analysis for deadlift\n",
    "                        for feature_idx in range(ratings[i].size(0)):  # Assume ratings[i] is the feature vector\n",
    "                            predicted = predicted_ratings[feature_idx].item()\n",
    "                            actual = ratings[i][feature_idx].item()\n",
    "\n",
    "                            # Update TP, FP, FN, TN for deadlift\n",
    "                            if predicted == 1 and actual == 1:\n",
    "                                feature_accuracies['deadlift']['TP'][feature_idx] += 1\n",
    "                            elif predicted == 1 and actual == 0:\n",
    "                                feature_accuracies['deadlift']['FP'][feature_idx] += 1\n",
    "                            elif predicted == 0 and actual == 1:\n",
    "                                feature_accuracies['deadlift']['FN'][feature_idx] += 1\n",
    "                            elif predicted == 0 and actual == 0:\n",
    "                                feature_accuracies['deadlift']['TN'][feature_idx] += 1\n",
    "\n",
    "                    elif actual_class == 1:  # Squat\n",
    "                        ratings_output = criteria_model.squat_criteria_model(combined_hidden[i])\n",
    "                        predicted_ratings = torch.sigmoid(ratings_output) > 0.5\n",
    "                        correct_ratings = (predicted_ratings == ratings[i].byte()).sum().item()\n",
    "                        rating_accumulators['squat'] += correct_ratings\n",
    "                        total_rating_samples['squat'] += ratings[i].numel()\n",
    "\n",
    "                        # Feature-level analysis for squat\n",
    "                        for feature_idx in range(ratings[i].size(0)):\n",
    "                            predicted = predicted_ratings[feature_idx].item()\n",
    "                            actual = ratings[i][feature_idx].item()\n",
    "\n",
    "                            # Update TP, FP, FN, TN for squat\n",
    "                            if predicted == 1 and actual == 1:\n",
    "                                feature_accuracies['squat']['TP'][feature_idx] += 1\n",
    "                            elif predicted == 1 and actual == 0:\n",
    "                                feature_accuracies['squat']['FP'][feature_idx] += 1\n",
    "                            elif predicted == 0 and actual == 1:\n",
    "                                feature_accuracies['squat']['FN'][feature_idx] += 1\n",
    "                            elif predicted == 0 and actual == 0:\n",
    "                                feature_accuracies['squat']['TN'][feature_idx] += 1\n",
    "\n",
    "                    elif actual_class == 2:  # Lunges\n",
    "                        ratings_output = criteria_model.lunges_criteria_model(combined_hidden[i])\n",
    "                        predicted_ratings = torch.sigmoid(ratings_output) > 0.5\n",
    "                        correct_ratings = (predicted_ratings == ratings[i].byte()).sum().item()\n",
    "                        rating_accumulators['lunges'] += correct_ratings\n",
    "                        total_rating_samples['lunges'] += ratings[i].numel()\n",
    "\n",
    "                        # Feature-level analysis for lunges\n",
    "                        for feature_idx in range(ratings[i].size(0)):\n",
    "                            predicted = predicted_ratings[feature_idx].item()\n",
    "                            actual = ratings[i][feature_idx].item()\n",
    "\n",
    "                            # Update TP, FP, FN, TN for lunges\n",
    "                            if predicted == 1 and actual == 1:\n",
    "                                feature_accuracies['lunges']['TP'][feature_idx] += 1\n",
    "                            elif predicted == 1 and actual == 0:\n",
    "                                feature_accuracies['lunges']['FP'][feature_idx] += 1\n",
    "                            elif predicted == 0 and actual == 1:\n",
    "                                feature_accuracies['lunges']['FN'][feature_idx] += 1\n",
    "                            elif predicted == 0 and actual == 0:\n",
    "                                feature_accuracies['lunges']['TN'][feature_idx] += 1\n",
    "\n",
    "    # Calculate the average loss and classification accuracy\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy_class = correct_predictions_class / total_samples_class if total_samples_class > 0 else 0.0\n",
    "\n",
    "    # Calculate rating accuracy for each action\n",
    "    rating_accuracy = {\n",
    "        'deadlift': rating_accumulators['deadlift'] / total_rating_samples['deadlift'] if total_rating_samples['deadlift'] > 0 else 0.0,\n",
    "        'squat': rating_accumulators['squat'] / total_rating_samples['squat'] if total_rating_samples['squat'] > 0 else 0.0,\n",
    "        'lunges': rating_accumulators['lunges'] / total_rating_samples['lunges'] if total_rating_samples['lunges'] > 0 else 0.0\n",
    "    }\n",
    "\n",
    "    # Print feature-level accuracy and performance metrics\n",
    "    for action in ['deadlift', 'squat', 'lunges']:\n",
    "        num_features = len(feature_accuracies[action]['TP'])  # Number of features for the action\n",
    "        for feature_idx in range(num_features):\n",
    "            TP = feature_accuracies[action]['TP'][feature_idx]\n",
    "            FP = feature_accuracies[action]['FP'][feature_idx]\n",
    "            FN = feature_accuracies[action]['FN'][feature_idx]\n",
    "            TN = feature_accuracies[action]['TN'][feature_idx]\n",
    "\n",
    "            precision = TP / (TP + FP) if TP + FP > 0 else 0.0\n",
    "            recall = TP / (TP + FN) if TP + FN > 0 else 0.0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "            accuracy = (TP + TN) / (TP + FP + FN + TN) if TP + FP + FN + TN > 0 else 0.0\n",
    "            print(f\"{action} feature {feature_idx + 1} - TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}\")\n",
    "            print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Write results to a file\n",
    "    with open('val_numbers.txt', 'a') as f:\n",
    "        f.write(f\"Classification Accuracy: {accuracy_class:.4f}\\n\")\n",
    "        f.write(f\"Rating Accuracy - Deadlift: {rating_accuracy['deadlift']:.4f}\\n\")\n",
    "        f.write(f\"Rating Accuracy - Squat: {rating_accuracy['squat']:.4f}\\n\")\n",
    "        f.write(f\"Rating Accuracy - Lunges: {rating_accuracy['lunges']:.4f}\\n\")\n",
    "        f.write(\"Feature-level Accuracies and Metrics:\\n\")\n",
    "        for action in ['deadlift', 'squat', 'lunges']:\n",
    "            num_features = len(feature_accuracies[action]['TP'])  # Number of features for the action\n",
    "            for feature_idx in range(num_features):\n",
    "                TP = feature_accuracies[action]['TP'][feature_idx]\n",
    "                FP = feature_accuracies[action]['FP'][feature_idx]\n",
    "                FN = feature_accuracies[action]['FN'][feature_idx]\n",
    "                TN = feature_accuracies[action]['TN'][feature_idx]\n",
    "\n",
    "                precision = TP / (TP + FP) if TP + FP > 0 else 0.0\n",
    "                recall = TP / (TP + FN) if TP + FN > 0 else 0.0\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "                accuracy = (TP + TN) / (TP + FP + FN + TN) if TP + FP + FN + TN > 0 else 0.0\n",
    "                f.write(f\"{action} feature {feature_idx + 1} - TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}\\n\")\n",
    "                f.write(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, Accuarcy: {accuracy:.4f}\\n\")\n",
    "\n",
    "    return avg_loss, accuracy_class, rating_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "\n",
    "dataloader = create_dataloader(train_df, 16, processor, batch_size=1)\n",
    "eval_dataloader = create_dataloader(val_df, 16, processor, batch_size=1)\n",
    "test_dataloader = create_dataloader(test_df, 16, processor, batch_size=1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "deadlift feature 1 - TP: 37, FP: 3, FN: 0, TN: 0\n",
      "Precision: 0.9250, Recall: 1.0000, F1-score: 0.9610, Accuracy: 0.9250\n",
      "deadlift feature 2 - TP: 38, FP: 2, FN: 0, TN: 0\n",
      "Precision: 0.9500, Recall: 1.0000, F1-score: 0.9744, Accuracy: 0.9500\n",
      "deadlift feature 3 - TP: 17, FP: 23, FN: 0, TN: 0\n",
      "Precision: 0.4250, Recall: 1.0000, F1-score: 0.5965, Accuracy: 0.4250\n",
      "deadlift feature 4 - TP: 0, FP: 0, FN: 18, TN: 22\n",
      "Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000, Accuracy: 0.5500\n",
      "deadlift feature 5 - TP: 35, FP: 5, FN: 0, TN: 0\n",
      "Precision: 0.8750, Recall: 1.0000, F1-score: 0.9333, Accuracy: 0.8750\n",
      "squat feature 1 - TP: 0, FP: 0, FN: 43, TN: 1\n",
      "Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000, Accuracy: 0.0227\n",
      "squat feature 2 - TP: 30, FP: 14, FN: 0, TN: 0\n",
      "Precision: 0.6818, Recall: 1.0000, F1-score: 0.8108, Accuracy: 0.6818\n",
      "squat feature 3 - TP: 44, FP: 0, FN: 0, TN: 0\n",
      "Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000, Accuracy: 1.0000\n",
      "squat feature 4 - TP: 15, FP: 29, FN: 0, TN: 0\n",
      "Precision: 0.3409, Recall: 1.0000, F1-score: 0.5085, Accuracy: 0.3409\n",
      "squat feature 5 - TP: 44, FP: 0, FN: 0, TN: 0\n",
      "Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000, Accuracy: 1.0000\n",
      "squat feature 6 - TP: 0, FP: 0, FN: 4, TN: 40\n",
      "Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000, Accuracy: 0.9091\n",
      "lunges feature 1 - TP: 9, FP: 6, FN: 0, TN: 0\n",
      "Precision: 0.6000, Recall: 1.0000, F1-score: 0.7500, Accuracy: 0.6000\n",
      "lunges feature 2 - TP: 11, FP: 4, FN: 0, TN: 0\n",
      "Precision: 0.7333, Recall: 1.0000, F1-score: 0.8462, Accuracy: 0.7333\n",
      "lunges feature 3 - TP: 13, FP: 2, FN: 0, TN: 0\n",
      "Precision: 0.8667, Recall: 1.0000, F1-score: 0.9286, Accuracy: 0.8667\n",
      "lunges feature 4 - TP: 8, FP: 7, FN: 0, TN: 0\n",
      "Precision: 0.5333, Recall: 1.0000, F1-score: 0.6957, Accuracy: 0.5333\n",
      "lunges feature 5 - TP: 15, FP: 0, FN: 0, TN: 0\n",
      "Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000, Accuracy: 1.0000\n",
      "lunges feature 6 - TP: 0, FP: 15, FN: 0, TN: 0\n",
      "Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000, Accuracy: 0.0000\n",
      "lunges feature 7 - TP: 9, FP: 6, FN: 0, TN: 0\n",
      "Precision: 0.6000, Recall: 1.0000, F1-score: 0.7500, Accuracy: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.001911617439202588,\n",
       " 1.0,\n",
       " {'deadlift': 0.745,\n",
       "  'squat': 0.6590909090909091,\n",
       "  'lunges': 0.6190476190476191})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Initialize the model\n",
    "# dual_combined_model = Dual_Combined_Model()\n",
    "# rating_model = DeepClassificationWithRatingModel()\n",
    "\n",
    "# print(device)\n",
    "\n",
    "# # Train the model\n",
    "# train_combined_model(dual_combined_model, rating_model, dataloader, eval_dataloader,\n",
    "#                      epochs=1000, lr=1e-4, device=device)\n",
    "\n",
    "\n",
    "# Define the path to the saved model weights\n",
    "model_path = \"best_combined_model_epoch_1.pt\"\n",
    "criteria_model_path = \"best_rating_model_epoch_1.pt\"\n",
    "\n",
    "dual_combined_model = Dual_Combined_Model()\n",
    "rating_model = DeepClassificationWithRatingModel()\n",
    "\n",
    "# Load the saved model weights\n",
    "dual_combined_model.load_state_dict(torch.load(model_path))\n",
    "rating_model.load_state_dict(torch.load(criteria_model_path))\n",
    "\n",
    "print(device)\n",
    "\n",
    "# Continue training the model\n",
    "\n",
    "train_combined_model(dual_combined_model, rating_model, dataloader, eval_dataloader,\n",
    "                      epochs=1000, lr=1e-4, device=device)\n",
    "\n",
    "# Evaluate the model on dataset\n",
    "evaluate_combined_model(dual_combined_model, rating_model, eval_dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ahmed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
>>>>>>> e76e339227393a5fcac87d93712334c7cbd6bc7c
