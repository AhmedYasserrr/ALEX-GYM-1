{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD7vpMS924a1"
   },
   "source": [
    "This Notebook contains implementations of neural network models that process video frames and pose landmarks to classify actions and predict ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fwXbWGQOyJDD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Courses\\ANA\\envs\\ahmed\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoImageProcessor, AutoModelForPreTraining\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FmdwF_C45U--",
    "outputId": "01dcd918-f449-45df-dc5e-e29054b2faee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch can use the GPU.\n",
      "Device name: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch can use the GPU.\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is using the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KdOB4Yjz50De",
    "outputId": "3889fa4e-aba3-4f6b-f899-b1e1766f94eb"
   },
   "outputs": [],
   "source": [
    "base_path = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsKfIuDB2zsP"
   },
   "source": [
    "## Functions\n",
    "\n",
    "### `load_and_resize_frames`\n",
    "- **Description**: Loads and resizes video frames from specified file paths.\n",
    "- **Parameters**:\n",
    "  - `num_video_frontal`: Identifier for the video file.\n",
    "  - `num_idx`: Index for the specific video segment.\n",
    "  - `num_frames`: Number of frames to load.\n",
    "  - `size`: Target size for resizing frames (default: (224, 224)).\n",
    "- **Returns**: List of resized frames.\n",
    "\n",
    "### `extract_pose_landmarks`\n",
    "- **Description**: Extracts pose landmarks from a list of video frames using MediaPipe.\n",
    "- **Parameters**:\n",
    "  - `video_frames`: List of frames (each of size [224, 224, 3]).\n",
    "- **Returns**: Array of pose landmarks with shape [num_frames, 33, 3].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "64pDhqby2ohg"
   },
   "outputs": [],
   "source": [
    "def load_and_resize_frames(num_video, action, frontORlat, num_idx, num_frames, size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Loads and resizes video frames from specified file paths based on the action type.\n",
    "\n",
    "    Args:\n",
    "        num_video (int): Identifier for the video file.\n",
    "        action (str): The action being performed ('deadlift', 'squat', 'lunges').\n",
    "        frontORlat (int): 1 for frontal frames, 0 for lateral frames.\n",
    "        num_idx (int): Index for the specific video segment.\n",
    "        num_frames (int): Number of frames to load.\n",
    "        size (tuple): Target size for resizing frames (default is (224, 224)).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of resized frames.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "\n",
    "    # Determine the folder paths based on the action\n",
    "    if action == 'Deadlift':\n",
    "        frontal_folder = base_path+'Deadlift_Frames'\n",
    "        lateral_folder =  base_path+'Deadlift_Frames'\n",
    "    elif action == 'Squat':\n",
    "        frontal_folder =  base_path+'Squat_Frames'\n",
    "        lateral_folder =  base_path+'Squat_Frames'\n",
    "    elif action == 'lunges':\n",
    "        frontal_folder =  base_path+'Lunges_Frames'\n",
    "        lateral_folder =  base_path+'Lunges_Frames'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown action: {action}\")\n",
    "\n",
    "    # Load the frames based on frontORlat flag\n",
    "    if frontORlat == 1:\n",
    "        folder = frontal_folder\n",
    "    else:\n",
    "        folder = lateral_folder\n",
    "\n",
    "    # Load and resize frames\n",
    "    for i in range(1, num_frames + 1):  # Loop from 1 to num_frames (e.g., 16)\n",
    "        path = f\"{folder}/{num_video}_idx_{num_idx}_{i}.jpg\"\n",
    "        img = cv2.imread(path)\n",
    "        if img is not None:\n",
    "            img_resized = cv2.resize(img, size)  # Resize to the specified size (default is 224x224)\n",
    "            frames.append(img_resized)\n",
    "        else:\n",
    "            print(f\"Warning: Could not load image at {path}\")\n",
    "\n",
    "    return frames\n",
    "\n",
    "def extract_pose_landmarks(video_frames):\n",
    "    \"\"\"\n",
    "    Extracts pose landmarks from a list of video frames using MediaPipe.\n",
    "\n",
    "    Args:\n",
    "        video_frames (list): List of frames (each of size [224, 224, 3]).\n",
    "\n",
    "    Returns:\n",
    "        np.array: Array of pose landmarks with shape [num_frames, 33, 3].\n",
    "    \"\"\"\n",
    "    pose_landmarks = []\n",
    "\n",
    "    for frame in video_frames:\n",
    "        # Ensure the frame is in the right format\n",
    "        if frame.shape != (224, 224, 3):\n",
    "            print(f\"Unexpected frame shape: {frame.shape}\")\n",
    "            continue\n",
    "        # Convert the frame from float64 [0, 1] to uint8 [0, 255]\n",
    "        frame_uint8 = (np.clip(frame * 255, 0, 255)).astype(np.uint8)  # Shape: [224, 224, 3]\n",
    "\n",
    "        # Extract pose landmarks\n",
    "        results = pose.process(frame_uint8)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            # Get the landmarks (33 landmarks, each with x, y, z)\n",
    "            landmarks = [[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark]\n",
    "        else:\n",
    "            # If no landmarks are detected, we use a zero vector\n",
    "            landmarks = np.zeros((33, 3))\n",
    "\n",
    "        pose_landmarks.append(landmarks)\n",
    "\n",
    "    return np.array(pose_landmarks)  # Shape: [num_frames, 33, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DNbu6JEg5U-_",
    "outputId": "2119f0de-5282-4087-e249-1a3845887f59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            front_pose  \\\n",
      "0    [[[0.49896547198295593, 0.24871453642845154, -...   \n",
      "1    [[[0.5140300393104553, 0.22558534145355225, -0...   \n",
      "2    [[[0.5227701663970947, 0.22155773639678955, -0...   \n",
      "3    [[[0.5229289531707764, 0.2611807584762573, -0....   \n",
      "4    [[[0.5273605585098267, 0.22662483155727386, -0...   \n",
      "..                                                 ...   \n",
      "290  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "291  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "292  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "293  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "294  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "\n",
      "                                              lat_pose  \n",
      "0    [[[0.48301321268081665, 0.2528548836708069, 0....  \n",
      "1    [[[0.5150753259658813, 0.25385501980781555, 0....  \n",
      "2    [[[0.523187518119812, 0.23705779016017914, 0.1...  \n",
      "3    [[[0.5000579953193665, 0.24061907827854156, 0....  \n",
      "4    [[[0.5319615006446838, 0.2524169683456421, 0.0...  \n",
      "..                                                 ...  \n",
      "290  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "291  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "292  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "293  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "294  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "\n",
      "[295 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_squat = pd.read_excel(base_path + 'squat_edited.xlsx')\n",
    "\n",
    "# Load JSON files and convert to NumPy arrays\n",
    "def load_json_as_numpy(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)  # Load the JSON data\n",
    "    return np.array(data)  # Convert the list to a NumPy array\n",
    "\n",
    "# Load the front and lateral poses\n",
    "front_pose_array = load_json_as_numpy(base_path + 'front_pose_squat.json')\n",
    "lat_pose_array = load_json_as_numpy(base_path + 'lat_pose_squat.json')\n",
    "\n",
    "# Make sure `front_pose` and `lat_pose` columns exist\n",
    "if 'front_pose' not in df_squat.columns:\n",
    "    df_squat['front_pose'] = None\n",
    "if 'lat_pose' not in df_squat.columns:\n",
    "    df_squat['lat_pose'] = None\n",
    "\n",
    "# Assign the loaded arrays back to the DataFrame\n",
    "# Make sure the lengths match\n",
    "if len(front_pose_array) == len(df_squat) and len(lat_pose_array) == len(df_squat):\n",
    "    df_squat['front_pose'] = list(front_pose_array)\n",
    "    df_squat['lat_pose'] = list(lat_pose_array)\n",
    "else:\n",
    "    print(\"Error: The length of the loaded arrays does not match the DataFrame.\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_squat[['front_pose', 'lat_pose']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "phgbHMZC5U_A"
   },
   "outputs": [],
   "source": [
    "# Define your split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "total_samples = len(df_squat)\n",
    "train_size = int(total_samples * train_ratio)\n",
    "val_size = int(total_samples * val_ratio)\n",
    "\n",
    "# Split the DataFrame\n",
    "train_df_squat = df_squat.iloc[:train_size]                    # First 70% for training\n",
    "val_df_squat = df_squat.iloc[train_size:train_size + val_size]  # Next 15% for validation\n",
    "test_df_squat = df_squat.iloc[train_size + val_size:]           # Last 15% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805
    },
    "id": "p6g0uol45U_B",
    "outputId": "5aa658a6-6992-405e-c119-af754fa4d441"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num Video Frontal</th>\n",
       "      <th>Num Video Lateral</th>\n",
       "      <th>NumIdx</th>\n",
       "      <th>Action</th>\n",
       "      <th>Feet Out 30 F</th>\n",
       "      <th>Score: Frontal( - / 1)</th>\n",
       "      <th>Whole Feet Flat On the Floor (1) L</th>\n",
       "      <th>Bend Hips and Knees Simultaniously (1) L</th>\n",
       "      <th>Hips backwards (1) L</th>\n",
       "      <th>Lower back neural (1) L</th>\n",
       "      <th>Hips are lower than knees level (1 point) L</th>\n",
       "      <th>Score: Lateral ( - / 5)</th>\n",
       "      <th>Total Score - /6</th>\n",
       "      <th>class</th>\n",
       "      <th>rating</th>\n",
       "      <th>front_pose</th>\n",
       "      <th>lat_pose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Squat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[[0.49896547198295593, 0.24871453642845154, -...</td>\n",
       "      <td>[[[0.48301321268081665, 0.2528548836708069, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Squat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[[[0.5140300393104553, 0.22558534145355225, -0...</td>\n",
       "      <td>[[[0.5150753259658813, 0.25385501980781555, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Squat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[[[0.5227701663970947, 0.22155773639678955, -0...</td>\n",
       "      <td>[[[0.523187518119812, 0.23705779016017914, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Squat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[[[0.5229289531707764, 0.2611807584762573, -0....</td>\n",
       "      <td>[[[0.5000579953193665, 0.24061907827854156, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Squat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[[[0.5273605585098267, 0.22662483155727386, -0...</td>\n",
       "      <td>[[[0.5319615006446838, 0.2524169683456421, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>Squat</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...</td>\n",
       "      <td>[[[0.3942027986049652, 0.23040449619293213, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>2</td>\n",
       "      <td>Squat</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[[0.4873969852924347, 0.1548309326171875, -0....</td>\n",
       "      <td>[[[0.4314277470111847, 0.22929038107395172, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>3</td>\n",
       "      <td>Squat</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[[0.4695884585380554, 0.15741464495658875, -0...</td>\n",
       "      <td>[[[0.39387422800064087, 0.2241220325231552, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>Squat</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[[0.47849878668785095, 0.20681476593017578, -...</td>\n",
       "      <td>[[[0.41384050250053406, 0.22573530673980713, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>87</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>Squat</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[[[0.43349573016166687, 0.1754622608423233, -0...</td>\n",
       "      <td>[[[0.3537933826446533, 0.18060621619224548, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Num Video Frontal  Num Video Lateral  NumIdx Action  Feet Out 30 F  \\\n",
       "0                    1                  2       0  Squat              0   \n",
       "1                    1                  2       1  Squat              0   \n",
       "2                    1                  2       2  Squat              0   \n",
       "3                    1                  2       3  Squat              0   \n",
       "4                    1                  2       4  Squat              0   \n",
       "..                 ...                ...     ...    ...            ...   \n",
       "201                 85                 86       1  Squat              1   \n",
       "202                 85                 86       2  Squat              1   \n",
       "203                 85                 86       3  Squat              1   \n",
       "204                 85                 86       4  Squat              1   \n",
       "205                 87                 88       0  Squat              1   \n",
       "\n",
       "     Score: Frontal( - / 1)  Whole Feet Flat On the Floor (1) L  \\\n",
       "0                         0                                   1   \n",
       "1                         0                                   0   \n",
       "2                         0                                   0   \n",
       "3                         0                                   0   \n",
       "4                         0                                   0   \n",
       "..                      ...                                 ...   \n",
       "201                       1                                   0   \n",
       "202                       1                                   0   \n",
       "203                       1                                   0   \n",
       "204                       1                                   0   \n",
       "205                       1                                   0   \n",
       "\n",
       "     Bend Hips and Knees Simultaniously (1) L  Hips backwards (1) L  \\\n",
       "0                                           1                     1   \n",
       "1                                           1                     1   \n",
       "2                                           1                     1   \n",
       "3                                           1                     1   \n",
       "4                                           1                     1   \n",
       "..                                        ...                   ...   \n",
       "201                                         1                     0   \n",
       "202                                         1                     0   \n",
       "203                                         1                     0   \n",
       "204                                         1                     0   \n",
       "205                                         1                     0   \n",
       "\n",
       "     Lower back neural (1) L  Hips are lower than knees level (1 point) L  \\\n",
       "0                          1                                            0   \n",
       "1                          1                                            0   \n",
       "2                          1                                            0   \n",
       "3                          1                                            0   \n",
       "4                          1                                            0   \n",
       "..                       ...                                          ...   \n",
       "201                        1                                            1   \n",
       "202                        1                                            1   \n",
       "203                        1                                            1   \n",
       "204                        1                                            1   \n",
       "205                        1                                            0   \n",
       "\n",
       "     Score: Lateral ( - / 5)  Total Score - /6  class  rating  \\\n",
       "0                          4                 4      1       4   \n",
       "1                          3                 3      1       3   \n",
       "2                          3                 3      1       3   \n",
       "3                          3                 3      1       3   \n",
       "4                          3                 3      1       3   \n",
       "..                       ...               ...    ...     ...   \n",
       "201                        3                 4      1       4   \n",
       "202                        3                 4      1       4   \n",
       "203                        3                 4      1       4   \n",
       "204                        3                 4      1       4   \n",
       "205                        2                 3      1       3   \n",
       "\n",
       "                                            front_pose  \\\n",
       "0    [[[0.49896547198295593, 0.24871453642845154, -...   \n",
       "1    [[[0.5140300393104553, 0.22558534145355225, -0...   \n",
       "2    [[[0.5227701663970947, 0.22155773639678955, -0...   \n",
       "3    [[[0.5229289531707764, 0.2611807584762573, -0....   \n",
       "4    [[[0.5273605585098267, 0.22662483155727386, -0...   \n",
       "..                                                 ...   \n",
       "201  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
       "202  [[[0.4873969852924347, 0.1548309326171875, -0....   \n",
       "203  [[[0.4695884585380554, 0.15741464495658875, -0...   \n",
       "204  [[[0.47849878668785095, 0.20681476593017578, -...   \n",
       "205  [[[0.43349573016166687, 0.1754622608423233, -0...   \n",
       "\n",
       "                                              lat_pose  \n",
       "0    [[[0.48301321268081665, 0.2528548836708069, 0....  \n",
       "1    [[[0.5150753259658813, 0.25385501980781555, 0....  \n",
       "2    [[[0.523187518119812, 0.23705779016017914, 0.1...  \n",
       "3    [[[0.5000579953193665, 0.24061907827854156, 0....  \n",
       "4    [[[0.5319615006446838, 0.2524169683456421, 0.0...  \n",
       "..                                                 ...  \n",
       "201  [[[0.3942027986049652, 0.23040449619293213, -0...  \n",
       "202  [[[0.4314277470111847, 0.22929038107395172, -0...  \n",
       "203  [[[0.39387422800064087, 0.2241220325231552, -0...  \n",
       "204  [[[0.41384050250053406, 0.22573530673980713, -...  \n",
       "205  [[[0.3537933826446533, 0.18060621619224548, -0...  \n",
       "\n",
       "[206 rows x 17 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_squat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cXvpCRM5U_B",
    "outputId": "644634f3-f7a9-4ecd-ada0-47ec9f8d097f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            front_pose  \\\n",
      "0    [[[0.6141521334648132, 0.2384583204984665, -0....   \n",
      "1    [[[0.7448023557662964, 0.29284247756004333, -0...   \n",
      "2    [[[0.6720446944236755, 0.26214927434921265, -1...   \n",
      "3    [[[0.7419819831848145, 0.2703408896923065, -0....   \n",
      "4    [[[0.7648568749427795, 0.3041684925556183, -1....   \n",
      "..                                                 ...   \n",
      "264  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "265  [[[0.6985482573509216, 0.3878808617591858, -0....   \n",
      "266  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "267  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "268  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "\n",
      "                                              lat_pose  \n",
      "0    [[[0.5667536854743958, 0.2648758590221405, -0....  \n",
      "1    [[[0.5568003058433533, 0.25841817259788513, 0....  \n",
      "2    [[[0.6044440865516663, 0.25434428453445435, -0...  \n",
      "3    [[[0.614557683467865, 0.25464609265327454, -0....  \n",
      "4    [[[0.591785728931427, 0.26665791869163513, 0.0...  \n",
      "..                                                 ...  \n",
      "264  [[[0.6106436848640442, 0.25392264127731323, -0...  \n",
      "265  [[[0.6022240519523621, 0.23920293152332306, -0...  \n",
      "266  [[[0.6188439130783081, 0.1534697264432907, -0....  \n",
      "267  [[[0.5743303894996643, 0.17498759925365448, -0...  \n",
      "268  [[[0.5832579731941223, 0.18293842673301697, -0...  \n",
      "\n",
      "[269 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_dead = pd.read_excel(base_path + 'deadlift_edited.xlsx')\n",
    "\n",
    "# Load JSON files and convert to NumPy arrays\n",
    "def load_json_as_numpy(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)  # Load the JSON data\n",
    "    return np.array(data)  # Convert the list to a NumPy array\n",
    "\n",
    "# Load the front and lateral poses\n",
    "front_pose_array = load_json_as_numpy(base_path + 'front_pose_dead.json')\n",
    "lat_pose_array = load_json_as_numpy(base_path + 'lat_pose_dead.json')\n",
    "\n",
    "# Make sure `front_pose` and `lat_pose` columns exist\n",
    "if 'front_pose' not in df_dead.columns:\n",
    "    df_dead['front_pose'] = None\n",
    "if 'lat_pose' not in df_dead.columns:\n",
    "    df_dead['lat_pose'] = None\n",
    "\n",
    "# Assign the loaded arrays back to the DataFrame\n",
    "# Make sure the lengths match\n",
    "if len(front_pose_array) == len(df_dead) and len(lat_pose_array) == len(df_dead):\n",
    "    df_dead['front_pose'] = list(front_pose_array)\n",
    "    df_dead['lat_pose'] = list(lat_pose_array)\n",
    "else:\n",
    "    print(\"Error: The length of the loaded arrays does not match the DataFrame.\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_dead[['front_pose', 'lat_pose']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "p7af0bK55U_B"
   },
   "outputs": [],
   "source": [
    "# Define your split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "total_samples = len(df_dead)\n",
    "train_size = int(total_samples * train_ratio)\n",
    "val_size = int(total_samples * val_ratio)\n",
    "\n",
    "# Split the DataFrame\n",
    "train_df_dead = df_dead.iloc[:train_size]                    # First 70% for training\n",
    "val_df_dead = df_dead.iloc[train_size:train_size + val_size]  # Next 15% for validation\n",
    "test_df_dead = df_dead.iloc[train_size + val_size:]           # Last 15% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-pXveAf5U_B",
    "outputId": "0ab18bab-bc66-453a-9998-77a8b4a42b21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            front_pose  \\\n",
      "0    [[[0.5307167768478394, 0.26830747723579407, -0...   \n",
      "1    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "2    [[[0.5470829606056213, 0.22924844920635223, -0...   \n",
      "3    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "4    [[[0.5109826922416687, 0.2688097357749939, -0....   \n",
      "..                                                 ...   \n",
      "101  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "102  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "103  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "104  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "105  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...   \n",
      "\n",
      "                                              lat_pose  \n",
      "0    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "1    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "2    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "3    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "4    [[[0.3619050979614258, 0.2965300977230072, -0....  \n",
      "..                                                 ...  \n",
      "101  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "102  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "103  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "104  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "105  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
      "\n",
      "[106 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_lunge = pd.read_excel(base_path + 'lunges_edited.xlsx')\n",
    "\n",
    "# Load JSON files and convert to NumPy arrays\n",
    "def load_json_as_numpy(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)  # Load the JSON data\n",
    "    return np.array(data)  # Convert the list to a NumPy array\n",
    "\n",
    "# Load the front and lateral poses\n",
    "front_pose_array = load_json_as_numpy(base_path + 'front_pose_lunges.json')\n",
    "lat_pose_array = load_json_as_numpy(base_path + 'lat_pose_lunges.json')\n",
    "\n",
    "# Make sure `front_pose` and `lat_pose` columns exist\n",
    "if 'front_pose' not in df_lunge.columns:\n",
    "    df_lunge['front_pose'] = None\n",
    "if 'lat_pose' not in df_lunge.columns:\n",
    "    df_lunge['lat_pose'] = None\n",
    "\n",
    "# Assign the loaded arrays back to the DataFrame\n",
    "# Make sure the lengths match\n",
    "if len(front_pose_array) == len(df_lunge) and len(lat_pose_array) == len(df_lunge):\n",
    "    df_lunge['front_pose'] = list(front_pose_array)\n",
    "    df_lunge['lat_pose'] = list(lat_pose_array)\n",
    "else:\n",
    "    print(\"Error: The length of the loaded arrays does not match the DataFrame.\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_lunge[['front_pose', 'lat_pose']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wJSBgqjs5U_C"
   },
   "outputs": [],
   "source": [
    "# Define your split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "total_samples = len(df_lunge)\n",
    "train_size = int(total_samples * train_ratio)\n",
    "val_size = int(total_samples * val_ratio)\n",
    "\n",
    "# Split the DataFrame\n",
    "train_df_lunge = df_lunge.iloc[:train_size]                    # First 70% for training\n",
    "val_df_lunge = df_lunge.iloc[train_size:train_size + val_size]  # Next 15% for validation\n",
    "test_df_lunge = df_lunge.iloc[train_size + val_size:]           # Last 15% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kHrmZE3a5U_C"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Keep only the specified columns and concatenate the DataFrames\n",
    "train_df = pd.concat([\n",
    "    train_df_squat,\n",
    "    train_df_lunge,\n",
    "    train_df_dead\n",
    "])\n",
    "\n",
    "train_df = train_df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "val_df = pd.concat([\n",
    "    val_df_squat,\n",
    "    val_df_lunge,\n",
    "    val_df_dead\n",
    "])\n",
    "\n",
    "val_df = val_df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "test_df = pd.concat([\n",
    "    test_df_squat,\n",
    "    test_df_lunge,\n",
    "    test_df_dead\n",
    "])\n",
    "\n",
    "test_df = test_df.sample(frac=1, random_state=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArEquwo729de"
   },
   "source": [
    "### `Video_Model`\n",
    "- **Description**: A model that utilizes a pretrained VideoMAE model for video representation learning.\n",
    "- **Methods**:\n",
    "  - `forward(pixel_values, bool_masked_pos)`: Forward pass through the VideoMAE model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VDb-Go7SBJD_"
   },
   "outputs": [],
   "source": [
    "class Video_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Video_Model is a PyTorch neural network module that leverages a pretrained\n",
    "    VideoMAE model for video representation learning.\n",
    "\n",
    "    Attributes:\n",
    "        videomae (AutoModelForPreTraining): Pretrained VideoMAE model.\n",
    "        pooling (nn.AdaptiveAvgPool1d): Layer for mean pooling of patch embeddings.\n",
    "        fc1, fc2, fc3 (nn.Linear): Fully connected layers for additional feature extraction.\n",
    "        relu1, relu2, relu3 (nn.ReLU): ReLU activation functions.\n",
    "\n",
    "    Methods:\n",
    "        forward(pixel_values, bool_masked_pos): Performs the forward pass using pixel values.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model_name=\"MCG-NJU/videomae-base\", hidden_size=2048):\n",
    "        super(Video_Model, self).__init__()\n",
    "\n",
    "        # Load the pretrained VideoMAE model\n",
    "        self.videomae = AutoModelForPreTraining.from_pretrained(pretrained_model_name)\n",
    "\n",
    "        # Pooling layer to aggregate the patch embeddings (mean pooling)\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)  # Converts [batch_size, 782, 1536] -> [batch_size, 1536]\n",
    "\n",
    "        # MLP layers with more depth (no final layer)\n",
    "        self.fc1 = nn.Linear(1536, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, pixel_values, bool_masked_pos=None):\n",
    "        # Forward pass through the pretrained VideoMAE model\n",
    "        outputs = self.videomae(pixel_values, bool_masked_pos=bool_masked_pos)\n",
    "\n",
    "        # Get the output embeddings [batch_size, 782, 1536]\n",
    "        embeddings = outputs[1]\n",
    "\n",
    "        # Apply pooling to get [batch_size, 1536]\n",
    "        pooled_embeddings = self.pooling(embeddings.permute(0, 2, 1)).squeeze(-1)\n",
    "\n",
    "        # Pass through the deeper MLP\n",
    "        x = self.fc1(pooled_embeddings)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        # Return the final feature vector instead of predictions\n",
    "        return x  # Shape: [batch_size, hidden_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWS-aEv93AO-"
   },
   "source": [
    "### `Pose_Model`\n",
    "- **Description**: A model that processes pose landmarks to output a feature vector.\n",
    "- **Methods**:\n",
    "  - `forward(pose_landmarks)`: Forward pass through the Pose model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fjTdgCRKCjfX"
   },
   "outputs": [],
   "source": [
    "class Pose_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Pose_Model is a PyTorch neural network module designed to process pose landmarks\n",
    "    and output a feature vector for further analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size=33 * 3, hidden_size1=512, hidden_size2=1024, hidden_size3=256, final_size=2048, dropout_rate=0.5):\n",
    "        super(Pose_Model, self).__init__()\n",
    "\n",
    "        # Define the neural network layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)  # First hidden layer with reduced size\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size1)  # Batch Normalization\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)  # Dropout for regularization\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)  # Second hidden layer with increased size\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size2)  # Batch Normalization\n",
    "        self.relu2 = nn.LeakyReLU(0.2)  # Leaky ReLU activation\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)  # Dropout for regularization\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)  # Third hidden layer with reduced size\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size3)  # Batch Normalization\n",
    "        self.relu3 = nn.ELU()  # ELU activation\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)  # Dropout for regularization\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_size3, final_size)  # Output layer\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "    def forward(self, pose_landmarks):\n",
    "        \"\"\"\n",
    "        Forward pass through the Pose Model.\n",
    "\n",
    "        pose_landmarks: Tensor of shape [batch_size, num_frames, 33, 3]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            batch_size, num_frames, _, _ = pose_landmarks.shape\n",
    "\n",
    "            # Flatten the pose landmarks for each frame: [batch_size, num_frames, 33*3]\n",
    "            pose_landmarks = pose_landmarks.view(batch_size, num_frames, -1)\n",
    "\n",
    "            # Process all frames at once\n",
    "            x = self.fc1(pose_landmarks.view(-1, pose_landmarks.size(-1)))  # Shape: [batch_size*num_frames, hidden_size1]\n",
    "            x = self.bn1(x)  # Batch Normalization\n",
    "            x = self.relu1(x)\n",
    "            x = self.dropout1(x)  # Apply Dropout\n",
    "\n",
    "            x = self.fc2(x)\n",
    "            x = self.bn2(x)  # Batch Normalization\n",
    "            x = self.relu2(x)\n",
    "            x = self.dropout2(x)  # Apply Dropout\n",
    "\n",
    "            x = self.fc3(x)\n",
    "            x = self.bn3(x)  # Batch Normalization\n",
    "            x = self.relu3(x)\n",
    "            x = self.dropout3(x)  # Apply Dropout\n",
    "\n",
    "            x = self.fc4(x)  # Final layer without additional activation (could add if needed)\n",
    "            x = self.relu4(x)\n",
    "\n",
    "            # Reshape back to [batch_size, num_frames, final_size]\n",
    "            frame_features = x.view(batch_size, num_frames, -1)  # Shape: [batch_size, num_frames, final_size]\n",
    "\n",
    "            # Aggregate the frame features (mean pooling)\n",
    "            pooled_features = frame_features.mean(dim=1)  # Shape: [batch_size, final_size]\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred during bala:\")\n",
    "            print(f\"Error: {e}\")\n",
    "        return pooled_features  # Shape: [batch_size, final_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UXT9pOQ3CLt"
   },
   "source": [
    "### `Combined_Video_Pose_Model`\n",
    "- **Description**: Combines the outputs of `Video_Model` and `Pose_Model` for comprehensive predictions.\n",
    "- **Methods**:\n",
    "  - `forward(pixel_values, bool_masked_pos, pose_landmarks)`: Combines features from video and pose models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BrwwwfUoDvLl"
   },
   "outputs": [],
   "source": [
    "class Combined_Video_Pose_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined_Video_Pose_Model integrates both video and pose models to provide\n",
    "    a comprehensive hidden representation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, video_hidden_size=2048, pose_hidden_size=2048, combined_hidden_size=1024, hidden_layer_size=2048, dropout_rate=0.5):\n",
    "        super(Combined_Video_Pose_Model, self).__init__()\n",
    "\n",
    "        # Video model (returns a 2048-dimensional feature vector)\n",
    "        self.video_model = Video_Model(hidden_size=video_hidden_size)\n",
    "\n",
    "        # Pose model (returns a 2048-dimensional feature vector)\n",
    "        self.pose_model = Pose_Model(final_size=pose_hidden_size)\n",
    "\n",
    "        # Separate processing for video features\n",
    "        self.video_fc1 = nn.Linear(video_hidden_size, combined_hidden_size)\n",
    "        self.video_relu1 = nn.ReLU()\n",
    "        self.video_dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Separate processing for pose features\n",
    "        self.pose_fc1 = nn.Linear(pose_hidden_size, combined_hidden_size)\n",
    "        self.pose_relu1 = nn.ReLU()\n",
    "        self.pose_dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Combine the features from the two models\n",
    "        combined_input_size = 2 * combined_hidden_size  # Combined input from both models\n",
    "\n",
    "        # Fully connected layers for the combined model\n",
    "        self.fc1 = nn.Linear(combined_input_size, hidden_layer_size)  # Input should be 4096\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_layer_size, combined_hidden_size)  # Output layer to maintain original architecture\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, pixel_values, bool_masked_pos, pose_landmarks):\n",
    "        \"\"\"\n",
    "        Forward pass through the combined model.\n",
    "        Args:\n",
    "            pixel_values: Input to the Video Model.\n",
    "            bool_masked_pos: Masking input to the Video Model.\n",
    "            pose_landmarks: Input to the Pose Model.\n",
    "\n",
    "        Returns:\n",
    "            x: The hidden representation of size 1024.\n",
    "        \"\"\"\n",
    "        # Ensure to log the input shapes for debuggi\n",
    "\n",
    "        # Forward pass through the video model\n",
    "        video_features = self.video_model(pixel_values, bool_masked_pos=bool_masked_pos)\n",
    "        if video_features is None:\n",
    "            raise ValueError(\"video_model returned None.\")\n",
    "\n",
    "        # Process video features separately\n",
    "        video_processed = self.video_fc1(video_features)\n",
    "        video_processed = self.video_relu1(video_processed)\n",
    "        video_processed = self.video_dropout1(video_processed)\n",
    "\n",
    "        # Forward pass through the pose model\n",
    "        pose_features = self.pose_model(pose_landmarks)\n",
    "        if pose_features is None:\n",
    "            raise ValueError(\"pose_model returned None.\")\n",
    "\n",
    "        # Process pose features separately\n",
    "        pose_processed = self.pose_fc1(pose_features)\n",
    "        pose_processed = self.pose_relu1(pose_processed)\n",
    "        pose_processed = self.pose_dropout1(pose_processed)\n",
    "\n",
    "        # Concatenate the processed feature vectors from both models\n",
    "        combined_features = torch.cat((video_processed, pose_processed), dim=1)  # Shape should be [batch_size, 4096]\n",
    "\n",
    "        # Pass through the combined MLP\n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Pass through the second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x  # Return hidden layer output only (Shape: [batch_size, combined_hidden_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Y8h4fUACDZm"
   },
   "source": [
    "## `Dual_Combined_Model`\n",
    "\n",
    "- **Description**: Combines two instances of `Combined_Video_Pose_Model` to produce classification and rating outputs.\n",
    "\n",
    "- **Methods**:\n",
    "  - `forward(pixel_values_1, bool_masked_pos_1, pose_landmarks_tensor_1, pixel_values_2, bool_masked_pos_2, pose_landmarks_tensor_2)`: Forward pass through the dual model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2HufVACb4BVN"
   },
   "outputs": [],
   "source": [
    "class Dual_Combined_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual_Combined_Model combines two instances of Combined_Video_Pose_Model\n",
    "    to produce classification and rating outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Dual_Combined_Model, self).__init__()\n",
    "\n",
    "        # Initialize two instances of Combined_Video_Pose_Model\n",
    "        self.model_1 = Combined_Video_Pose_Model()\n",
    "        self.model_2 = Combined_Video_Pose_Model()\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        self.classification_layer = nn.Linear(2048, 512)  # Initial layer after concatenation\n",
    "        self.classification_relu = nn.ReLU()\n",
    "        self.classification_dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.classification_output = nn.Linear(512, 3)  # Output layer for 3 classes\n",
    "\n",
    "    def forward(self, pixel_values_1, bool_masked_pos_1, pose_landmarks_tensor_1,\n",
    "                pixel_values_2, bool_masked_pos_2, pose_landmarks_tensor_2):\n",
    "        try:\n",
    "            # Forward pass through the first combined model\n",
    "            hidden_output_1 = self.model_1(pixel_values_1, bool_masked_pos_1, pose_landmarks_tensor_1)\n",
    "\n",
    "            # Forward pass through the second combined model\n",
    "            hidden_output_2 = self.model_2(pixel_values_2, bool_masked_pos_2, pose_landmarks_tensor_2)\n",
    "\n",
    "            # Check for None outputs\n",
    "            if hidden_output_1 is None or hidden_output_2 is None:\n",
    "                raise ValueError(\"One of the models returned None.\")\n",
    "\n",
    "            # Concatenate the hidden outputs from both models\n",
    "            combined_hidden = torch.cat((hidden_output_1, hidden_output_2), dim=1)  # Shape: [batch_size, 4096]\n",
    "\n",
    "            # Classification path\n",
    "            classification_hidden = self.classification_layer(combined_hidden)\n",
    "            classification_hidden = self.classification_relu(classification_hidden)\n",
    "            classification_hidden = self.classification_dropout(classification_hidden)\n",
    "\n",
    "            classification_output = self.classification_output(classification_hidden)  # Shape: [batch_size, 3]\n",
    "\n",
    "            return classification_output, combined_hidden    # Return outputs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred during forward pass:\")\n",
    "            print(f\"Error: {e}\")\n",
    "            raise  # Reraise the exception to maintain the stack trace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHyY8adi9rN6"
   },
   "source": [
    "## `CriteriaPredictionModel`\n",
    "\n",
    "- **Description**: A deep neural network for predicting multiple binary ratings (yes/no) using a series of fully connected layers, ReLU activations, and dropout for regularization. The model outputs a probability for each rating.\n",
    "\n",
    "- **Methods**:\n",
    "  - `forward(x)`: Forward pass through the network. The input is passed through five fully connected layers, each with ReLU and dropout, and the final output is processed by a sigmoid activation to produce binary classification probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rQAWzMLI5U_E"
   },
   "outputs": [],
   "source": [
    "class CriteriaPredictionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    RatingPredictionModel predicts multiple yes/no ratings with a deeper network architecture.\n",
    "    The model uses binary classification for each output rating (5 in total).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=4096, hidden_size1=2048, hidden_size2=1024, output_size=5, dropout_rate=0.5):\n",
    "        super(CriteriaPredictionModel, self).__init__()\n",
    "\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Second fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Adding more depth\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size2 // 2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_size2 // 2, hidden_size2 // 4)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc5 = nn.Linear(hidden_size2 // 4, hidden_size2 // 8)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout5 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Output layer for binary classification (yes/no for each of the 5 ratings)\n",
    "        self.output_layer = nn.Linear(hidden_size2 // 8, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid for binary yes/no predictions\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network with multiple layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.dropout5(x)\n",
    "\n",
    "        # Final binary classification\n",
    "        ratings = self.output_layer(x)\n",
    "        return self.sigmoid(ratings)  # Returns 5 values, each in range [0, 1] for yes/no classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PytDrrlz9xq7"
   },
   "source": [
    "## `DeepClassificationWithRatingModel`\n",
    "\n",
    "- **Description**: The `DeepClassificationWithRatingModel` integrates the `DeepDualCombinedModel` and adds a separate rating prediction model. If the classification output predicts class `0`, the rating model is triggered. It uses different criteria models for deadlift, squat, and lunges based on the predicted class.\n",
    "\n",
    "- **Methods**:\n",
    "  - `forward(combined_hidden, predicted_class)`:\n",
    "    - Takes the `combined_hidden` state and the `predicted_class`.\n",
    "    - If class `0` is predicted, the `dead_criteria_model` is used to predict ratings.\n",
    "    - If class `1` is predicted, the `squat_criteria_model` is used.\n",
    "    - If class `2` is predicted, the `lunges_criteria_model` is used.\n",
    "    - Returns the predicted ratings based on the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xisbxmLc5U_E"
   },
   "outputs": [],
   "source": [
    "class DeepClassificationWithRatingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepClassificationWithRatingModel integrates the DeepDualCombinedModel and\n",
    "    adds a separate rating prediction model. If the classification output predicts class `0`,\n",
    "    the rating model is triggered.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DeepClassificationWithRatingModel, self).__init__()\n",
    "\n",
    "        # Rating model with more layers\n",
    "        self.dead_criteria_model = CriteriaPredictionModel(input_size=2048, hidden_size1=2048, hidden_size2=1024, output_size=5, dropout_rate=0.5)\n",
    "        self.lunges_criteria_model = CriteriaPredictionModel(input_size=2048, hidden_size1=2048, hidden_size2=1024, output_size=7, dropout_rate=0.5)\n",
    "        self.squat_criteria_model = CriteriaPredictionModel(input_size=2048, hidden_size1=2048, hidden_size2=1024, output_size=6, dropout_rate=0.5)\n",
    "\n",
    "    def forward(self,combined_hidden,predicted_class):\n",
    "        try:\n",
    "            # Initialize ratings as None\n",
    "            ratings = None\n",
    "\n",
    "            # If class `0` is predicted, trigger rating prediction\n",
    "            if predicted_class == 0:\n",
    "                ratings = self.dead_criteria_model(combined_hidden)\n",
    "            elif predicted_class == 1:\n",
    "                ratings = self.squat_criteria_model(combined_hidden)\n",
    "            elif predicted_class == 2:\n",
    "                ratings = self.lunges_criteria_model(combined_hidden)\n",
    "            return ratings\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during forward pass in DeepClassificationWithRatingModel: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmiORcTVCe8E"
   },
   "source": [
    "## `PoseVideoDataset`\n",
    "\n",
    "- **Description**: A custom PyTorch Dataset designed to load video frames and pose landmarks, along with action class labels and ratings. It supports deadlift, squat, and lunge actions with different rating models for each. The dataset processes video frames, extracts pose landmarks, and normalizes ratings based on action class.\n",
    "\n",
    "- **Methods**:\n",
    "  - `__len__()`: Returns the number of samples in the dataset.\n",
    "  - `__getitem__(idx)`: Loads and processes the data at the specified index:\n",
    "    - Loads video frames for both frontal and lateral views.\n",
    "    - Processes and normalizes video frames using the processor.\n",
    "    - Extracts pose landmarks and action class labels.\n",
    "    - Retrieves and normalizes ratings based on action class (Deadlift, Squat, or Lunge).\n",
    "  - `_process_ratings(df)`: Processes and normalizes the ratings data from the corresponding DataFrame:\n",
    "    - Extracts relevant columns (ending in 'F' or 'L').\n",
    "    - Normalizes the scores and applies a threshold (0 or 1 based on the condition).\n",
    "    - Returns the mean of the scores for each rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gzjcU07Q9FLq"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PoseVideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset to load video frames and pose landmarks, along with class and ratings labels.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A DataFrame containing the dataset information, including video paths and labels.\n",
    "        num_frames (int): Number of frames to load per video.\n",
    "        processor: A pre-processing function to resize and normalize video frames.\n",
    "        train_df_dead (pd.DataFrame): DataFrame containing ratings for deadlifts.\n",
    "        train_df_squat (pd.DataFrame): DataFrame containing ratings for squats.\n",
    "        train_df_lunge (pd.DataFrame): DataFrame containing ratings for lunges.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the processed video frames, pose landmarks, labels, and ratings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, num_frames, processor):\n",
    "        self.df = df\n",
    "        self.num_frames = num_frames\n",
    "        self.processor = processor\n",
    "        self.train_df_dead = train_df_dead\n",
    "        self.train_df_squat = train_df_squat\n",
    "        self.train_df_lunge = train_df_lunge\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Get frontal and lateral video paths and index\n",
    "        num_video_frontal = row['Num Video Frontal']\n",
    "        num_video_lateral = row['Num Video Lateral']\n",
    "        num_idx = row['NumIdx']\n",
    "        action = row['Action']\n",
    "\n",
    "        # Load video frames and extract pose landmarks\n",
    "        video_frames_frontal = load_and_resize_frames(num_video_frontal, action, 1, num_idx, self.num_frames)\n",
    "        video_frames_lateral = load_and_resize_frames(num_video_lateral, action, 0, num_idx, self.num_frames)\n",
    "\n",
    "        # Process the video frames to get pixel values\n",
    "        pixel_values_frontal = self.processor(list(np.clip(np.array(video_frames_frontal), 0, 255)), return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        pixel_values_lateral = self.processor(list(np.clip(np.array(video_frames_lateral), 0, 255)), return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "\n",
    "        model = AutoModelForPreTraining.from_pretrained(\"MCG-NJU/videomae-base\")  # Load model for masking\n",
    "        num_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\n",
    "        seq_length = (16 // model.config.tubelet_size) * num_patches_per_frame\n",
    "        bool_masked_pos_frontal = torch.randint(0, 2, (seq_length,)).bool()\n",
    "        bool_masked_pos_lateral = torch.randint(0, 2, (seq_length,)).bool()\n",
    "\n",
    "        # Extract pose landmarks from video frames\n",
    "        pose_landmarks_frontal = row['front_pose']\n",
    "        pose_landmarks_lateral = row['lat_pose']\n",
    "        pose_landmarks_tensor_frontal = torch.tensor(pose_landmarks_frontal).float()\n",
    "        pose_landmarks_tensor_lateral = torch.tensor(pose_landmarks_lateral).float()\n",
    "\n",
    "        # Get labels (action class)\n",
    "        label_class = torch.tensor(row['class'], dtype=torch.long)\n",
    "\n",
    "        # Initialize ratings\n",
    "        ratings = None\n",
    "\n",
    "        # Check label_class and load appropriate DataFrame\n",
    "        if label_class.item() == 0:  # Deadlift\n",
    "            ratings = self._process_ratings(self.train_df_dead,row)\n",
    "        elif label_class.item() == 1:  # Squat\n",
    "            ratings = self._process_ratings(self.train_df_squat,row)\n",
    "        elif label_class.item() == 2:  # Lunge\n",
    "            ratings = self._process_ratings(self.train_df_lunge,row)\n",
    "\n",
    "        # Convert ratings to tensor\n",
    "        ratings = torch.tensor(ratings, dtype=torch.float32) if ratings is not None else None\n",
    "\n",
    "        return (pixel_values_frontal, bool_masked_pos_frontal, pose_landmarks_tensor_frontal,\n",
    "                pixel_values_lateral, bool_masked_pos_lateral, pose_landmarks_tensor_lateral,\n",
    "                label_class, ratings)\n",
    "\n",
    "    def _process_ratings(self, df,row):\n",
    "        \"\"\"\n",
    "        Process the ratings DataFrame to extract and normalize scores.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing ratings for the specific action.\n",
    "\n",
    "        Returns:\n",
    "            list: Normalized and thresholded scores.\n",
    "        \"\"\"\n",
    "        # Select columns ending with 'F' or 'L'\n",
    "        relevant_columns = [col for col in df.columns if col.endswith('F') or col.endswith('L')]\n",
    "\n",
    "        # Extract ratings and normalize\n",
    "        scores = row[relevant_columns].values\n",
    "\n",
    "\n",
    "        # Apply threshold: Convert to 0 or 1 based on specific thresholding conditions\n",
    "        thresholded_scores = np.where(scores >= 0.5, 1, 0)\n",
    "\n",
    "        return thresholded_scores.tolist()  # Return mean of the scores for each sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CBl_FBpQBa1e"
   },
   "outputs": [],
   "source": [
    "def create_dataloader(df, num_frames, processor, batch_size=8, shuffle=True):\n",
    "    \"\"\"\n",
    "    Creates a PyTorch DataLoader from the PoseVideoDataset.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the dataset information.\n",
    "        num_frames (int): The number of frames to extract from each video.\n",
    "        processor: A video frame pre-processing function.\n",
    "        batch_size (int, optional): Batch size for the DataLoader. Defaults to 8.\n",
    "        shuffle (bool, optional): Whether to shuffle the data. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: A PyTorch DataLoader for the dataset.\n",
    "    \"\"\"\n",
    "    dataset = PoseVideoDataset(df, num_frames, processor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvW6aZGJ-QGS"
   },
   "source": [
    "## `train_combined_model`\n",
    "\n",
    "- **Description**: This function trains both the classification model (`Dual_Combined_Model`) and the rating prediction model (`CriteriaPredictionModel`). It evaluates them after each epoch and saves the best-performing models based on validation loss. It also includes early stopping if no improvement is observed for a set number of epochs.\n",
    "\n",
    "- **Args**:\n",
    "  - `model (torch.nn.Module)`: The classification model (e.g., `Dual_Combined_Model`).\n",
    "  - `criteria_model (torch.nn.Module)`: The rating prediction model (e.g., `CriteriaPredictionModel`).\n",
    "  - `train_dataloader (DataLoader)`: A DataLoader providing the training data.\n",
    "  - `eval_dataloader (DataLoader)`: A DataLoader providing the evaluation data.\n",
    "  - `epochs (int, optional)`: Number of epochs to train. Defaults to 1000.\n",
    "  - `lr (float, optional)`: Learning rate for the optimizer. Defaults to 1e-4.\n",
    "  - `device (str, optional)`: The device to train the model on ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "  - `clip_grad_norm (float, optional)`: Maximum norm for gradient clipping. Defaults to 1.0.\n",
    "  - `patience (int, optional)`: Number of epochs with no improvement after which training will be stopped. Defaults to 5.\n",
    "\n",
    "- **Returns**:\n",
    "  - None: The function prints the training progress, validation loss, classification accuracy, and rating accuracy during training.\n",
    "\n",
    "- **Steps**:\n",
    "  1. **Model Initialization**: Moves both the classification and rating models to the specified device.\n",
    "  2. **Optimizer Setup**: Uses the Adam optimizer for both models with the specified learning rate.\n",
    "  3. **Loss Functions**: Defines loss functions for both classification (`CrossEntropyLoss`) and ratings (`BCEWithLogitsLoss`).\n",
    "  4. **Training Loop**:\n",
    "     - Performs a forward pass through the classification model and computes the classification loss.\n",
    "     - Computes the rating prediction loss only if the predicted class matches the actual class.\n",
    "     - Combines both the classification and rating losses and performs backpropagation.\n",
    "  5. **Gradient Clipping**: Clips gradients to avoid exploding gradients during backpropagation.\n",
    "  6. **Model Evaluation**: Evaluates the models after each epoch and prints the results.\n",
    "  7. **Early Stopping**: Monitors validation loss and triggers early stopping if no improvement is observed for a specified number of epochs.\n",
    "  8. **Model Checkpointing**: Saves the best-performing models based on validation loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PSZbVvGz-Itz"
   },
   "outputs": [],
   "source": [
    "def train_combined_model(model, criteria_model, train_dataloader, eval_dataloader, epochs=1000, lr=1e-4,\n",
    "                         device='cpu', clip_grad_norm=1.0, patience=5):\n",
    "    \"\"\"\n",
    "    Trains both the classification and rating prediction models, evaluates them after each epoch.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The classification model (Dual_Combined_Model).\n",
    "        criteria_model (torch.nn.Module): The rating prediction model (RatingPredictionModel).\n",
    "        train_dataloader (DataLoader): A DataLoader providing the training data.\n",
    "        eval_dataloader (DataLoader): A DataLoader providing the evaluation data.\n",
    "        epochs (int, optional): Number of epochs to train. Defaults to 1000.\n",
    "        lr (float, optional): Learning rate for the optimizer. Defaults to 1e-4.\n",
    "        device (str, optional): The device to train the model on ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "        clip_grad_norm (float, optional): Maximum norm for gradient clipping. Defaults to 1.0.\n",
    "        patience (int, optional): Number of epochs with no improvement after which training will be stopped. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the training progress, validation loss, classification accuracy, and rating accuracy.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    criteria_model.to(device)\n",
    "\n",
    "    # Set up optimizer and loss functions\n",
    "    optimizer = optim.Adam(list(model.parameters()) + list(criteria_model.parameters()), lr=lr)\n",
    "    criterion_class = nn.CrossEntropyLoss()  # Loss for classification task\n",
    "    criterion_ratings = nn.BCEWithLogitsLoss()  # Loss for rating task (binary)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "\n",
    "    best_eval_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        criteria_model.train()\n",
    "        running_loss = 0.0\n",
    "        a = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            # Unpack batch data and move to the specified device\n",
    "            (pixel_values_frontal, bool_masked_pos_frontal, pose_landmarks_tensor_frontal,\n",
    "             pixel_values_lateral, bool_masked_pos_lateral, pose_landmarks_tensor_lateral,\n",
    "             label_class, ratings) = [tensor.to(device) for tensor in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through the classification model\n",
    "            classification_output, combined_hidden = model(\n",
    "                pixel_values_frontal, bool_masked_pos_frontal, pose_landmarks_tensor_frontal,\n",
    "                pixel_values_lateral, bool_masked_pos_lateral, pose_landmarks_tensor_lateral\n",
    "            )\n",
    "            _, predicted_class = torch.max(classification_output, 1)\n",
    "\n",
    "            # Loss for classification\n",
    "            loss_class = criterion_class(classification_output, label_class)\n",
    "            print(f\"real class : {label_class} and predicted class : {predicted_class}\")\n",
    "            print(f\"loss class : {loss_class}\")\n",
    "            # Forward pass through the rating prediction model\n",
    "            # We need to trigger the correct rating model based on the predicted class\n",
    "            for i in range(label_class.size(0)):  # Iterate over each sample in the batch\n",
    "                actual_class = label_class[i].item()\n",
    "                predicted_class_item = predicted_class[i].item()\n",
    "                # Loss for ratings (if the predicted class matches the actual class)\n",
    "                if predicted_class_item == actual_class:\n",
    "                    if actual_class == 0:  # Deadlift\n",
    "                        ratings_output = criteria_model.dead_criteria_model(combined_hidden[i])\n",
    "                    elif actual_class == 1:  # Squat\n",
    "                        ratings_output = criteria_model.squat_criteria_model(combined_hidden[i])\n",
    "                    elif actual_class == 2:  # Lunges\n",
    "                        ratings_output = criteria_model.lunges_criteria_model(combined_hidden[i])\n",
    "\n",
    "                    print(f\"ratings : {ratings[i]} and ratings_output : {ratings_output}\")\n",
    "                    loss_ratings = criterion_ratings(ratings_output, ratings[i].float())\n",
    "                    \n",
    "                else:\n",
    "                    # High loss if the predicted class doesn't match\n",
    "                    loss_ratings = torch.tensor(100.0, device=device, requires_grad=True)\n",
    "                \n",
    "                print(f\"loss ratings : {loss_ratings}\")\n",
    "                # Combine classification and rating losses\n",
    "                loss = loss_class + loss_ratings\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(list(model.parameters()) + list(criteria_model.parameters()), clip_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                print(f\"{a} and data_size: {len(train_dataloader)} and epoch: {epoch + 1}\")\n",
    "                a += 1\n",
    "\n",
    "        avg_loss = running_loss / len(train_dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Evaluate both models after each epoch\n",
    "        eval_loss, eval_accuracy, eval_rating_accuracy = evaluate_combined_model(\n",
    "            model, criteria_model, eval_dataloader, device\n",
    "        )\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {eval_loss:.4f}, \"\n",
    "              f\"Classification Accuracy: {eval_accuracy:.4f}, \"\n",
    "              f\"Rating Accuracy (Deadlift): {eval_rating_accuracy['deadlift']:.4f}, \"\n",
    "              f\"Rating Accuracy (Squat): {eval_rating_accuracy['squat']:.4f}, \"\n",
    "              f\"Rating Accuracy (Lunges): {eval_rating_accuracy['lunges']:.4f}\")\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step(eval_loss)\n",
    "\n",
    "        # Check for early stopping\n",
    "        if eval_loss < best_eval_loss:\n",
    "            best_eval_loss = eval_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f\"best_combined_model_epoch_{epoch + 1}.pt\")\n",
    "            torch.save(criteria_model.state_dict(), f\"best_rating_model_epoch_{epoch + 1}.pt\")\n",
    "            print(\"Model checkpoint saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dusNp1b-Z1k"
   },
   "source": [
    "## `evaluate_combined_model`\n",
    "\n",
    "- **Description**: This function evaluates both the classification and rating prediction models on the provided dataset. It computes the average evaluation loss, classification accuracy, and rating accuracies for different actions (deadlift, squat, lunges) using a given `DataLoader` and loss functions.\n",
    "\n",
    "- **Args**:\n",
    "  - `classification_model (torch.nn.Module)`: The classification model (e.g., `Dual_Combined_Model`).\n",
    "  - `criteria_model (torch.nn.Module)`: The rating prediction model (e.g., `CriteriaPredictionModel`).\n",
    "  - `dataloader (DataLoader)`: A DataLoader providing the evaluation data.\n",
    "  - `criterion_class (nn.Module)`: The loss function for the classification task.\n",
    "  - `criterion_ratings (nn.Module)`: The loss function for the rating task.\n",
    "  - `device (str)`: The device to perform evaluation on ('cpu' or 'cuda').\n",
    "\n",
    "- **Returns**:\n",
    "  - `float`: The average evaluation loss across the dataset.\n",
    "  - `dict`: A dictionary containing the classification accuracy and rating accuracies for each action (deadlift, squat, lunges).\n",
    "\n",
    "- **Steps**:\n",
    "  1. **Set Evaluation Mode**: Sets the models (`classification_model` and `criteria_model`) to evaluation mode.\n",
    "  2. **Initialize Metrics**: Initializes accumulators for loss, classification accuracy, and rating accuracy for each action.\n",
    "  3. **Evaluation Loop**:\n",
    "     - Performs a forward pass through the classification model to get predictions and computes classification loss.\n",
    "     - For each sample in the batch, if the predicted class matches the actual class, it computes the rating accuracy for that action.\n",
    "     - The rating predictions are compared with the ground truth ratings, and accuracy is calculated for each action (deadlift, squat, lunges).\n",
    "  4. **Compute Average Metrics**: Computes the average classification loss, overall classification accuracy, and rating accuracy for each action.\n",
    "  5. **Return Results**: Returns the average loss, classification accuracy, and rating accuracies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "hqEEcG285U_E"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_combined_model(classification_model, criteria_model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluates both the classification and rating models on the provided dataset.\n",
    "\n",
    "    Args:\n",
    "        classification_model (torch.nn.Module): The classification model.\n",
    "        criteria_model (torch.nn.Module): The rating prediction model.\n",
    "        dataloader (DataLoader): A DataLoader providing the evaluation data.\n",
    "        criterion_class (nn.Module): The loss function for classification.\n",
    "        criterion_ratings (nn.Module): The loss function for ratings.\n",
    "        device (str): The device to perform evaluation on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        float: Average evaluation loss.\n",
    "        dict: Classification accuracy and rating accuracies for each action (deadlift, squat, lunges).\n",
    "    \"\"\"\n",
    "    classification_model.to(device)\n",
    "    criteria_model.to(device)\n",
    "    classification_model.eval()\n",
    "    criteria_model.eval()\n",
    "    criterion_class = nn.CrossEntropyLoss()  # Loss for classification task\n",
    "    criterion_ratings = nn.BCEWithLogitsLoss()  # Loss for rating task (binary)\n",
    "    total_loss = 0.0\n",
    "    correct_predictions_class = 0\n",
    "    total_samples_class = 0\n",
    "\n",
    "    # Initialize accumulators for rating accuracy per feature\n",
    "    rating_accumulators = {'deadlift': 0, 'squat': 0, 'lunges': 0}\n",
    "    total_rating_samples = {'deadlift': 0, 'squat': 0, 'lunges': 0}\n",
    "\n",
    "    # Initialize feature-level accuracy accumulators for different numbers of features per action\n",
    "    feature_accuracies = {\n",
    "        'deadlift': {'TP': [0] * 5, 'FP': [0] * 5, 'FN': [0] * 5, 'TN': [0] * 5},\n",
    "        'squat': {'TP': [0] * 6, 'FP': [0] * 6, 'FN': [0] * 6, 'TN': [0] * 6},\n",
    "        'lunges': {'TP': [0] * 7, 'FP': [0] * 7, 'FN': [0] * 7, 'TN': [0] * 7}\n",
    "    }\n",
    "    a = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            (pixel_values_frontal, bool_masked_pos_frontal, pose_landmarks_tensor_frontal,\n",
    "             pixel_values_lateral, bool_masked_pos_lateral, pose_landmarks_tensor_lateral,\n",
    "             label_class, ratings) = [tensor.to(device) for tensor in batch]\n",
    "\n",
    "            # Forward pass through the classification model\n",
    "            classification_output, combined_hidden = classification_model(\n",
    "                pixel_values_frontal, bool_masked_pos_frontal, pose_landmarks_tensor_frontal,\n",
    "                pixel_values_lateral, bool_masked_pos_lateral, pose_landmarks_tensor_lateral\n",
    "            )\n",
    "            _, predicted_class = torch.max(classification_output, 1)\n",
    "\n",
    "            # Classification loss and accuracy\n",
    "            loss_class = criterion_class(classification_output, label_class)\n",
    "            total_loss += loss_class.item()\n",
    "            correct_predictions_class += (predicted_class == label_class).sum().item()\n",
    "            total_samples_class += label_class.size(0)\n",
    "\n",
    "            a=a+1\n",
    "            print(a)\n",
    "            # Rating accuracy calculation per feature (only if predicted class matches actual class)\n",
    "            for i in range(label_class.size(0)):  # Loop over each sample in the batch\n",
    "                actual_class = label_class[i].item()\n",
    "                predicted_class_item = predicted_class[i].item()\n",
    "\n",
    "                if actual_class == predicted_class_item:\n",
    "                    if actual_class == 0:  # Deadlift\n",
    "                        ratings_output = criteria_model.dead_criteria_model(combined_hidden[i])\n",
    "                        predicted_ratings = torch.sigmoid(ratings_output) > 0.5\n",
    "                        correct_ratings = (predicted_ratings == ratings[i].byte()).sum().item()\n",
    "                        rating_accumulators['deadlift'] += correct_ratings\n",
    "                        total_rating_samples['deadlift'] += ratings[i].numel()\n",
    "\n",
    "                        # Feature-level analysis for deadlift\n",
    "                        for feature_idx in range(ratings[i].size(0)):  # Assume ratings[i] is the feature vector\n",
    "                            predicted = predicted_ratings[feature_idx].item()\n",
    "                            actual = ratings[i][feature_idx].item()\n",
    "\n",
    "                            # Update TP, FP, FN, TN for deadlift\n",
    "                            if predicted == 1 and actual == 1:\n",
    "                                feature_accuracies['deadlift']['TP'][feature_idx] += 1\n",
    "                            elif predicted == 1 and actual == 0:\n",
    "                                feature_accuracies['deadlift']['FP'][feature_idx] += 1\n",
    "                            elif predicted == 0 and actual == 1:\n",
    "                                feature_accuracies['deadlift']['FN'][feature_idx] += 1\n",
    "                            elif predicted == 0 and actual == 0:\n",
    "                                feature_accuracies['deadlift']['TN'][feature_idx] += 1\n",
    "\n",
    "                    elif actual_class == 1:  # Squat\n",
    "                        ratings_output = criteria_model.squat_criteria_model(combined_hidden[i])\n",
    "                        predicted_ratings = torch.sigmoid(ratings_output) > 0.5\n",
    "                        correct_ratings = (predicted_ratings == ratings[i].byte()).sum().item()\n",
    "                        rating_accumulators['squat'] += correct_ratings\n",
    "                        total_rating_samples['squat'] += ratings[i].numel()\n",
    "\n",
    "                        # Feature-level analysis for squat\n",
    "                        for feature_idx in range(ratings[i].size(0)):\n",
    "                            predicted = predicted_ratings[feature_idx].item()\n",
    "                            actual = ratings[i][feature_idx].item()\n",
    "\n",
    "                            # Update TP, FP, FN, TN for squat\n",
    "                            if predicted == 1 and actual == 1:\n",
    "                                feature_accuracies['squat']['TP'][feature_idx] += 1\n",
    "                            elif predicted == 1 and actual == 0:\n",
    "                                feature_accuracies['squat']['FP'][feature_idx] += 1\n",
    "                            elif predicted == 0 and actual == 1:\n",
    "                                feature_accuracies['squat']['FN'][feature_idx] += 1\n",
    "                            elif predicted == 0 and actual == 0:\n",
    "                                feature_accuracies['squat']['TN'][feature_idx] += 1\n",
    "\n",
    "                    elif actual_class == 2:  # Lunges\n",
    "                        ratings_output = criteria_model.lunges_criteria_model(combined_hidden[i])\n",
    "                        predicted_ratings = torch.sigmoid(ratings_output) > 0.5\n",
    "                        correct_ratings = (predicted_ratings == ratings[i].byte()).sum().item()\n",
    "                        rating_accumulators['lunges'] += correct_ratings\n",
    "                        total_rating_samples['lunges'] += ratings[i].numel()\n",
    "\n",
    "                        # Feature-level analysis for lunges\n",
    "                        for feature_idx in range(ratings[i].size(0)):\n",
    "                            predicted = predicted_ratings[feature_idx].item()\n",
    "                            actual = ratings[i][feature_idx].item()\n",
    "\n",
    "                            # Update TP, FP, FN, TN for lunges\n",
    "                            if predicted == 1 and actual == 1:\n",
    "                                feature_accuracies['lunges']['TP'][feature_idx] += 1\n",
    "                            elif predicted == 1 and actual == 0:\n",
    "                                feature_accuracies['lunges']['FP'][feature_idx] += 1\n",
    "                            elif predicted == 0 and actual == 1:\n",
    "                                feature_accuracies['lunges']['FN'][feature_idx] += 1\n",
    "                            elif predicted == 0 and actual == 0:\n",
    "                                feature_accuracies['lunges']['TN'][feature_idx] += 1\n",
    "\n",
    "    # Calculate the average loss and classification accuracy\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy_class = correct_predictions_class / total_samples_class if total_samples_class > 0 else 0.0\n",
    "\n",
    "    # Calculate rating accuracy for each action\n",
    "    rating_accuracy = {\n",
    "        'deadlift': rating_accumulators['deadlift'] / total_rating_samples['deadlift'] if total_rating_samples['deadlift'] > 0 else 0.0,\n",
    "        'squat': rating_accumulators['squat'] / total_rating_samples['squat'] if total_rating_samples['squat'] > 0 else 0.0,\n",
    "        'lunges': rating_accumulators['lunges'] / total_rating_samples['lunges'] if total_rating_samples['lunges'] > 0 else 0.0\n",
    "    }\n",
    "\n",
    "    # Print feature-level accuracy and performance metrics\n",
    "    for action in ['deadlift', 'squat', 'lunges']:\n",
    "        num_features = len(feature_accuracies[action]['TP'])  # Number of features for the action\n",
    "        for feature_idx in range(num_features):\n",
    "            TP = feature_accuracies[action]['TP'][feature_idx]\n",
    "            FP = feature_accuracies[action]['FP'][feature_idx]\n",
    "            FN = feature_accuracies[action]['FN'][feature_idx]\n",
    "            TN = feature_accuracies[action]['TN'][feature_idx]\n",
    "\n",
    "            precision = TP / (TP + FP) if TP + FP > 0 else 0.0\n",
    "            recall = TP / (TP + FN) if TP + FN > 0 else 0.0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "            accuracy = (TP + TN) / (TP + FP + FN + TN) if TP + FP + FN + TN > 0 else 0.0\n",
    "            print(f\"{action} feature {feature_idx + 1} - TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}\")\n",
    "            print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Write results to a file\n",
    "    with open('val_numbers.txt', 'a') as f:\n",
    "        f.write(f\"Classification Accuracy: {accuracy_class:.4f}\\n\")\n",
    "        f.write(f\"Rating Accuracy - Deadlift: {rating_accuracy['deadlift']:.4f}\\n\")\n",
    "        f.write(f\"Rating Accuracy - Squat: {rating_accuracy['squat']:.4f}\\n\")\n",
    "        f.write(f\"Rating Accuracy - Lunges: {rating_accuracy['lunges']:.4f}\\n\")\n",
    "        f.write(\"Feature-level Accuracies and Metrics:\\n\")\n",
    "        for action in ['deadlift', 'squat', 'lunges']:\n",
    "            num_features = len(feature_accuracies[action]['TP'])  # Number of features for the action\n",
    "            for feature_idx in range(num_features):\n",
    "                TP = feature_accuracies[action]['TP'][feature_idx]\n",
    "                FP = feature_accuracies[action]['FP'][feature_idx]\n",
    "                FN = feature_accuracies[action]['FN'][feature_idx]\n",
    "                TN = feature_accuracies[action]['TN'][feature_idx]\n",
    "\n",
    "                precision = TP / (TP + FP) if TP + FP > 0 else 0.0\n",
    "                recall = TP / (TP + FN) if TP + FN > 0 else 0.0\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "                accuracy = (TP + TN) / (TP + FP + FN + TN) if TP + FP + FN + TN > 0 else 0.0\n",
    "                f.write(f\"{action} feature {feature_idx + 1} - TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}\\n\")\n",
    "                f.write(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, Accuarcy: {accuracy:.4f}\\n\")\n",
    "\n",
    "    return avg_loss, accuracy_class, rating_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "\n",
    "dataloader = create_dataloader(train_df, 16, processor, batch_size=1)\n",
    "eval_dataloader = create_dataloader(val_df, 16, processor, batch_size=1)\n",
    "test_dataloader = create_dataloader(test_df, 16, processor, batch_size=1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "deadlift feature 1 - TP: 37, FP: 3, FN: 0, TN: 0\n",
      "Precision: 0.9250, Recall: 1.0000, F1-score: 0.9610, Accuracy: 0.9250\n",
      "deadlift feature 2 - TP: 38, FP: 2, FN: 0, TN: 0\n",
      "Precision: 0.9500, Recall: 1.0000, F1-score: 0.9744, Accuracy: 0.9500\n",
      "deadlift feature 3 - TP: 17, FP: 23, FN: 0, TN: 0\n",
      "Precision: 0.4250, Recall: 1.0000, F1-score: 0.5965, Accuracy: 0.4250\n",
      "deadlift feature 4 - TP: 0, FP: 0, FN: 18, TN: 22\n",
      "Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000, Accuracy: 0.5500\n",
      "deadlift feature 5 - TP: 35, FP: 5, FN: 0, TN: 0\n",
      "Precision: 0.8750, Recall: 1.0000, F1-score: 0.9333, Accuracy: 0.8750\n",
      "squat feature 1 - TP: 0, FP: 0, FN: 43, TN: 1\n",
      "Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000, Accuracy: 0.0227\n",
      "squat feature 2 - TP: 30, FP: 14, FN: 0, TN: 0\n",
      "Precision: 0.6818, Recall: 1.0000, F1-score: 0.8108, Accuracy: 0.6818\n",
      "squat feature 3 - TP: 44, FP: 0, FN: 0, TN: 0\n",
      "Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000, Accuracy: 1.0000\n",
      "squat feature 4 - TP: 15, FP: 29, FN: 0, TN: 0\n",
      "Precision: 0.3409, Recall: 1.0000, F1-score: 0.5085, Accuracy: 0.3409\n",
      "squat feature 5 - TP: 44, FP: 0, FN: 0, TN: 0\n",
      "Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000, Accuracy: 1.0000\n",
      "squat feature 6 - TP: 0, FP: 0, FN: 4, TN: 40\n",
      "Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000, Accuracy: 0.9091\n",
      "lunges feature 1 - TP: 9, FP: 6, FN: 0, TN: 0\n",
      "Precision: 0.6000, Recall: 1.0000, F1-score: 0.7500, Accuracy: 0.6000\n",
      "lunges feature 2 - TP: 11, FP: 4, FN: 0, TN: 0\n",
      "Precision: 0.7333, Recall: 1.0000, F1-score: 0.8462, Accuracy: 0.7333\n",
      "lunges feature 3 - TP: 13, FP: 2, FN: 0, TN: 0\n",
      "Precision: 0.8667, Recall: 1.0000, F1-score: 0.9286, Accuracy: 0.8667\n",
      "lunges feature 4 - TP: 8, FP: 7, FN: 0, TN: 0\n",
      "Precision: 0.5333, Recall: 1.0000, F1-score: 0.6957, Accuracy: 0.5333\n",
      "lunges feature 5 - TP: 15, FP: 0, FN: 0, TN: 0\n",
      "Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000, Accuracy: 1.0000\n",
      "lunges feature 6 - TP: 0, FP: 15, FN: 0, TN: 0\n",
      "Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000, Accuracy: 0.0000\n",
      "lunges feature 7 - TP: 9, FP: 6, FN: 0, TN: 0\n",
      "Precision: 0.6000, Recall: 1.0000, F1-score: 0.7500, Accuracy: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.001911617439202588,\n",
       " 1.0,\n",
       " {'deadlift': 0.745,\n",
       "  'squat': 0.6590909090909091,\n",
       "  'lunges': 0.6190476190476191})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Initialize the model\n",
    "# dual_combined_model = Dual_Combined_Model()\n",
    "# rating_model = DeepClassificationWithRatingModel()\n",
    "\n",
    "# print(device)\n",
    "\n",
    "# # Train the model\n",
    "# train_combined_model(dual_combined_model, rating_model, dataloader, eval_dataloader,\n",
    "#                      epochs=1000, lr=1e-4, device=device)\n",
    "\n",
    "\n",
    "# Define the path to the saved model weights\n",
    "model_path = \"best_combined_model_epoch_1.pt\"\n",
    "criteria_model_path = \"best_rating_model_epoch_1.pt\"\n",
    "\n",
    "dual_combined_model = Dual_Combined_Model()\n",
    "rating_model = DeepClassificationWithRatingModel()\n",
    "\n",
    "# Load the saved model weights\n",
    "dual_combined_model.load_state_dict(torch.load(model_path))\n",
    "rating_model.load_state_dict(torch.load(criteria_model_path))\n",
    "\n",
    "print(device)\n",
    "\n",
    "# Continue training the model\n",
    "\n",
    "train_combined_model(dual_combined_model, rating_model, dataloader, eval_dataloader,\n",
    "                      epochs=1000, lr=1e-4, device=device)\n",
    "\n",
    "# Evaluate the model on dataset\n",
    "evaluate_combined_model(dual_combined_model, rating_model, eval_dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ahmed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
