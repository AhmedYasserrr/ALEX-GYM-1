{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11263209,"sourceType":"datasetVersion","datasetId":7039907}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\n\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoImageProcessor, AutoModelForPreTraining\n\n\n# Check if CUDA (GPU support) is available\nif torch.cuda.is_available():\n    print(\"CUDA is available. PyTorch can use the GPU.\")\n    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"CUDA is not available. PyTorch is using the CPU.\")\n\n\nbase_path = ''\nFeatureNum = 6\n\n\ndef process_action_data(base_path, action_name):\n    \"\"\"\n    Process data for a specific action by loading the corresponding Excel and JSON files,\n    adding pose data, and splitting into train, validation, and test sets.\n\n    Args:\n        base_path (str): The base directory containing the files.\n        action_name (str): The name of the action (e.g., 'squat', 'deadlift', 'lunges').\n\n    Returns:\n        tuple: train_df, val_df, test_df DataFrames.\n    \"\"\"\n    # Load the Excel file\n    excel_file = f\"{action_name}.xlsx\"\n    df = pd.read_excel(os.path.join(base_path, excel_file))\n\n    # Load the JSON files for front and lateral poses\n    front_pose_file = f\"front_pose_{action_name}.json\"\n    lat_pose_file = f\"lat_pose_{action_name}.json\"\n\n    def load_json_as_numpy(json_file):\n        with open(json_file, 'r') as file:\n            data = json.load(file)\n        return np.array(data)\n\n    front_pose_array = load_json_as_numpy(os.path.join(base_path, front_pose_file))\n    lat_pose_array = load_json_as_numpy(os.path.join(base_path, lat_pose_file))\n\n    # Ensure `front_pose` and `lat_pose` columns exist\n    if 'front_pose' not in df.columns:\n        df['front_pose'] = None\n    if 'lat_pose' not in df.columns:\n        df['lat_pose'] = None\n\n    # Assign the loaded arrays to the DataFrame if lengths match\n    if len(front_pose_array) == len(df) and len(lat_pose_array) == len(df):\n        df['front_pose'] = list(front_pose_array)\n        df['lat_pose'] = list(lat_pose_array)\n    else:\n        raise ValueError(\"The length of the loaded arrays does not match the DataFrame.\")\n\n    # Shuffle the DataFrame\n    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n    # Define split ratios\n    train_ratio, val_ratio, test_ratio = 0.7, 0.15, 0.15\n\n    # Calculate the number of samples for each set\n    total_samples = len(df)\n    train_size = int(total_samples * train_ratio)\n    val_size = int(total_samples * val_ratio)\n\n    # Split the DataFrame\n    train_df = df.iloc[:train_size]\n    val_df = df.iloc[train_size:train_size + val_size]\n    test_df = df.iloc[train_size + val_size:]\n\n    return train_df, val_df, test_df\n\n\n\ntrain_df_squat, val_df_squat, test_df_squat = process_action_data(base_path, 'squat')\ntrain_df_dead, val_df_dead, test_df_dead = process_action_data(base_path, 'deadlift')\ntrain_df_lunge, val_df_lunge, test_df_lunge = process_action_data(base_path, 'lunges')\n\n\n# Define the actions\nactions = ['squat', 'deadlift', 'lunges']\n\n# Process data for each action and store results in dictionaries\nsplits = {action: process_action_data(base_path, action) for action in actions}\n\n# Concatenate and shuffle DataFrames for each split\ntrain_df = pd.concat([splits[action][0] for action in actions]).sample(frac=1, random_state=1).reset_index(drop=True)\nval_df = pd.concat([splits[action][1] for action in actions]).sample(frac=1, random_state=1).reset_index(drop=True)\ntest_df = pd.concat([splits[action][2] for action in actions]).sample(frac=1, random_state=1).reset_index(drop=True)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:16:10.983469Z","iopub.execute_input":"2025-09-04T17:16:10.983876Z","iopub.status.idle":"2025-09-04T17:16:36.625484Z","shell.execute_reply.started":"2025-09-04T17:16:10.983835Z","shell.execute_reply":"2025-09-04T17:16:36.622541Z"}},"outputs":[{"name":"stdout","text":"CUDA is not available. PyTorch is using the CPU.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e6569042c2f1>\u001b[0m in \u001b[0;36m<cell line: 90>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mtrain_df_squat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df_squat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df_squat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_action_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'squat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0mtrain_df_dead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df_dead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df_dead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_action_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deadlift'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mtrain_df_lunge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df_lunge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df_lunge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_action_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lunges'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-e6569042c2f1>\u001b[0m in \u001b[0;36mprocess_action_data\u001b[0;34m(base_path, action_name)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Load the Excel file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mexcel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{action_name}.xlsx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexcel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Load the JSON files for front and lateral poses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'squat.xlsx'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'squat.xlsx'","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"# Vision","metadata":{}},{"cell_type":"code","source":"\nimport torch.nn as nn\nimport torch\nimport torchvision.models.video as models\n\n\nclass PretrainedResNet3D(nn.Module):\n    \"\"\"\n    A wrapper around the 3D ResNet model (ResNet3D) to leverage pretrained weights and provide feature extraction.\n    The final fully connected layer is replaced with an identity layer, making it suitable for downstream tasks \n    like feature extraction for multi-input models.\n\n    Args:\n        pretrained (bool, optional): Whether to load the pretrained weights for ResNet3D (default is True).\n    \n    Attributes:\n        resnet3d (nn.Module): The ResNet3D model with the final layer replaced with an identity layer.\n    \n    Methods:\n        forward(x): Performs a forward pass through the ResNet3D model, returning the extracted features.\n\n    \"\"\"\n    def __init__(self, pretrained=True):\n        super(PretrainedResNet3D, self).__init__()\n        try:\n            # Load the pretrained ResNet3D model\n            self.resnet3d = models.r3d_18(pretrained=pretrained)\n            # Replace the final fully connected layer with an identity layer\n            self.resnet3d.fc = nn.Identity()\n\n            for name, param in self.resnet3d.named_parameters():\n                if \"layer3\" not in name and \"layer4\" not in name:\n                    param.requires_grad = False\n        except Exception as e:\n            print(f\"Error in PretrainedResNet3D initialization: {e}\")\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the ResNet3D model. This method processes the input tensor through the ResNet3D \n        architecture, extracting features by passing through the layers.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape [batch_size, channels, num_frames, height, width].\n\n        Returns:\n            torch.Tensor: Extracted feature vector after passing through the ResNet3D model.\n        \"\"\"\n        try:\n            return self.resnet3d(x)\n        except Exception as e:\n            print(f\"Error in PretrainedResNet3D forward pass: {e}\")\n            return None\n\n\nimport torch.nn.functional as F\n\nclass ResidualBlock(nn.Module):\n    \"\"\"\n    An optimized residual block with balanced main and shortcut paths for improved gradient flow.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResidualBlock, self).__init__()\n        try:\n            # Main path with bottleneck architecture for efficiency\n            self.main_path = nn.Sequential(\n                # Reduce channels first (bottleneck)\n                nn.Conv2d(in_channels, out_channels//2, kernel_size=(1, 1), stride=1),\n                nn.BatchNorm2d(out_channels//2),\n                nn.ReLU(),\n                # Extract features with larger kernel\n                nn.Conv2d(out_channels//2, out_channels//2, kernel_size=(3, 1), stride=(stride, 1), padding=(1, 0)),\n                nn.BatchNorm2d(out_channels//2),\n                nn.ReLU(),\n                # Expand channels back\n                nn.Conv2d(out_channels//2, out_channels, kernel_size=(1, 1), stride=1),\n                nn.BatchNorm2d(out_channels),\n                nn.Dropout(0.2)  # Lower dropout for better feature retention\n            )\n\n            # Adaptive shortcut connection\n            if stride != 1 or in_channels != out_channels:\n                self.shortcut = nn.Sequential(\n                    nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1), stride=(stride, 1)),\n                    nn.BatchNorm2d(out_channels)\n                )\n            else:\n                self.shortcut = nn.Identity()\n                \n        except Exception as e:\n            print(f\"Error in ResidualBlock initialization: {e}\")\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the enhanced ResidualBlock with pre-activation design.\n        \"\"\"\n        try:\n            # Process paths\n            residual = self.shortcut(x)\n            main_output = self.main_path(x)\n            \n            # Combine and activate\n            x = main_output + residual\n            return F.relu(x)  # Apply ReLU after addition\n        except Exception as e:\n            print(f\"Error in ResidualBlock forward pass: {e}\")\n            return None\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:16:36.626219Z","iopub.status.idle":"2025-09-04T17:16:36.626575Z","shell.execute_reply":"2025-09-04T17:16:36.626435Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pose","metadata":{}},{"cell_type":"code","source":"\nclass Pose_Model(nn.Module):\n    \"\"\"\n    A CNN model for extracting features from pose data with enhanced architecture.\n    \"\"\"\n    def __init__(self, input_channels=5984, hidden_dim=256, expansion_factor=2, feature_dim=512):\n        super(Pose_Model, self).__init__()\n        try:\n            mid_channels = hidden_dim * expansion_factor\n            \n            # Initial dimensionality reduction\n            self.initial_conv = nn.Sequential(\n                nn.Conv2d(input_channels, hidden_dim, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0)),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(0.2)\n            )\n\n            # Enhanced multi-stage residual pathway\n            self.res_stage1 = ResidualBlock(hidden_dim, mid_channels, stride=2)\n            self.res_stage2 = ResidualBlock(mid_channels, feature_dim, stride=1)\n            \n            # Spatial pooling to further compact the representation\n            self.pool = nn.AdaptiveAvgPool2d((1, 1))\n\n        except Exception as e:\n            print(f\"Error in Pose_Model initialization: {e}\")\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass with enhanced feature extraction.\n        \n        Args:\n            x (torch.Tensor): Input tensor of shape [batch_size, num_frames, channels]\n        \n        Returns:\n            torch.Tensor: Features of shape [batch_size, feature_dim]\n        \"\"\"\n        try:\n            batch_size, num_frames, channels = x.shape\n            x = x.permute(0, 2, 1).unsqueeze(-1)  # [batch_size, channels, num_frames, 1]\n\n            # Multi-stage feature extraction\n            x = self.initial_conv(x)\n            x = self.res_stage1(x)\n            x = self.res_stage2(x)\n            \n            # Global pooling and flatten\n            x = self.pool(x)\n            x = x.view(batch_size, -1)  # [batch_size, feature_dim]\n            \n            return x\n            \n        except Exception as e:\n            print(f\"Error in Pose_Model forward pass: {e}\")\n            return None\n\n\nclass DualInputPose(nn.Module):\n    \"\"\"\n    A model that processes pose data from two views with enhanced feature extraction.\n    \"\"\"\n    def __init__(self, MergedOrAlone=1, output_size=FeatureNum, pose_input_channels=5984, feature_dim=512):\n        super(DualInputPose, self).__init__()\n        try:\n            # Pose models for front and lateral views with consistent output dimension\n            self.front_model = Pose_Model(input_channels=pose_input_channels, feature_dim=feature_dim)\n            self.lat_model = Pose_Model(input_channels=pose_input_channels, feature_dim=feature_dim)\n            \n            combined_dim = feature_dim * 2\n            \n            # Path selection based on standalone vs. merged operation\n            if MergedOrAlone == 1:\n                # Progressive reduction for standalone predictions\n                self.fc = nn.Sequential(\n                    nn.Linear(combined_dim, combined_dim//2),  # 1024 → 512\n                    nn.BatchNorm1d(combined_dim//2),\n                    nn.ReLU(),\n                    nn.Dropout(0.3),\n                    nn.Linear(combined_dim//2, combined_dim//4),  # 512 → 256\n                    nn.ReLU(),\n                    nn.Dropout(0.2),\n                    nn.Linear(combined_dim//4, output_size)  # 256 → output\n                )\n            else:\n                # Maintain dimension for fusion - output same size as individual streams for balanced fusion\n                self.fc = nn.Sequential(\n                    nn.Linear(combined_dim, feature_dim),  # 1024 → 512\n                    nn.BatchNorm1d(feature_dim),\n                    nn.ReLU(),\n                    nn.Dropout(0.2)\n                )\n        except Exception as e:\n            print(f\"Error in DualInputPose initialization: {e}\")\n\n    def forward(self, front_input, lat_input):\n        \"\"\"\n        Forward pass of the DualInputPose model. The model processes the front and lateral views of pose data \n        through separate Pose_Model instances, concatenates the resulting features, and makes predictions.\n\n        Args:\n            front_input (torch.Tensor): Input tensor of shape [batch_size, num_frames, channels] for frontal pose data.\n            lat_input (torch.Tensor): Input tensor of shape [batch_size, num_frames, channels] for lateral pose data.\n        \n        Returns:\n            torch.Tensor: The predicted output of shape [batch_size, output_size].\n        \"\"\"\n        try:\n            # Pass through the front and lateral Pose models\n            front_features = self.front_model(front_input)  # Shape: [batch_size, feature_dim]\n            print(f\"Front pose features shape: {front_features.shape}\")  # Debugging\n\n            lat_features = self.lat_model(lat_input)  # Shape: [batch_size, feature_dim]\n            print(f\"Lateral pose features shape: {lat_features.shape}\")  # Debugging\n\n            # Concatenate features\n            combined_features = torch.cat((front_features, lat_features), dim=1)  # Shape: [batch_size, feature_dim * 2]\n            print(f\"Combined pose features shape: {combined_features.shape}\")  # Debugging\n\n            # Predict criteria\n            output = self.fc(combined_features)  # Shape: [batch_size, output_size]\n            print(f\"Final pose features shape: {output.shape}\")  # Debugging\n\n            return output\n        except Exception as e:\n            print(f\"Error in DualInputPose forward pass: {e}\")\n            return None\n\n\nclass DualInputResNet3D(nn.Module):\n    \"\"\"\n    A model that uses two separate streams of the 3D ResNet architecture with enhanced fusion.\n    \"\"\"\n    def __init__(self, MergedOrAlone, output_size=FeatureNum, hidden_size=512, dropout_rate=0.3):\n        super(DualInputResNet3D, self).__init__()\n        try:\n            # Pretrained ResNet3D streams remain unchanged\n            self.resnet3d_frontal = PretrainedResNet3D()\n            self.resnet3d_lateral = PretrainedResNet3D()\n            \n            combined_dim = hidden_size * 2\n            \n            # Path selection logic\n            if MergedOrAlone == 1:\n                # Standalone prediction path with advanced normalization\n                self.fc_layers = nn.Sequential(\n                    nn.Linear(combined_dim, combined_dim//2),  # 1024 → 512\n                    nn.BatchNorm1d(combined_dim//2),\n                    nn.ReLU(),\n                    nn.Dropout(dropout_rate),\n                    nn.Linear(combined_dim//2, combined_dim//4),  # 512 → 256\n                    nn.ReLU(),\n                    nn.Dropout(dropout_rate * 0.7),  # Reduced dropout\n                    nn.Linear(combined_dim//4, output_size)  # Final prediction\n                )\n            else:\n                # Maintain dimension for fusion - consistent with pose stream\n                self.fc_layers = nn.Sequential(\n                    nn.Linear(combined_dim, hidden_size),  # 1024 → 512\n                    nn.BatchNorm1d(hidden_size),\n                    nn.ReLU(),\n                    nn.Dropout(dropout_rate * 0.7)\n                )\n        except Exception as e:\n            print(f\"Error in DualInputResNet3D initialization: {e}\")\n\n    def forward(self, frontal, lateral):\n        \"\"\"\n        Forward pass through the dual input ResNet3D model. The inputs (frontal and lateral views) are processed\n        through separate ResNet3D models, and the extracted features are concatenated and passed through fully\n        connected layers to produce the final output.\n\n        Args:\n            frontal (torch.Tensor): Input tensor for the frontal view, shape [batch_size, channels, num_frames, height, width].\n            lateral (torch.Tensor): Input tensor for the lateral view, shape [batch_size, channels, num_frames, height, width].\n\n        Returns:\n            torch.Tensor: Final output tensor, shape [batch_size, output_size], where output_size is the number of classes or features.\n        \"\"\"\n        try:\n            # Permute to [batch_size, channels, num_frames, height, width]\n            frontal = frontal.permute(0, 2, 1, 3, 4).contiguous()\n            lateral = lateral.permute(0, 2, 1, 3, 4).contiguous()\n\n            # Process frontal input through ResNet3D\n            x_f = self.resnet3d_frontal(frontal)\n            x_f = x_f.view(x_f.size(0), -1)  # Flatten features\n            print(f\"Frontal features shape: {x_f.shape}\")  # Debugging\n\n            # Process lateral input through ResNet3D\n            x_l = self.resnet3d_lateral(lateral)\n            x_l = x_l.view(x_l.size(0), -1)  # Flatten features\n            print(f\"Lateral features shape: {x_l.shape}\")  # Debugging\n\n            # Concatenate features from frontal and lateral streams\n            x = torch.cat((x_f, x_l), dim=1)\n            print(f\"Concatenated features shape: {x.shape}\")  # Debugging\n\n            # Pass through fully connected layers\n            x = self.fc_layers(x)\n            print(f\"Final output shape: {x.shape}\")  # Debugging\n\n            return x\n        except Exception as e:\n            print(f\"Error in DualInputResNet3D forward pass: {e}\")\n            return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:16:36.630298Z","iopub.status.idle":"2025-09-04T17:16:36.630833Z","shell.execute_reply":"2025-09-04T17:16:36.630625Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Multi","metadata":{}},{"cell_type":"code","source":"\nclass MultiModalModel(nn.Module):\n    \"\"\"\n    Multi-modal model combining vision and pose streams with enhanced fusion mechanisms.\n    \"\"\"\n    def __init__(self, output_size=FeatureNum):\n        super(MultiModalModel, self).__init__()\n        try:\n            # Both feature extractors now output 512-dimensional vectors\n            self.vision_model = DualInputResNet3D(0, output_size=0)\n            self.pose_model = DualInputPose(0, output_size=0)\n            \n            # Balanced fusion network with consistent dimensions\n            self.fusion_layers = nn.Sequential(\n                nn.Linear(512 + 512, 768),  # Combined 1024D → 768D\n                nn.BatchNorm1d(768),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(768, 384),  # 768D → 384D\n                nn.BatchNorm1d(384),\n                nn.ReLU(),\n                nn.Dropout(0.25),\n                nn.Linear(384, 192),  # 384D → 192D\n                nn.BatchNorm1d(192),\n                nn.ReLU(),\n                nn.Dropout(0.2)\n            )\n            \n            # Classification head with progressive reduction\n            self.classifier = nn.Sequential(\n                nn.Linear(192, 96),\n                nn.ReLU(),\n                nn.Dropout(0.2),\n                nn.Linear(96, output_size)\n            )\n            \n        except Exception as e:\n            print(f\"Error in MultiModalModel initialization: {e}\")\n\n    def forward(self, image_frontal, image_lateral, pose_frontal, pose_lateral):\n        \"\"\"\n        Forward pass of the MultiModalModel combining image and pose features.\n\n        Args:\n            image_frontal (Tensor): The frontal view video data.\n            image_lateral (Tensor): The lateral view video data.\n            pose_frontal (Tensor): The frontal pose data.\n            pose_lateral (Tensor): The lateral pose data.\n        \n        Returns:\n            Tensor: Final predicted output after processing both vision and pose data.\n        \"\"\"\n        try:\n            image_features = self.vision_model(image_frontal, image_lateral)\n            pose_features = self.pose_model(pose_frontal, pose_lateral)\n\n            combined_features = torch.cat((image_features, pose_features), dim=1)\n            fused_features = self.fusion_layers(combined_features)\n\n            output = self.classifier(fused_features)\n            return output\n        except Exception as e:\n            print(f\"Error in MultiModalModel forward pass: {e}\")\n            return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:16:36.631959Z","iopub.status.idle":"2025-09-04T17:16:36.632481Z","shell.execute_reply":"2025-09-04T17:16:36.632237Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataloadr and utils","metadata":{}},{"cell_type":"code","source":"\ndef compute_pairwise_distances(points_tensor):\n    \"\"\"\n    Compute pairwise distances between 33 pose landmarks for each frame.\n\n    Args:\n        points_tensor (torch.Tensor): Tensor of shape [num_frames, 33, 3],\n                                       where each row is a point (x, y, z) for each frame.\n\n    Returns:\n        torch.Tensor: Tensor of shape [num_frames, 528], containing pairwise distances for each frame.\n    \"\"\"\n    num_frames, num_points, _ = points_tensor.size()\n\n    # Generate index pairs for the upper triangle of a matrix (excluding diagonal)\n    pairs = torch.combinations(torch.arange(num_points), r=2, with_replacement=False)  # Shape: [528, 2]\n\n    # Gather the coordinates for each pair of points\n    point1 = points_tensor[:, pairs[:, 0]]  # Shape: [num_frames, 528, 3]\n    point2 = points_tensor[:, pairs[:, 1]]  # Shape: [num_frames, 528, 3]\n\n    # Compute pairwise Euclidean distances\n    pairwise_distances = torch.norm(point1 - point2, dim=2)  # Shape: [num_frames, 528]\n\n    return pairwise_distances\n\n\ndef compute_distances_and_angles_combined(points_tensor):\n    \"\"\"\n    Compute and concatenate pairwise distances and angles between every three points for 33 pose landmarks for each frame.\n\n    Args:\n        points_tensor (torch.Tensor): Tensor of shape [num_frames, 33, 3],\n                                       where each row is a point (x, y, z) for each frame.\n\n    Returns:\n        torch.Tensor: Tensor of shape [num_frames, 528 + comb(33, 3)],\n                      containing pairwise distances and angles for each frame.\n    \"\"\"\n    num_frames, num_points, _ = points_tensor.size()\n\n    # Precompute all pairs and triplets of indices\n    pairs = torch.combinations(torch.arange(num_points), r=2, with_replacement=False)\n    triplets = torch.combinations(torch.arange(num_points), r=3, with_replacement=False)\n\n    # Compute pairwise distances\n    point_diffs = points_tensor[:, pairs[:, 0]] - points_tensor[:, pairs[:, 1]]  # [num_frames, num_pairs, 3]\n    pairwise_distances = torch.norm(point_diffs, dim=2)  # [num_frames, num_pairs]\n\n    # Compute angles between triplets\n    vec1 = points_tensor[:, triplets[:, 0]] - points_tensor[:, triplets[:, 1]]  # [num_frames, num_triplets, 3]\n    vec2 = points_tensor[:, triplets[:, 2]] - points_tensor[:, triplets[:, 1]]  # [num_frames, num_triplets, 3]\n    dot_products = torch.sum(vec1 * vec2, dim=2)  # [num_frames, num_triplets]\n    norms = torch.norm(vec1, dim=2) * torch.norm(vec2, dim=2)  # [num_frames, num_triplets]\n    cos_angles = dot_products / (norms + 1e-8)  # Add epsilon to avoid division by zero\n    # angles = torch.acos(cos_angles)  # [num_frames, num_triplets]\n\n    # Concatenate distances and angles\n    combined_features = torch.cat([pairwise_distances, cos_angles], dim=1)  # [num_frames, 5984]\n    return combined_features\n\n\n\n\nclass PreprocessedPoseVideoDataset(Dataset):\n\n    def __init__(self, df, num_frames, preprocessed_dir):\n        self.df = df\n        self.num_frames = num_frames\n        self.preprocessed_dir = preprocessed_dir\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n\n        # Get frontal and lateral video identifiers\n        num_video_frontal = row['Num Video Frontal']\n        num_video_lateral = row['Num Video Lateral']\n        num_idx = row['NumIdx']\n        action = row['Action']\n\n        # Load preprocessed frames\n        frontal_frames = self._load_preprocessed_frames(num_video_frontal, action, num_idx)\n        lateral_frames = self._load_preprocessed_frames(num_video_lateral, action, num_idx)\n\n        # Normalize images\n        image_frontal = torch.tensor(frontal_frames, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n        image_lateral = torch.tensor(lateral_frames, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n\n        label_class = torch.tensor(row['class'], dtype=torch.long)\n        ratings = self._process_ratings(train_df_squat, row)\n        ratings = torch.tensor(ratings, dtype=torch.float32) if ratings is not None else None\n\n        # Extract pose landmarks from video frames\n        pose_landmarks_frontal = row['front_pose']\n        pose_landmarks_lateral = row['lat_pose']\n        pose_landmarks_tensor_frontal = torch.tensor(pose_landmarks_frontal).float()\n        pose_landmarks_tensor_lateral = torch.tensor(pose_landmarks_lateral).float()\n        pose_frontal = compute_distances_and_angles_combined(pose_landmarks_tensor_frontal)\n        pose_lateral = compute_distances_and_angles_combined(pose_landmarks_tensor_lateral)\n\n        return (image_frontal, image_lateral, pose_frontal, pose_lateral, label_class, ratings)\n\n    def _load_preprocessed_frames(self, num_video, action, num_idx):\n        frames = []\n        for i in range(1, self.num_frames + 1):\n            # Construct the preprocessed file path\n            file_name = f\"{num_video}_idx_{num_idx}_{i}.npy\"\n            file_path = os.path.join(self.preprocessed_dir, action, file_name)\n\n            # Load the preprocessed .npy file\n            frame = np.load(file_path)\n            frames.append(frame)\n        return np.stack(frames, axis=0)\n\n    def _process_ratings(self, df, row):\n        relevant_columns = [col for col in df.columns if col.endswith('F') or col.endswith('L')]\n        scores = row[relevant_columns].values\n        thresholded_scores = np.where(scores >= 0.5, 1, 0)\n        return thresholded_scores.tolist()\n\n\n\ndef create_dataloader(df, num_frames, batch_size=16, shuffle=True):\n    dataset = PreprocessedPoseVideoDataset(df, num_frames, preprocessed_dir=os.path.join('', 'preprocessed_images'))\n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        pin_memory=True           # Optimize for GPU training\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:16:36.633835Z","iopub.status.idle":"2025-09-04T17:16:36.634283Z","shell.execute_reply":"2025-09-04T17:16:36.634087Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train and eval","metadata":{}},{"cell_type":"code","source":"\ndef train_combined_model(CustomModel, train_dataloader, eval_dataloader, epochs=1000, lr=1e-4,\n                         device='cpu', clip_grad_norm=1.0, patience=10):\n    CustomModel.to(device)\n\n    # Set up optimizer and loss functions\n    optimizer = optim.Adam(list(CustomModel.parameters()), lr=lr)\n    feature_loss = nn.BCEWithLogitsLoss()  # Loss for binary feature classification\n\n    # Learning rate scheduler\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n    best_eval_loss = float('inf')\n    best_hamming_distance = float('inf')\n    patience_counter = 0\n    print(\"Training the model...\")\n    \n    for epoch in range(epochs):\n        CustomModel.train()\n        running_loss = 0.0\n        print(f'Start of Epoch {epoch+1}')\n        for batch_idx, batch in enumerate(train_dataloader):\n            # Calculate progress percentage\n            progress = (batch_idx + 1) / len(train_dataloader) * 100\n            # Unpack batch data and move to the specified device\n            (image_frontal, image_lateral, pose_frontal, pose_lateral, label_class, ratings) = [tensor.to(device) for tensor in batch]\n            optimizer.zero_grad()\n            \n            # Forward pass - Check dimensions:\n            # image_frontal: [batch_size, 16, 3, 224, 224]\n            # image_lateral: [batch_size, 16, 3, 224, 224]\n            # pose_frontal: [batch_size, 16, 5984]\n            # pose_lateral: [batch_size, 16, 5984]\n            # ratings: [batch_size, FeatureNum]\n            ratings_output = CustomModel(image_frontal, image_lateral).to(device)\n            # ratings_output should be: [batch_size, FeatureNum]\n\n            loss = feature_loss(ratings_output, ratings)\n\n            # Backward pass and optimization\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(list(CustomModel.parameters()), clip_grad_norm)\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            # Print progress during each epoch\n            print(f\"Epoch [{epoch + 1}/{epochs}], Progress: {progress:.2f}%, Batch [{batch_idx + 1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n\n        avg_loss = running_loss / len(train_dataloader)\n\n        # Evaluate model after each epoch\n        eval_loss, hamming_distances, metrics = evaluate_combined_model(CustomModel, eval_dataloader, feature_loss, device)\n        mean_hamming_distance = sum(hamming_distances.values()) / len(hamming_distances) if len(hamming_distances) > 0 else 0.0\n\n        tp = metrics['TP']\n        tn = metrics['TN']\n        fp = metrics['FP']\n        fn = metrics['FN']\n\n        # Class 1 metrics\n        precision_1 = tp / (tp + fp) if (tp + fp) > 0 else 0\n        recall_1 = tp / (tp + fn) if (tp + fn) > 0 else 0\n        f1_score_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0\n\n        # Class 0 metrics\n        precision_0 = tn / (tn + fn) if (tn + fn) > 0 else 0\n        recall_0 = tn / (tn + fp) if (tn + fp) > 0 else 0\n        f1_score_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0) if (precision_0 + recall_0) > 0 else 0\n\n        jaccard_index = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0.0\n\n        # Print summary for each epoch\n        print(f\"Epoch [{epoch + 1}/{epochs}] with lr: {lr} Summary: \"\n              f\"Train Loss: {avg_loss:.4f}, Eval Loss: {eval_loss:.4f},\"\n              f\"Precision: {(precision_1 +precision_0)/2:.4f}, Recall: {(recall_1 +recall_0)/2:.4f},\"\n              f\" F1-Score 0: {f1_score_0:.4f}, F1-Score 1: {f1_score_1:.4f}, F1-Score: {(f1_score_1 +f1_score_0)/2:.4f}\"\n              )\n        print(f\"Hamming Loss: {mean_hamming_distance:.4f}, jaccard index: {jaccard_index:.4f}\", )\n\n        scheduler.step(eval_loss)\n\n        # Early stopping and model saving\n        if (eval_loss < best_eval_loss):\n            best_eval_loss = eval_loss\n            patience_counter = 0\n            torch.save(CustomModel.state_dict(), f\"best_rating_model_epoch_{epoch + 1}_eval_pose.pt\")\n            print(\"Model checkpoint saved.\")\n        elif (mean_hamming_distance < best_hamming_distance):\n            best_hamming_distance = mean_hamming_distance\n            patience_counter = 0\n            torch.save(CustomModel.state_dict(), f\"best_rating_model_epoch_{epoch + 1}_ham_pose.pt\")\n            print(\"Model checkpoint saved.\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    print(\"Training complete.\")\n\n\n\ndef evaluate_combined_model(CustomSwinTransformerModel, dataloader, feature_loss, device):\n    \"\"\"\n    Evaluates the model and computes evaluation metrics for each feature.\n\n    Args:\n        CustomSwinTransformerModel (torch.nn.Module): The rating prediction model.\n        dataloader (DataLoader): A DataLoader providing the evaluation data.\n        feature_loss (nn.Module): The loss function for ratings.\n        device (str): The device to perform evaluation on ('cpu' or 'cuda').\n\n    Returns:\n        float: Average evaluation loss.\n        dict: Hamming distance for squat features.\n        dict: TP, TN, FP, FN for each feature.\n    \"\"\"\n    CustomSwinTransformerModel.eval()\n    total_loss = 0.0\n    num_features = FeatureNum\n    metrics = {\n            feature_idx: {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0} for feature_idx in range(num_features)\n        }\n    total_samples = {feature_idx: 0 for feature_idx in range(num_features)}\n\n    with torch.no_grad():\n        a=0\n        for batch_idx, batch in enumerate(dataloader):\n            # Move batch data to the device\n            print(f\"a ={a},  batch_idx = {batch_idx}\")\n            a=a+1\n            image_frontal, image_lateral, pose_frontal, pose_lateral, label_class, ratings = [tensor.to(device) for tensor in batch]\n\n            # Predict ratings using the model\n            ratings_output = CustomSwinTransformerModel(image_frontal, image_lateral).to(device)\n            total_loss += feature_loss(ratings_output, ratings)\n\n            # Sigmoid activation for binary predictions\n            predicted_ratings = torch.sigmoid(ratings_output) > 0.5\n            actual_ratings = ratings.byte()\n\n            # Compute TP, TN, FP, FN for each feature\n            for feature_idx in range(num_features):\n                preds = predicted_ratings[:, feature_idx]\n                trues = actual_ratings[:, feature_idx]\n\n                metrics[feature_idx]['TP'] += (preds & trues).sum().item()\n                metrics[feature_idx]['TN'] += (~preds & ~trues).sum().item()\n                metrics[feature_idx]['FP'] += (preds & ~trues).sum().item()\n                metrics[feature_idx]['FN'] += (~preds & trues).sum().item()\n                total_samples[feature_idx] += trues.numel()\n\n    # Calculate Hamming distance for each feature\n    hamming_distances = {}\n    for feature_idx in range(num_features):\n          hamming_distances[feature_idx] = (\n              metrics[feature_idx]['FP'] + metrics[feature_idx]['FN']\n          ) / total_samples[feature_idx] if total_samples[feature_idx] > 0 else 0.0\n\n    avg_loss = total_loss / len(dataloader)\n\n    total_metrics = {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0}\n    # Print TP, TN, FP, FN for each feature\n    for feature_idx in range(num_features):\n        print(f\"Feature {feature_idx}:\")\n        print(f\"  TP: {metrics[feature_idx]['TP']}\")\n        print(f\"  TN: {metrics[feature_idx]['TN']}\")\n        print(f\"  FP: {metrics[feature_idx]['FP']}\")\n        print(f\"  FN: {metrics[feature_idx]['FN']}\")\n        print(f\"  Hamming Loss: {hamming_distances[feature_idx]:.4f}\")\n        total_metrics['TP'] += metrics[feature_idx]['TP']\n        total_metrics['TN'] += metrics[feature_idx]['TN']\n        total_metrics['FP'] += metrics[feature_idx]['FP']\n        total_metrics['FN'] += metrics[feature_idx]['FN']\n    mean_hamming_distance = sum(hamming_distances.values()) / len(hamming_distances) if len(hamming_distances) > 0 else 0.0\n\n    tp = total_metrics['TP']\n    tn = total_metrics['TN']\n    fp = total_metrics['FP']\n    fn = total_metrics['FN']\n\n    # Class 1 metrics\n    precision_1 = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall_1 = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1_score_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0\n\n    # Class 0 metrics\n    precision_0 = tn / (tn + fn) if (tn + fn) > 0 else 0\n    recall_0 = tn / (tn + fp) if (tn + fp) > 0 else 0\n    f1_score_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0) if (precision_0 + recall_0) > 0 else 0\n\n    jaccard_index = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0.0\n\n    # Print summary for each epoch\n    print(\n          f\"Precision: {(precision_1 +precision_0)/2:.4f}, Recall: {(recall_1 +recall_0)/2:.4f},\"\n          f\" F1-Score 0: {f1_score_0:.4f}, F1-Score 1: {f1_score_1:.4f}, F1-Score: {(f1_score_1 +f1_score_0)/2:.4f}\"\n          )\n    print(f\"Hamming Loss: {mean_hamming_distance:.4f}, jaccard index: {jaccard_index:.4f}\", )\n    return avg_loss, hamming_distances, total_metrics\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:16:36.635143Z","iopub.status.idle":"2025-09-04T17:16:36.635650Z","shell.execute_reply":"2025-09-04T17:16:36.635431Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Run Merge\n","metadata":{}},{"cell_type":"code","source":"\ndataloader = create_dataloader(train_df_squat, 16, batch_size=16)\neval_dataloader = create_dataloader(val_df_squat, 16, batch_size=16)\ntest_dataloader = create_dataloader(test_df_squat, 16, batch_size=16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:16:36.636827Z","iopub.status.idle":"2025-09-04T17:16:36.637268Z","shell.execute_reply":"2025-09-04T17:16:36.637076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Initialize the model\n# rating_model = MultiModalModel()\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# print(device)\n\n# # Train the model\n# train_combined_model(rating_model, dataloader, eval_dataloader, epochs=1000, lr=4e-4, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:16:36.638326Z","iopub.status.idle":"2025-09-04T17:16:36.638878Z","shell.execute_reply":"2025-09-04T17:16:36.638626Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Merge","metadata":{}},{"cell_type":"code","source":"# # Load the model for testing\n# def load_model(model, path, device):\n#     model.load_state_dict(torch.load(path, map_location=device))\n#     model.to(device)\n#     model.eval()\n#     return model\n# feature_loss = nn.BCEWithLogitsLoss()\n# # Load the trained model\n# test_model = MultiModalModel().to(device)\n# test_model = load_model(test_model, \"/kaggle/working/best_rating_model_epoch_32_ham.pt\", device)\n# evaluate_combined_model(test_model, test_dataloader, feature_loss, device)\n# # ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:16:36.639826Z","iopub.status.idle":"2025-09-04T17:16:36.640261Z","shell.execute_reply":"2025-09-04T17:16:36.640073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# torch.save(rating_model.state_dict(), \"final.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:16:36.641362Z","iopub.status.idle":"2025-09-04T17:16:36.641846Z","shell.execute_reply":"2025-09-04T17:16:36.641644Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Vision Run","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize the model\nrating_model = DualInputPose(1)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(device)\n\n# Train the model\ntrain_combined_model(rating_model, dataloader, eval_dataloader, epochs=1000, lr=4e-4, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:16:36.642712Z","iopub.status.idle":"2025-09-04T17:16:36.643155Z","shell.execute_reply":"2025-09-04T17:16:36.642962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the model for testing\ndef load_model(model, path, device):\n    model.load_state_dict(torch.load(path, map_location=device))\n    model.to(device)\n    model.eval()\n    return model\nfeature_loss = nn.BCEWithLogitsLoss()\n# Load the trained model\ntest_model = DualInputResNet3D(1).to(device)\ntest_model = load_model(test_model, \"/kaggle/working/best_rating_model_epoch_32_ham.pt\", device)\nevaluate_combined_model(test_model, test_dataloader, feature_loss, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:16:36.643985Z","iopub.status.idle":"2025-09-04T17:16:36.644492Z","shell.execute_reply":"2025-09-04T17:16:36.644251Z"}},"outputs":[],"execution_count":null}]}